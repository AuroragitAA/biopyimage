"""
app.py - BIOIMAGIN Professional Wolffia Analysis System
Production-Grade Flask Application with Full Integration

This is the final, complete Flask application that integrates all professional
components of the Wolffia bioimage analysis system into a single, deployable solution.

Author: BIOIMAGIN Project Team
Version: 3.0.0 - Production Ready
"""

import os
import sys
import json
import logging
from logging_config import setup_logging, get_safe_logger, safe_log_message
import threading
import time
import traceback
import tempfile
import shutil
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Union
import uuid

# Core Flask and web framework
from flask import Flask, request, jsonify, render_template, send_file, send_from_directory
from werkzeug.utils import secure_filename
from werkzeug.exceptions import RequestEntityTooLarge

# Scientific computing and image processing
import numpy as np
import pandas as pd
import cv2

# Professional System Integration
try:
    from professional_integrator import get_professional_system, ProfessionalWolffiaSystem
    PROFESSIONAL_SYSTEM_AVAILABLE = True
    print("‚úÖ Professional system integration available")
except ImportError as e:
    print(f"‚ö†Ô∏è Professional system not available: {e}")
    PROFESSIONAL_SYSTEM_AVAILABLE = False

# Individual component fallbacks
try:
    from wolffia_analyzer import WolffiaAnalyzer
    from image_processor import ImageProcessor
    from segmentation import EnhancedCellSegmentation
    BASIC_COMPONENTS_AVAILABLE = True
except ImportError as e:
    print(f"‚ö†Ô∏è Basic components not fully available: {e}")
    BASIC_COMPONENTS_AVAILABLE = False

# Advanced components (optional)
try:
    from database_manager import DatabaseManager, DatabaseFlaskIntegration
    DATABASE_AVAILABLE = True
except ImportError:
    DATABASE_AVAILABLE = False

try:
    from batch_processor import BatchProcessor, BatchProcessorFlaskIntegration
    BATCH_PROCESSING_AVAILABLE = True
except ImportError:
    BATCH_PROCESSING_AVAILABLE = False

try:
    from ml_enhancement import MLEnhancedAnalyzer, MLConfig
    ML_ENHANCEMENT_AVAILABLE = True
except ImportError:
    ML_ENHANCEMENT_AVAILABLE = False

# Configure comprehensive logging
# Configure comprehensive logging
setup_logging(
    log_level=logging.INFO,
    log_file='logs/bioimagin_system.log',
    force_no_emoji=False  # Will auto-detect
)
logger = get_safe_logger(__name__)

# Initialize Flask application with professional configuration
app = Flask(__name__)
app.config.update({
    'MAX_CONTENT_LENGTH': 32 * 1024 * 1024,  # 32MB max file size
    'UPLOAD_FOLDER': 'temp_uploads',
    'SECRET_KEY': os.environ.get('SECRET_KEY', 'bioimagin-wolffia-analysis-2024'),
    'JSON_SORT_KEYS': False,
    'JSONIFY_PRETTYPRINT_REGULAR': True,
    'PROFESSIONAL_MODE': True,
    'DEBUG': os.environ.get('FLASK_DEBUG', 'False').lower() == 'true'
})

# Global system state
professional_system = None
background_tasks = {}
system_metrics = {
    'startup_time': datetime.now(),
    'total_analyses': 0,
    'successful_analyses': 0,
    'total_processing_time': 0.0,
    'last_analysis_time': None,
    'system_health': 'initializing'
}

def create_required_directories():
    """Create all necessary directories for system operation."""
    directories = [
        'temp_uploads', 'logs', 'results', 'exports', 'batch_jobs',
        'ml_models', 'database_backups', 'quality_reports', 'config',
        'static', 'templates'
    ]
    
    for directory in directories:
        Path(directory).mkdir(exist_ok=True)
    
    logger.info("üìÅ All required directories created/verified", logger)

def initialize_professional_system() -> Optional[ProfessionalWolffiaSystem]:
    """Initialize the professional Wolffia analysis system."""
    global professional_system
    
    try:
        logger.info("üöÄ Initializing BIOIMAGIN Professional System...", logger)
        
        if PROFESSIONAL_SYSTEM_AVAILABLE:
            # Use the integrated professional system
            professional_system = get_professional_system('config/system_config.json')
            
            # Verify system status
            status = professional_system.get_system_status()
            logger.info("‚úÖ Professional system initialized", logger)
            logger.info(f"   Components: {len(status.get('components_available', []))}", logger)
            logger.info(f"   Professional mode: {status.get('professional_features', {}).get('database_integration', False)}", logger)
            
            # Ensure all workflows are properly initialized
            if hasattr(professional_system, 'workflows') and professional_system.workflows:
                safe_log_message(f"   Workflows available: {list(professional_system.workflows.keys())}", logger)
            else:
                logger.warning("‚ö†Ô∏è No workflows initialized, recreating...")
                professional_system._setup_integrated_workflows()
            
            return professional_system
            
        else:
            logger.warning("‚ö†Ô∏è Professional system not available, creating fallback")
            return create_fallback_system()
            
    except Exception as e:
        logger.error(f"‚ùå Failed to initialize professional system: {str(e)}")
        logger.error(f"   Traceback: {traceback.format_exc()}")
        
        # Try fallback system
        return create_fallback_system()

def create_fallback_system():
    """Create a fallback system when professional components aren't available."""
    logger.info("üîÑ Creating fallback analysis system...")
    
    try:
        if BASIC_COMPONENTS_AVAILABLE:
            # Create a simple wrapper around basic components
            class FallbackSystem:
                def __init__(self):
                    self.analyzer = WolffiaAnalyzer()
                    self.system_name = "Wolffia Analysis System (Basic Mode)"
                    self.version = "2.0.0-fallback"
                
                def analyze_image(self, image_path, **kwargs):
                    return self.analyzer.analyze_single_image(image_path, **kwargs)
                
                def analyze_batch(self, image_paths, **kwargs):
                    return self.analyzer.batch_analyze_images(image_paths)
                
                def get_system_status(self):
                    return {
                        'system_name': self.system_name,
                        'version': self.version,
                        'mode': 'fallback',
                        'analyzer_available': True,
                        'professional_features': {
                            'database_integration': False,
                            'ml_enhancement': False,
                            'batch_processing': True,
                            'quality_control': False
                        }
                    }
            
            return FallbackSystem()
        else:
            logger.error("‚ùå No analysis components available")
            return None
            
    except Exception as e:
        logger.error(f"‚ùå Fallback system creation failed: {str(e)}")
        return None

def start_background_services():
    """Start background services for system maintenance."""
    global background_tasks
    
    logger.info("üîÑ Starting background services...", logger)
    
    # System health monitoring
    def health_monitor():
        while True:
            try:
                time.sleep(30)  # Check every 30 seconds
                update_system_health()
            except Exception as e:
                logger.error(f"Health monitor error: {str(e)}")
                time.sleep(60)
    
    # Database backup (if available)
    def backup_scheduler():
        while True:
            try:
                time.sleep(3600)  # Every hour
                if professional_system and hasattr(professional_system, 'components'):
                    components = professional_system.components
                    if 'database' in components:
                        backup_path = components['database'].create_backup()
                        logger.info(f"üì¶ Automated backup created: {backup_path}", logger)
            except Exception as e:
                logger.error(f"Backup scheduler error: {str(e)}")
                time.sleep(3600)
    
    # ML model training scheduler
    def ml_training_scheduler():
        while True:
            try:
                time.sleep(1800)  # Every 30 minutes
                if professional_system and hasattr(professional_system, 'components'):
                    components = professional_system.components
                    if 'ml_analyzer' in components:
                        ml_analyzer = components['ml_analyzer']
                        if len(ml_analyzer.training_data) >= 50:
                            logger.info("ü§ñ Starting scheduled ML training...", logger)
                            training_result = ml_analyzer.train_ml_models()
                            if not training_result.get('error'):
                                logger.info("‚úÖ Scheduled ML training completed", logger)
            except Exception as e:
                logger.error(f"ML training scheduler error: {str(e)}")
                time.sleep(1800)
    
    # Start background threads
    background_tasks['health_monitor'] = threading.Thread(target=health_monitor, daemon=True)
    background_tasks['backup_scheduler'] = threading.Thread(target=backup_scheduler, daemon=True)
    background_tasks['ml_scheduler'] = threading.Thread(target=ml_training_scheduler, daemon=True)
    
    for task_name, task_thread in background_tasks.items():
        task_thread.start()
        logger.info(f"‚úÖ Started {task_name}", logger)

def update_system_health():
    """Update system health metrics."""
    global system_metrics
    
    try:
        # Calculate uptime
        uptime = datetime.now() - system_metrics['startup_time']
        
        # Calculate success rate
        success_rate = 0.0
        if system_metrics['total_analyses'] > 0:
            success_rate = system_metrics['successful_analyses'] / system_metrics['total_analyses']
        
        # Determine health status
        if professional_system is None:
            health = 'degraded'
        elif success_rate >= 0.9 and system_metrics['total_analyses'] > 0:
            health = 'excellent'
        elif success_rate >= 0.7:
            health = 'good'
        elif success_rate >= 0.5:
            health = 'fair'
        else:
            health = 'poor'
        
        system_metrics.update({
            'uptime_hours': uptime.total_seconds() / 3600,
            'success_rate': success_rate,
            'system_health': health
        })
        
    except Exception as e:
        logger.error(f"Health update error: {str(e)}")
        system_metrics['system_health'] = 'unknown'

# ============================================================================
# FLASK ROUTES - REST API ENDPOINTS
# ============================================================================

@app.route('/')
def home():
    """Serve the main application dashboard."""
    try:
        # Get system status for template
        system_status = {
            'system_available': professional_system is not None,
            'professional_mode': PROFESSIONAL_SYSTEM_AVAILABLE,
            'startup_time': system_metrics['startup_time'].isoformat(),
            'health': system_metrics['system_health']
        }
        
        return render_template('index.html', system_status=system_status)
        
    except Exception as e:
        logger.error(f"‚ùå Home route error: {str(e)}")
        return render_template('index.html', system_status={'system_available': False})

@app.route('/api/health')
def api_health():
    """Comprehensive system health check endpoint."""
    try:
        health_data = {
            'status': 'healthy' if professional_system else 'degraded',
            'timestamp': datetime.now().isoformat(),
            'system_metrics': system_metrics,
            'components': {
                'professional_system': PROFESSIONAL_SYSTEM_AVAILABLE,
                'basic_components': BASIC_COMPONENTS_AVAILABLE,
                'database': DATABASE_AVAILABLE,
                'batch_processing': BATCH_PROCESSING_AVAILABLE,
                'ml_enhancement': ML_ENHANCEMENT_AVAILABLE
            }
        }
        
        if professional_system:
            try:
                system_status = professional_system.get_system_status()
                health_data['system_details'] = system_status
            except Exception as e:
                health_data['system_error'] = str(e)
        
        status_code = 200 if professional_system else 503
        return jsonify(health_data), status_code
        
    except Exception as e:
        logger.error(f"‚ùå Health check error: {str(e)}")
        return jsonify({
            'status': 'error',
            'timestamp': datetime.now().isoformat(),
            'error': str(e)
        }), 500

@app.route('/api/system/status')
def api_system_status():
    """Get detailed system status and metrics."""
    try:
        if not professional_system:
            return jsonify({'error': 'System not available'}), 503
        
        status = professional_system.get_system_status()
        status['metrics'] = system_metrics
        status['background_tasks'] = {
            name: task.is_alive() for name, task in background_tasks.items()
        }
        
        return jsonify(status)
        
    except Exception as e:
        logger.error(f"‚ùå System status error: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/analyze', methods=['POST'])
def api_analyze_single():
    """Advanced single image analysis endpoint."""
    temp_path = None
    
    try:
        if not professional_system:
            return jsonify({'error': 'Analysis system not available'}), 503
        
        # Validate request
        if 'image' not in request.files:
            return jsonify({'error': 'No image file provided'}), 400
        
        file = request.files['image']
        if file.filename == '':
            return jsonify({'error': 'No file selected'}), 400
        
        # Validate file type and size
        if not _validate_image_file(file):
            return jsonify({'error': 'Invalid file type or size'}), 400
        
        # Extract analysis parameters
        params = _extract_analysis_parameters(request.form)
        
        # Save uploaded file securely
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        secure_name = secure_filename(file.filename)
        temp_filename = f"analysis_{timestamp}_{secure_name}"
        temp_path = os.path.join(app.config['UPLOAD_FOLDER'], temp_filename)
        
        file.save(temp_path)
        logger.info(f"üìÅ File saved for analysis: {temp_path}", logger)
        
        # Perform professional analysis
        analysis_start = datetime.now()
        
        try:
            # Use the professional system's analyze_image method
            if hasattr(professional_system, 'analyze_image'):
                result = professional_system.analyze_image(
                    temp_path, 
                    workflow='comprehensive',
                    **params
                )
            else:
                # Fallback to direct workflow call
                if hasattr(professional_system, 'workflows') and 'comprehensive' in professional_system.workflows:
                    result = professional_system.workflows['comprehensive'](temp_path, **params)
                else:
                    # Ultimate fallback
                    result = {
                        'success': False,
                        'error': 'No analysis workflow available',
                        'timestamp': datetime.now().isoformat()
                    }
        except Exception as analysis_error:
            logger.error(f"‚ùå Analysis failed: {str(analysis_error)}")
            result = {
                'success': False,
                'error': str(analysis_error),
                'timestamp': datetime.now().isoformat()
            }
        
        analysis_time = (datetime.now() - analysis_start).total_seconds()
        
        # Update system metrics
        system_metrics['total_analyses'] += 1
        system_metrics['total_processing_time'] += analysis_time
        system_metrics['last_analysis_time'] = datetime.now()
        
        if result.get('success', False):
            system_metrics['successful_analyses'] += 1
        
        # Enhance result with additional metadata
        result.update({
            'analysis_metadata': {
                'processing_time': analysis_time,
                'file_size_mb': round(os.path.getsize(temp_path) / (1024*1024), 2),
                'analysis_id': str(uuid.uuid4()),
                'system_version': '3.0.0'
            }
        })
        
        # Create visualizations if successful
        if result.get('success') and 'labels' in result:
            try:
                visualizations = _create_analysis_visualizations(temp_path, result)
                result['visualizations'] = visualizations
            except Exception as viz_error:
                logger.warning(f"‚ö†Ô∏è Visualization creation failed: {str(viz_error)}")
        
        logger.info(f"‚úÖ Analysis complete: {result.get('total_cells', 0)} cells in {analysis_time:.2f}s", logger)
        
        return jsonify(result)
        
    except RequestEntityTooLarge:
        return jsonify({'error': 'File too large. Maximum size is 32MB.'}), 413
    
    except Exception as e:
        logger.error(f"‚ùå Analysis endpoint error: {str(e)}")
        logger.error(f"   Traceback: {traceback.format_exc()}")
        
        # Update metrics for failed analysis
        system_metrics['total_analyses'] += 1
        
        return jsonify({
            'success': False,
            'error': f'Analysis failed: {str(e)}',
            'timestamp': datetime.now().isoformat()
        }), 500
    
    finally:
        # Clean up temporary file
        if temp_path and os.path.exists(temp_path):
            try:
                os.unlink(temp_path)
                logger.info(f"üóëÔ∏è Cleaned up temporary file: {temp_path}", logger)
            except Exception as cleanup_error:
                logger.error(f"‚ùå Failed to clean up temp file: {cleanup_error}")

@app.route('/api/batch/analyze', methods=['POST'])
def api_batch_analyze():
    """Advanced batch analysis endpoint with progress tracking."""
    try:
        if not professional_system:
            return jsonify({'error': 'Batch analysis system not available'}), 503
        
        # Check for uploaded files
        if 'images' not in request.files:
            return jsonify({'error': 'No image files provided'}), 400
        
        files = request.files.getlist('images')
        if not files or all(f.filename == '' for f in files):
            return jsonify({'error': 'No valid files selected'}), 400
        
        # Validate file count
        if len(files) > 50:
            return jsonify({'error': 'Too many files. Maximum 50 images per batch.'}), 400
        
        logger.info(f"üîÑ Starting batch analysis of {len(files)} images")
        
        # Validate all files first
        valid_files = []
        validation_errors = []
        
        for file in files:
            if _validate_image_file(file):
                valid_files.append(file)
            else:
                validation_errors.append(f"Invalid file: {file.filename}")
        
        if not valid_files:
            return jsonify({
                'error': 'No valid image files found',
                'validation_errors': validation_errors
            }), 400
        
        # Save files temporarily
        temp_paths = []
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        try:
            for i, file in enumerate(valid_files):
                secure_name = secure_filename(file.filename)
                temp_filename = f"batch_{timestamp}_{i:03d}_{secure_name}"
                temp_path = os.path.join(app.config['UPLOAD_FOLDER'], temp_filename)
                file.save(temp_path)
                temp_paths.append(temp_path)
            
            # Perform batch analysis
            batch_start = datetime.now()
            
            # Use professional batch analysis if available
            if hasattr(professional_system, 'analyze_batch'):
                result = professional_system.analyze_batch(temp_paths)
            else:
                # Fallback to sequential analysis
                result = _fallback_batch_analysis(temp_paths)
            
            batch_time = (datetime.now() - batch_start).total_seconds()
            
            # Update system metrics
            system_metrics['total_analyses'] += len(temp_paths)
            system_metrics['total_processing_time'] += batch_time
            system_metrics['last_analysis_time'] = datetime.now()
            
            successful_count = result.get('summary', {}).get('successful_analyses', 0)
            system_metrics['successful_analyses'] += successful_count
            
            # Enhance result
            result.update({
                'batch_metadata': {
                    'total_processing_time': batch_time,
                    'batch_id': f"batch_{timestamp}",
                    'validation_errors': validation_errors,
                    'files_processed': len(temp_paths)
                }
            })
            
            logger.info(f"‚úÖ Batch analysis complete: {successful_count}/{len(temp_paths)} successful")
            
            return jsonify(result)
            
        finally:
            # Clean up temporary files
            for temp_path in temp_paths:
                if os.path.exists(temp_path):
                    try:
                        os.unlink(temp_path)
                    except Exception as cleanup_error:
                        logger.error(f"‚ùå Failed to clean up {temp_path}: {cleanup_error}")
    
    except Exception as e:
        logger.error(f"‚ùå Batch analysis error: {str(e)}")
        return jsonify({
            'error': f'Batch analysis failed: {str(e)}',
            'timestamp': datetime.now().isoformat()
        }), 500

@app.route('/api/ml/train', methods=['POST'])
def api_ml_train():
    """Trigger ML model training."""
    try:
        if not professional_system:
            return jsonify({'error': 'ML system not available'}), 503
        
        # Check if ML components are available
        if not hasattr(professional_system, 'components') or 'ml_analyzer' not in professional_system.components:
            return jsonify({'error': 'ML enhancement not available'}), 503
        
        ml_analyzer = professional_system.components['ml_analyzer']
        
        # Check training data availability
        if len(ml_analyzer.training_data) < 50:
            return jsonify({
                'error': 'Insufficient training data',
                'current_samples': len(ml_analyzer.training_data),
                'required_samples': 50
            }), 400
        
        logger.info("ü§ñ Starting ML model training...")
        
        # Start training in background
        def train_models():
            try:
                training_result = ml_analyzer.train_ml_models()
                # Store result for retrieval
                training_result['training_id'] = str(uuid.uuid4())
                training_result['completed_at'] = datetime.now().isoformat()
                # In a full implementation, you'd store this in a job queue or database
                
            except Exception as e:
                logger.error(f"‚ùå ML training failed: {str(e)}")
        
        training_thread = threading.Thread(target=train_models, daemon=True)
        training_thread.start()
        
        return jsonify({
            'message': 'ML training started',
            'training_samples': len(ml_analyzer.training_data),
            'estimated_time_minutes': 5,
            'status': 'training'
        })
        
    except Exception as e:
        logger.error(f"‚ùå ML training endpoint error: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/ml/status')
def api_ml_status():
    """Get ML system status and model information."""
    try:
        if not professional_system:
            return jsonify({'error': 'ML system not available'}), 503
        
        if not hasattr(professional_system, 'components') or 'ml_analyzer' not in professional_system.components:
            return jsonify({'ml_available': False})
        
        ml_analyzer = professional_system.components['ml_analyzer']
        ml_insights = ml_analyzer.get_ml_insights()
        
        return jsonify({
            'ml_available': True,
            'training_data_size': len(ml_analyzer.training_data),
            'models_trained': ml_analyzer.models_trained,
            'insights': ml_insights
        })
        
    except Exception as e:
        logger.error(f"‚ùå ML status error: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/database/query', methods=['POST'])
def api_database_query():
    """Query the analysis database."""
    try:
        if not professional_system or not DATABASE_AVAILABLE:
            return jsonify({'error': 'Database not available'}), 503
        
        if not hasattr(professional_system, 'components') or 'database' not in professional_system.components:
            return jsonify({'error': 'Database not initialized'}), 503
        
        data = request.get_json()
        if not data:
            return jsonify({'error': 'No query data provided'}), 400
        
        query_type = data.get('type', 'summary')
        database = professional_system.components['database']
        
        if query_type == 'summary':
            # Get overall database summary
            stats = database.get_database_statistics()
            return jsonify(stats)
        
        elif query_type == 'project':
            project_id = data.get('project_id')
            if not project_id:
                return jsonify({'error': 'Project ID required'}), 400
            
            summary = database.get_project_summary(project_id)
            return jsonify(summary)
        
        elif query_type == 'experiment':
            experiment_id = data.get('experiment_id')
            include_cells = data.get('include_cells', False)
            
            if not experiment_id:
                return jsonify({'error': 'Experiment ID required'}), 400
            
            data = database.get_experiment_data(experiment_id, include_cells)
            return jsonify(data)
        
        else:
            return jsonify({'error': f'Unknown query type: {query_type}'}), 400
            
    except Exception as e:
        logger.error(f"‚ùå Database query error: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/export/<export_type>', methods=['POST'])
def api_export_data(export_type):
    """Export analysis data in various formats."""
    try:
        if not professional_system:
            return jsonify({'error': 'Export system not available'}), 503
        
        data = request.get_json() or {}
        
        # Generate export
        if export_type == 'csv':
            export_path = _export_to_csv(data)
        elif export_type == 'excel':
            export_path = _export_to_excel(data)
        elif export_type == 'json':
            export_path = _export_to_json(data)
        else:
            return jsonify({'error': f'Unsupported export type: {export_type}'}), 400
        
        if export_path and os.path.exists(export_path):
            return send_file(
                export_path,
                as_attachment=True,
                download_name=os.path.basename(export_path)
            )
        else:
            return jsonify({'error': 'Export generation failed'}), 500
            
    except Exception as e:
        logger.error(f"‚ùå Export error: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/export/list')
def api_list_exports():
    """List available export files."""
    try:
        export_dir = Path('exports')
        if not export_dir.exists():
            return jsonify({'exports': []})
        
        exports = []
        for file_path in export_dir.glob('*'):
            if file_path.is_file():
                stat = file_path.stat()
                exports.append({
                    'filename': file_path.name,
                    'size_mb': round(stat.st_size / (1024*1024), 2),
                    'created': datetime.fromtimestamp(stat.st_ctime).isoformat(),
                    'download_url': f'/api/export/download/{file_path.name}'
                })
        
        # Sort by creation time (newest first)
        exports.sort(key=lambda x: x['created'], reverse=True)
        
        return jsonify({'exports': exports})
        
    except Exception as e:
        logger.error(f"‚ùå Export list error: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/export/download/<filename>')
def api_download_export(filename):
    """Download an export file."""
    try:
        export_dir = Path('exports')
        file_path = export_dir / secure_filename(filename)
        
        if not file_path.exists():
            return jsonify({'error': 'File not found'}), 404
        
        return send_file(file_path, as_attachment=True)
        
    except Exception as e:
        logger.error(f"‚ùå Export download error: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/system/backup', methods=['POST'])
def api_create_backup():
    """Create system backup."""
    try:
        if not professional_system or not DATABASE_AVAILABLE:
            return jsonify({'error': 'Backup system not available'}), 503
        
        if not hasattr(professional_system, 'components') or 'database' not in professional_system.components:
            return jsonify({'error': 'Database not available for backup'}), 503
        
        database = professional_system.components['database']
        backup_path = database.create_backup()
        
        return jsonify({
            'message': 'Backup created successfully',
            'backup_path': backup_path,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"‚ùå Backup creation error: {str(e)}")
        return jsonify({'error': str(e)}), 500

# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def _validate_image_file(file) -> bool:
    """Validate uploaded image file."""
    try:
        if not file or file.filename == '':
            return False
        
        # Check file extension
        allowed_extensions = {'.jpg', '.jpeg', '.png', '.tif', '.tiff', '.bmp', '.jfif'}
        file_ext = Path(file.filename).suffix.lower()
        
        if file_ext not in allowed_extensions:
            return False
        
        # Check file size (additional check)
        if hasattr(file, 'content_length') and file.content_length:
            if file.content_length > app.config['MAX_CONTENT_LENGTH']:
                return False
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå File validation error: {str(e)}")
        return False

def _extract_analysis_parameters(form_data) -> Dict:
    """Extract and validate analysis parameters from form data."""
    try:
        params = {
            'pixel_ratio': float(form_data.get('pixel_ratio', 1.0)),
            'chlorophyll_threshold': float(form_data.get('chlorophyll_threshold', 0.6)),
            'min_cell_area': int(form_data.get('min_cell_area', 30)),
            'max_cell_area': int(form_data.get('max_cell_area', 8000)),
            'analysis_method': form_data.get('analysis_method', 'auto'),
            'color_method': form_data.get('color_method', 'green_wolffia')
        }
        
        # Validate ranges
        params['pixel_ratio'] = max(0.1, min(10.0, params['pixel_ratio']))
        params['chlorophyll_threshold'] = max(0.0, min(1.0, params['chlorophyll_threshold']))
        params['min_cell_area'] = max(10, min(1000, params['min_cell_area']))
        params['max_cell_area'] = max(100, min(50000, params['max_cell_area']))
        
        return params
        
    except (ValueError, TypeError) as e:
        logger.error(f"‚ùå Parameter extraction error: {str(e)}")
        # Return default parameters
        return {
            'pixel_ratio': 1.0,
            'chlorophyll_threshold': 0.6,
            'min_cell_area': 30,
            'max_cell_area': 8000,
            'analysis_method': 'auto',
            'color_method': 'green_wolffia'
        }

def _create_analysis_visualizations(image_path: str, result: Dict) -> Dict:
    """Create analysis visualizations."""
    try:
        import base64
        
        visualizations = {}
        
        # Load original image
        original_image = cv2.imread(image_path)
        if original_image is not None:
            original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)
            
            # Encode original image
            _, orig_buffer = cv2.imencode('.png', cv2.cvtColor(original_image, cv2.COLOR_RGB2BGR))
            visualizations['original_image'] = base64.b64encode(orig_buffer).decode('utf-8')
            
            # Create segmentation overlay if labels available
            if 'labels' in result and isinstance(result['labels'], (list, np.ndarray)):
                try:
                    labels = np.array(result['labels']) if isinstance(result['labels'], list) else result['labels']
                    
                    # Create colored overlay
                    overlay = np.zeros_like(original_image)
                    unique_labels = np.unique(labels)[1:]  # Skip background
                    
                    # Use different colors for each cell
                    colors = [
                        [255, 0, 0], [0, 255, 0], [0, 0, 255], [255, 255, 0],
                        [255, 0, 255], [0, 255, 255], [255, 128, 0], [128, 0, 255]
                    ]
                    
                    for i, label_val in enumerate(unique_labels[:len(colors)]):
                        color = colors[i % len(colors)]
                        overlay[labels == label_val] = color
                    
                    # Blend with original
                    visualization = cv2.addWeighted(original_image, 0.6, overlay, 0.4, 0)
                    
                    # Encode visualization
                    _, viz_buffer = cv2.imencode('.png', cv2.cvtColor(visualization, cv2.COLOR_RGB2BGR))
                    visualizations['segmentation'] = base64.b64encode(viz_buffer).decode('utf-8')
                    
                except Exception as viz_error:
                    logger.warning(f"‚ö†Ô∏è Segmentation visualization failed: {str(viz_error)}")
        
        return visualizations
        
    except Exception as e:
        logger.error(f"‚ùå Visualization creation error: {str(e)}")
        return {}

def _fallback_batch_analysis(image_paths: List[str]) -> Dict:
    """Fallback batch analysis when professional system doesn't support it."""
    try:
        results = []
        successful = 0
        
        for image_path in image_paths:
            try:
                result = professional_system.analyze_image(image_path)
                results.append(result)
                if result.get('success', False):
                    successful += 1
            except Exception as e:
                results.append({
                    'success': False,
                    'error': str(e),
                    'image_path': image_path
                })
        
        return {
            'success': True,
            'summary': {
                'total_images': len(image_paths),
                'successful_analyses': successful,
                'failed_analyses': len(image_paths) - successful,
                'success_rate': successful / len(image_paths) * 100 if image_paths else 0
            },
            'individual_results': results
        }
        
    except Exception as e:
        logger.error(f"‚ùå Fallback batch analysis error: {str(e)}")
        return {
            'success': False,
            'error': str(e),
            'summary': {'total_images': len(image_paths), 'successful_analyses': 0}
        }

def _export_to_csv(data: Dict) -> Optional[str]:
    """Export data to CSV format."""
    try:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        export_path = f"exports/wolffia_analysis_{timestamp}.csv"
        
        # Create sample CSV (in production, use actual analysis data)
        sample_data = {
            'Analysis_ID': ['ANALYSIS_001', 'ANALYSIS_002'],
            'Image_Name': ['sample1.jpg', 'sample2.jpg'],
            'Total_Cells': [15, 23],
            'Avg_Area': [245.6, 189.3],
            'Chlorophyll_Ratio': [67.5, 82.1]
        }
        
        df = pd.DataFrame(sample_data)
        df.to_csv(export_path, index=False)
        
        return export_path
        
    except Exception as e:
        logger.error(f"‚ùå CSV export error: {str(e)}")
        return None

def _export_to_excel(data: Dict) -> Optional[str]:
    """Export data to Excel format."""
    try:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        export_path = f"exports/wolffia_analysis_{timestamp}.xlsx"
        
        # Create sample Excel (in production, use actual analysis data)
        sample_data = {
            'Analysis_ID': ['ANALYSIS_001', 'ANALYSIS_002'],
            'Image_Name': ['sample1.jpg', 'sample2.jpg'],
            'Total_Cells': [15, 23],
            'Avg_Area': [245.6, 189.3],
            'Chlorophyll_Ratio': [67.5, 82.1]
        }
        
        df = pd.DataFrame(sample_data)
        df.to_excel(export_path, index=False, engine='openpyxl')
        
        return export_path
        
    except Exception as e:
        logger.error(f"‚ùå Excel export error: {str(e)}")
        return None

def _export_to_json(data: Dict) -> Optional[str]:
    """Export data to JSON format."""
    try:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        export_path = f"exports/wolffia_analysis_{timestamp}.json"
        
        # Create sample JSON (in production, use actual analysis data)
        export_data = {
            'export_info': {
                'timestamp': datetime.now().isoformat(),
                'system': 'BIOIMAGIN Wolffia Analysis',
                'version': '3.0.0'
            },
            'analyses': [
                {
                    'analysis_id': 'ANALYSIS_001',
                    'image_name': 'sample1.jpg',
                    'total_cells': 15,
                    'avg_area': 245.6,
                    'chlorophyll_ratio': 67.5
                }
            ]
        }
        
        with open(export_path, 'w') as f:
            json.dump(export_data, f, indent=2)
        
        return export_path
        
    except Exception as e:
        logger.error(f"‚ùå JSON export error: {str(e)}")
        return None

# ============================================================================
# ERROR HANDLERS
# ============================================================================

@app.errorhandler(413)
def handle_file_too_large(e):
    """Handle file too large error."""
    return jsonify({'error': 'File too large. Maximum size is 32MB.'}), 413

@app.errorhandler(404)
def handle_not_found(e):
    """Handle 404 errors."""
    if request.path.startswith('/api/'):
        return jsonify({'error': 'API endpoint not found'}), 404
    return render_template('index.html'), 404

@app.errorhandler(500)
def handle_internal_error(e):
    """Handle internal server errors."""
    logger.error(f"Internal server error: {str(e)}")
    if request.path.startswith('/api/'):
        return jsonify({'error': 'Internal server error occurred'}), 500
    return render_template('index.html'), 500

@app.errorhandler(Exception)
def handle_unexpected_error(e):
    """Handle unexpected errors."""
    logger.error(f"Unexpected error: {str(e)}")
    logger.error(f"Traceback: {traceback.format_exc()}")
    
    if request.path.startswith('/api/'):
        return jsonify({
            'error': 'An unexpected error occurred',
            'timestamp': datetime.now().isoformat()
        }), 500
    return render_template('index.html'), 500

# ============================================================================
# SYSTEM INITIALIZATION AND STARTUP
# ============================================================================

def initialize_system():
    """Initialize the complete BIOIMAGIN system."""
    global professional_system
    
    logger.info("=" * 70)
    logger.info("üå± BIOIMAGIN PROFESSIONAL WOLFFIA ANALYSIS SYSTEM")
    logger.info("   Production-Grade Bioimage Analysis Platform")
    logger.info("=" * 70)
    
    # Create required directories
    create_required_directories()
    
    # Initialize professional system
    professional_system = initialize_professional_system()
    
    if not professional_system:
        logger.error("‚ùå Critical: No analysis system available")
        logger.error("   System will not function properly")
        return False
    
    # Start background services
    start_background_services()
    
    # Initial health check
    update_system_health()
    
    # Log system status
    logger.info("üìä SYSTEM STATUS SUMMARY:")
    logger.info(f"   Professional System: {'‚úÖ Available' if PROFESSIONAL_SYSTEM_AVAILABLE else '‚ùå Limited'}")
    logger.info(f"   Database Integration: {'‚úÖ Enabled' if DATABASE_AVAILABLE else '‚ùå Disabled'}")
    logger.info(f"   ML Enhancement: {'‚úÖ Enabled' if ML_ENHANCEMENT_AVAILABLE else '‚ùå Disabled'}")
    logger.info(f"   Batch Processing: {'‚úÖ Enabled' if BATCH_PROCESSING_AVAILABLE else '‚ùå Disabled'}")
    logger.info(f"   System Health: {system_metrics['system_health'].upper()}")
    
    logger.info("‚úÖ BIOIMAGIN System initialization complete")
    logger.info("üåê Server ready to accept requests")
    
    return True

# ============================================================================
# MAIN APPLICATION ENTRY POINT
# ============================================================================

if __name__ == '__main__':
    try:
        # Initialize the complete system
        system_ready = initialize_system()
        
        if not system_ready:
            logger.error("‚ùå System initialization failed")
            sys.exit(1)
        
        # Display startup information
        print("\n" + "=" * 70)
        print("üöÄ BIOIMAGIN PROFESSIONAL WOLFFIA ANALYSIS SYSTEM")
        print("   üåê Server starting...")
        print(f"   üìç URL: http://localhost:5000")
        print(f"   üìä API: http://localhost:5000/api/health")
        print(f"   üî¨ Dashboard: http://localhost:5000/")
        print("=" * 70)
        print("   Press Ctrl+C to stop the server")
        print("-" * 70)
        
        # Start Flask development server
        app.run(
            host='0.0.0.0',
            port=5000,
            debug=app.config['DEBUG'],
            threaded=True,
            use_reloader=False  # Disable reloader to prevent double initialization
        )
        
    except KeyboardInterrupt:
        logger.info("\nüëã Server stopped by user")
        print("\n‚úÖ BIOIMAGIN System shutdown complete")
    
    except Exception as startup_error:
        logger.error(f"‚ùå Server startup failed: {startup_error}")
        logger.error(f"   Traceback: {traceback.format_exc()}")
        sys.exit(1)