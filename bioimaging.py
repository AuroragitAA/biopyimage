# bioimaging.py - Enhanced Production-Grade Wolffia Analysis System

import base64
import json
import os
import warnings
from datetime import datetime
from io import BytesIO

import cv2
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from scipy import ndimage, signal, stats
from skimage import feature, filters, measure, morphology, restoration, segmentation
from skimage.color import rgb2gray, rgb2hsv, rgb2lab
from skimage.exposure import adjust_gamma, equalize_adapthist
from skimage.filters import (
    gaussian,
    threshold_local,
    threshold_multiotsu,
    threshold_otsu,
)
from skimage.morphology import (
    closing,
    disk,
    opening,
    remove_small_objects,
    white_tophat,
)
from skimage.segmentation import clear_border, felzenszwalb, slic, watershed
from sklearn.cluster import DBSCAN, KMeans
from sklearn.decomposition import PCA
from sklearn.ensemble import IsolationForest, RandomForestClassifier
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler

matplotlib.use('Agg')
warnings.filterwarnings('ignore')

class WolffiaAnalyzer:
    """
    Production-grade Wolffia bioimage analysis pipeline with ML enhancement
    """
    
    def __init__(self, pixel_to_micron_ratio=1.0, chlorophyll_threshold=0.6):
        self.pixel_to_micron = pixel_to_micron_ratio
        self.chlorophyll_threshold = chlorophyll_threshold
        self.results_history = []
        self.ml_classifier = None
        self.anomaly_detector = None
        self.feature_importance = None
        
        # Enhanced wavelength data for spectral analysis
        self.wavelength_data = {
            'red': {'nm': 660, 'chlorophyll_absorption': 0.85},
            'green': {'nm': 550, 'chlorophyll_absorption': 0.15},
            'blue': {'nm': 450, 'chlorophyll_absorption': 0.65},
            'nir': {'nm': 850, 'chlorophyll_absorption': 0.05}  # Near-infrared if available
        }
        
        # Wolffia-specific parameters
        self.wolffia_params = {
            'min_area_microns': 30,  # Minimum area in Œºm¬≤
            'max_area_microns': 12000,  # Maximum area in Œºm¬≤
            'expected_circularity': 0.85,  # Expected circularity for healthy cells
            'chlorophyll_peaks': [435, 670],  # Chlorophyll a absorption peaks
            'growth_rate_range': [0.1, 0.5],  # Daily growth rate range
            'doubling_time_hours': [24, 96]  # Cell doubling time range
        }
        
        # Initialize ML components
        self._initialize_ml_components()
        
    def _initialize_ml_components(self):
        """Initialize machine learning components"""
        # Random Forest for cell classification
        self.ml_classifier = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            n_jobs=-1
        )
        
        # Isolation Forest for anomaly detection
        self.anomaly_detector = IsolationForest(
            contamination=0.1,
            random_state=42,
            n_jobs=-1
        )
        
    def advanced_preprocess_image(self, image_path, enhance_contrast=True, denoise=True):
        """
        Advanced preprocessing optimized for petri dish Wolffia images
        """
        try:
            # Load image
            if isinstance(image_path, str):
                image = cv2.imread(image_path)
                if image is None:
                    raise ValueError(f"Could not load image from {image_path}")
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            else:
                image = image_path
                
            original = image.copy()
            
            # Multi-scale denoising
            if denoise:
                image = restoration.denoise_bilateral(
                    image, 
                    sigma_color=0.05, 
                    sigma_spatial=15,
                    channel_axis=-1
                )
            
            # Color space conversions
            gray = rgb2gray(image)
            hsv = rgb2hsv(image)
            lab = rgb2lab(image)
            
            # Illumination correction using white top-hat
            selem = disk(30)
            background = morphology.white_tophat(gray, selem)
            gray_corrected = gray - background
            gray_corrected = np.clip(gray_corrected, 0, 1)
            
            # Enhanced chlorophyll detection using multiple color spaces
            # Extract channels
            h_channel = hsv[:, :, 0]
            s_channel = hsv[:, :, 1]
            v_channel = hsv[:, :, 2]
            l_channel = lab[:, :, 0] / 100.0
            a_channel = (lab[:, :, 1] + 128) / 255.0
            b_channel = (lab[:, :, 2] + 128) / 255.0
            
            # Green color detection in HSV (Hue: 60-140 degrees)
            green_mask_hsv = (
                (h_channel >= 60/360) & (h_channel <= 140/360) &
                (s_channel >= 0.2) & (v_channel >= 0.2)
            )
            
            # Green detection in LAB (negative a values indicate green)
            green_mask_lab = (a_channel < 0.45) & (l_channel > 0.2)
            
            # Combine masks
            green_mask = green_mask_hsv | green_mask_lab
            
            # Chlorophyll index calculation
            red_channel = image[:, :, 0] / 255.0
            green_channel = image[:, :, 1] / 255.0
            blue_channel = image[:, :, 2] / 255.0
            nir_channel = green_channel  # Approximation when NIR not available
            
            # Multiple vegetation indices
            # Normalized Difference Vegetation Index (NDVI) approximation
            ndvi = (nir_channel - red_channel) / (nir_channel + red_channel + 1e-10)
            
            # Green Chlorophyll Index (GCI)
            gci = (nir_channel / green_channel) - 1
            
            # Excess Green Index (ExG)
            exg = 2 * green_channel - red_channel - blue_channel
            
            # Combined chlorophyll index
            chlorophyll_enhanced = np.maximum.reduce([ndvi, gci, exg])
            chlorophyll_enhanced = np.clip(chlorophyll_enhanced, 0, 1)
            
            # Apply green mask to focus on Wolffia
            chlorophyll_enhanced = np.where(green_mask, chlorophyll_enhanced, chlorophyll_enhanced * 0.3)
            green_channel = np.where(green_mask, green_channel, 0)
            gray = np.where(green_mask, gray, 0)
            
            # Adaptive contrast enhancement
            if enhance_contrast:
                gray_corrected = equalize_adapthist(gray_corrected, clip_limit=0.03)
                chlorophyll_enhanced = equalize_adapthist(chlorophyll_enhanced, clip_limit=0.03)
            
            # Edge-preserving smoothing
            gray_smoothed = filters.median(gray_corrected, disk(2))
            chlorophyll_smoothed = filters.median(chlorophyll_enhanced, disk(2))
            
            return {
                'original': original,
                'gray': gray_smoothed,
                'gray_corrected': gray_corrected,
                'green_channel': green_channel * green_mask,
                'chlorophyll_enhanced': chlorophyll_smoothed,
                'hsv': hsv,
                'lab': lab,
                'green_mask': green_mask,
                'ndvi': ndvi,
                'gci': gci,
                'exg': exg
            }
            
        except Exception as e:
            print(f"Error in advanced preprocessing: {str(e)}")
            return None
            
    def multi_method_segmentation(self, preprocessed_data, min_cell_area=30, max_cell_area=8000):
        """
        Robust multi-method segmentation tuned for broad sensitivity
        """
        try:
            gray = preprocessed_data['gray_corrected']
            chlorophyll = preprocessed_data['chlorophyll_enhanced']
            green_mask = preprocessed_data['green_mask']

            # Method 1: Multi-Otsu thresholding
            thresholds = threshold_multiotsu(gray, classes=3)
            regions = np.digitize(gray, bins=thresholds)
            binary_otsu = (regions >= 1) & green_mask

            # Method 2: Adaptive thresholding with multiple block sizes
            block_sizes = [21, 35, 51]
            adaptive_masks = [(gray > threshold_local(gray, block_size=b, method='gaussian', offset=0.02)) & green_mask for b in block_sizes]
            binary_adaptive = np.logical_or.reduce(adaptive_masks)

            # Method 3: K-means clustering
            lab = preprocessed_data['lab']
            pixels = lab.reshape(-1, 3)
            sample_indices = np.random.choice(len(pixels), min(10000, len(pixels)), replace=False)
            kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
            kmeans.fit(pixels[sample_indices])
            labels_kmeans = kmeans.predict(pixels).reshape(gray.shape)
            green_clusters = [i for i in range(5) if np.mean(green_mask[labels_kmeans == i]) > 0.5]
            binary_kmeans = np.isin(labels_kmeans, green_clusters)

            # Method 4: Felzenszwalb
            segments_fz = felzenszwalb(preprocessed_data['original'], scale=100, sigma=0.5, min_size=50)
            binary_fz = np.zeros_like(gray, dtype=bool)
            for sid in np.unique(segments_fz):
                mask = segments_fz == sid
                if np.mean(green_mask[mask]) > 0.7 and np.sum(mask) >= min_cell_area:
                    binary_fz |= mask

            # Method 5: SLIC
            segments_slic = slic(preprocessed_data['original'], n_segments=500, compactness=10, start_label=1)
            binary_slic = np.zeros_like(gray, dtype=bool)
            for sid in np.unique(segments_slic):
                mask = segments_slic == sid
                if np.mean(chlorophyll[mask]) > 0.2 and np.sum(mask) >= min_cell_area:
                    binary_slic |= mask

            # Lenient combination
            combined_binary = (
                binary_otsu.astype(int) * 2 +
                binary_adaptive.astype(int) +
                binary_kmeans.astype(int) * 2 +
                binary_fz.astype(int) +
                binary_slic.astype(int)
            ) >= 2

            if np.sum(combined_binary) < 10:
                print("‚ö†Ô∏è Combined mask too weak. Attempting fallback using green mask and chlorophyll...")
                fallback = green_mask & (chlorophyll > 0.1)
                if np.sum(fallback) > 10:
                    combined_binary = fallback
                    print("‚úÖ Fallback succeeded with", np.sum(combined_binary), "pixels")
                else:
                    print("‚ùå Fallback failed: no viable regions")
                    return np.zeros_like(gray, dtype=np.int32), {}

            # Morphological refinement
            combined_binary = remove_small_objects(combined_binary, min_size=20)
            combined_binary = clear_border(combined_binary)
            combined_binary = opening(combined_binary, disk(2))
            combined_binary = closing(combined_binary, disk(3))
            combined_binary = ndimage.binary_fill_holes(combined_binary)

            # Watershed segmentation
            distance = ndimage.distance_transform_edt(combined_binary)
            print("üîç Distance map max:", np.max(distance))

            threshold_abs = 0.3
            local_maxima = feature.peak_local_max(
                distance,
                min_distance=max(1, int(np.sqrt(min_cell_area / np.pi))),
                threshold_abs=threshold_abs,
                exclude_border=False
            )

            local_maxi_mask = np.zeros_like(distance, dtype=bool)
            if local_maxima.size > 0:
                local_maxi_mask[tuple(local_maxima.T)] = True
            else:
                print("‚ö†Ô∏è No local maxima ‚Äî trying h-maxima fallback...")
                local_maxi_mask = morphology.h_maxima(distance, h=0.1)
                if np.sum(local_maxi_mask) == 0:
                    print("‚ö†Ô∏è Still no markers ‚Äî trying region centroid fallback...")
                    temp_labels = measure.label(combined_binary)
                    for prop in measure.regionprops(temp_labels):
                        if prop.area > 5:
                            r, c = map(int, np.round(prop.centroid))
                            if 0 <= r < local_maxi_mask.shape[0] and 0 <= c < local_maxi_mask.shape[1]:
                                local_maxi_mask[r, c] = True
                    if np.sum(local_maxi_mask) == 0:
                        print("‚ùå Final fallback failed: No markers for watershed")
                        return np.zeros_like(distance, dtype=np.int32), {}

            print("üß© Markers for watershed:", np.sum(local_maxi_mask))
            markers = measure.label(local_maxi_mask)
            labels = watershed(-distance, markers, mask=combined_binary)
            print("üî¨ Regions from watershed:", np.max(labels))

            # ‚úÖ No merging or filtering ‚Äì use all watershed labels directly
            filtered_labels = labels.copy()

            print("‚úÖ Final region count (no filtering):", np.max(filtered_labels))
            print("[DEBUG] Binary mask pixel counts:")
            print(" - Otsu:", np.sum(binary_otsu))
            print(" - Adaptive:", np.sum(binary_adaptive))
            print(" - K-means:", np.sum(binary_kmeans))
            print(" - Felzenszwalb:", np.sum(binary_fz))
            print(" - SLIC:", np.sum(binary_slic))
            print(" - Combined:", np.sum(combined_binary))
            print(" - Green mask coverage:", np.sum(green_mask))
            print(" - Chlorophyll max:", np.max(chlorophyll))
            print(" - Distance map max:", np.max(distance))

            return filtered_labels, {
                'binary_otsu': binary_otsu,
                'binary_adaptive': binary_adaptive,
                'binary_kmeans': binary_kmeans,
                'binary_fz': binary_fz,
                'binary_slic': binary_slic,
                'combined_binary': combined_binary,
                'distance_map': distance
            }

        except Exception as e:
            print(f"Error in multi-method segmentation: {str(e)}")
            return np.zeros_like(preprocessed_data['gray_corrected'], dtype=np.int32), {}

    def _merge_oversegmented_cells(self, labels, distance_map, min_distance_ratio=0.5):
        """
        Optional post-processing to merge over-segmented regions
        (based on watershed distance minima proximity)
        """
        try:
            from scipy.spatial import distance
            from skimage.segmentation import relabel_sequential

            props = measure.regionprops(labels)
            centroids = [prop.centroid for prop in props]

            # Compute pairwise distances
            dists = distance.squareform(distance.pdist(centroids))
            np.fill_diagonal(dists, np.inf)

            merge_map = {}
            merged_labels = set()

            for i, row in enumerate(dists):
                j = np.argmin(row)
                if row[j] < min_distance_ratio * np.mean(distance_map.shape):
                    li, lj = props[i].label, props[j].label
                    if li not in merged_labels and lj not in merged_labels:
                        merge_map[li] = lj
                        merged_labels.add(li)

            for li, lj in merge_map.items():
                labels[labels == li] = lj

            # Relabel to ensure sequential integers
            new_labels, _, _ = relabel_sequential(labels)
            return new_labels

        except Exception as e:
            print(f"Error in merging oversegmented cells: {str(e)}")
            return labels


    def extract_ml_features(self, labels, preprocessed_data):
        """
        Extract comprehensive features for ML analysis
        """
        try:
            original = preprocessed_data['original']
            chlorophyll = preprocessed_data['chlorophyll_enhanced']
            ndvi = preprocessed_data['ndvi']
            gci = preprocessed_data['gci']
            exg = preprocessed_data['exg']
            hsv = preprocessed_data['hsv']
            lab = preprocessed_data['lab']
            
            props = measure.regionprops(labels, intensity_image=chlorophyll)
            
            all_features = []
            
            for prop in props:
                # Basic morphological features
                area_microns = prop.area * (self.pixel_to_micron ** 2)
                perimeter = prop.perimeter * self.pixel_to_micron
                
                # Shape features
                features = {
                    # Size features
                    'area_microns_sq': area_microns,
                    'perimeter_microns': perimeter,
                    'equivalent_diameter': prop.equivalent_diameter * self.pixel_to_micron,
                    'major_axis': prop.major_axis_length * self.pixel_to_micron,
                    'minor_axis': prop.minor_axis_length * self.pixel_to_micron,
                    
                    # Shape descriptors
                    'circularity': (4 * np.pi * prop.area) / (prop.perimeter ** 2) if prop.perimeter > 0 else 0,
                    'eccentricity': prop.eccentricity,
                    'solidity': prop.solidity,
                    'aspect_ratio': prop.major_axis_length / prop.minor_axis_length if prop.minor_axis_length > 0 else 1,
                    'roundness': 4 * prop.area / (np.pi * prop.major_axis_length ** 2) if prop.major_axis_length > 0 else 0,
                    'convexity': prop.convex_area / prop.area if prop.area > 0 else 0,
                    
                    # Texture features
                    'mean_intensity': prop.mean_intensity,
                    'max_intensity': prop.max_intensity,
                    'min_intensity': prop.min_intensity,
                    'intensity_std': np.std(chlorophyll[labels == prop.label]),
                    
                    # Moments
                    'hu_moment_1': prop.moments_hu[0],
                    'hu_moment_2': prop.moments_hu[1],
                    'hu_moment_3': prop.moments_hu[2],
                }
                
                # Extract region pixels
                cell_mask = labels == prop.label
                
                # Color features from different color spaces
                if np.any(cell_mask):
                    # RGB features
                    rgb_pixels = original[cell_mask]
                    features['mean_red'] = np.mean(rgb_pixels[:, 0]) / 255.0
                    features['mean_green'] = np.mean(rgb_pixels[:, 1]) / 255.0
                    features['mean_blue'] = np.mean(rgb_pixels[:, 2]) / 255.0
                    features['std_red'] = np.std(rgb_pixels[:, 0]) / 255.0
                    features['std_green'] = np.std(rgb_pixels[:, 1]) / 255.0
                    features['std_blue'] = np.std(rgb_pixels[:, 2]) / 255.0
                    
                    # HSV features
                    hsv_pixels = hsv[cell_mask]
                    features['mean_hue'] = np.mean(hsv_pixels[:, 0])
                    features['mean_saturation'] = np.mean(hsv_pixels[:, 1])
                    features['mean_value'] = np.mean(hsv_pixels[:, 2])
                    features['std_hue'] = np.std(hsv_pixels[:, 0])
                    features['std_saturation'] = np.std(hsv_pixels[:, 1])
                    
                    # LAB features
                    lab_pixels = lab[cell_mask]
                    features['mean_lightness'] = np.mean(lab_pixels[:, 0])
                    features['mean_a'] = np.mean(lab_pixels[:, 1])
                    features['mean_b'] = np.mean(lab_pixels[:, 2])
                    
                    # Vegetation indices
                    features['mean_ndvi'] = np.mean(ndvi[cell_mask])
                    features['mean_gci'] = np.mean(gci[cell_mask])
                    features['mean_exg'] = np.mean(exg[cell_mask])
                    features['std_ndvi'] = np.std(ndvi[cell_mask])
                    
                    # Spectral features
                    features['green_red_ratio'] = features['mean_green'] / (features['mean_red'] + 1e-10)
                    features['green_blue_ratio'] = features['mean_green'] / (features['mean_blue'] + 1e-10)
                    features['chlorophyll_index'] = features['mean_green'] - 0.5 * (features['mean_red'] + features['mean_blue'])
                    
                    # Edge features
                    gray = preprocessed_data.get('gray')
                    if gray is None:
                        raise ValueError("Preprocessed data does not contain 'gray'.")
                    edges = filters.sobel(gray[cell_mask])
                    features['edge_density'] = np.mean(edges)
                    features['edge_std'] = np.std(edges)
                
                # Add region properties
                features['centroid_x'] = prop.centroid[1] * self.pixel_to_micron
                features['centroid_y'] = prop.centroid[0] * self.pixel_to_micron
                features['orientation'] = prop.orientation
                features['label'] = prop.label
                
                all_features.append(features)
            
            return pd.DataFrame(all_features)
            
        except Exception as e:
            print(f"Error in ML feature extraction: {str(e)}")
            return pd.DataFrame()
            
    def ml_classify_cells(self, features_df):
        """
        Machine learning based cell classification
        """
        if len(features_df) == 0:
            return features_df
            
        try:
            # Prepare features for ML
            feature_columns = [
                'area_microns_sq', 'circularity', 'eccentricity', 'solidity',
                'mean_intensity', 'mean_ndvi', 'mean_gci', 'chlorophyll_index',
                'green_red_ratio', 'edge_density', 'aspect_ratio', 'convexity'
            ]
            
            X = features_df[feature_columns].fillna(0)
            
            # Scale features
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            # Unsupervised clustering for cell types
            n_clusters = min(5, max(2, len(features_df) // 10))
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            clusters = kmeans.fit_predict(X_scaled)
            
            # Analyze clusters
            cluster_stats = []
            for i in range(n_clusters):
                mask = clusters == i
                if np.any(mask):
                    cluster_features = X[mask]
                    cluster_stats.append({
                        'cluster': i,
                        'mean_area': cluster_features['area_microns_sq'].mean(),
                        'mean_chlorophyll': cluster_features['chlorophyll_index'].mean(),
                        'mean_circularity': cluster_features['circularity'].mean(),
                        'count': np.sum(mask)
                    })
            
            cluster_df = pd.DataFrame(cluster_stats).sort_values('mean_area')
            
            # Assign cell types based on cluster characteristics
            cell_types = ['tiny', 'small', 'medium', 'large', 'extra_large']
            cluster_to_type = {}
            for i, row in cluster_df.iterrows():
                type_index = min(i, len(cell_types) - 1)
                cluster_to_type[row['cluster']] = cell_types[type_index]
            
            features_df['ml_cell_type'] = [cluster_to_type[c] for c in clusters]
            
            # Health classification based on multiple factors
            health_scores = np.zeros(len(features_df))
            
            # Circularity score (closer to expected is better)
            circ_score = 1 - np.abs(features_df['circularity'] - self.wolffia_params['expected_circularity'])
            
            # Chlorophyll score
            chl_score = features_df['chlorophyll_index'] / features_df['chlorophyll_index'].max()
            
            # Size score (penalize extremes)
            size_mean = features_df['area_microns_sq'].mean()
            size_std = features_df['area_microns_sq'].std()
            size_z = np.abs((features_df['area_microns_sq'] - size_mean) / size_std)
            size_score = 1 / (1 + size_z)
            
            # Combined health score
            health_scores = (circ_score * 0.3 + chl_score * 0.5 + size_score * 0.2)
            
            # Classify health
            features_df['ml_health_score'] = health_scores
            features_df['ml_health_status'] = pd.cut(
                health_scores,
                bins=[0, 0.4, 0.7, 1.0],
                labels=['stressed', 'moderate', 'healthy']
            )
            
            # Anomaly detection
            if self.anomaly_detector is not None:
                anomalies = self.anomaly_detector.fit_predict(X_scaled)
                features_df['is_anomaly'] = anomalies == -1
            
            # Growth stage prediction based on size and chlorophyll
            growth_stages = []
            for _, row in features_df.iterrows():
                if row['area_microns_sq'] < 500:
                    stage = 'daughter_frond'
                elif row['area_microns_sq'] < 2000:
                    stage = 'young'
                elif row['area_microns_sq'] < 5000:
                    stage = 'mature'
                else:
                    stage = 'mother_frond'
                growth_stages.append(stage)
            
            features_df['ml_growth_stage'] = growth_stages
            
            # Calculate feature importance
            if len(features_df) > 10:
                # Use Random Forest to determine feature importance
                y = health_scores > 0.7  # Binary classification: healthy vs not
                rf = RandomForestClassifier(n_estimators=50, random_state=42)
                rf.fit(X, y)
                
                self.feature_importance = pd.DataFrame({
                    'feature': feature_columns,
                    'importance': rf.feature_importances_
                }).sort_values('importance', ascending=False)
            
            return features_df
            
        except Exception as e:
            print(f"Error in ML classification: {str(e)}")
            return features_df
            
    def predict_biomass(self, features_df):
        """
        Advanced biomass prediction using multiple models
        """
        if len(features_df) == 0:
            return features_df
            
        try:
            # Multiple biomass estimation models
            biomass_estimates = []
            
            for _, cell in features_df.iterrows():
                # Model 1: Volume-based estimation
                # Assume ellipsoid shape
                a = cell['major_axis'] / 2  # Semi-major axis
                b = cell['minor_axis'] / 2  # Semi-minor axis
                c = b * 0.7  # Thickness approximation
                volume = (4/3) * np.pi * a * b * c
                
                # Dry weight estimation (Œºg)
                # Based on Wolffia literature: ~0.05-0.1 mg/mm¬≥
                biomass_volume = volume * 0.075 / 1000  # Convert to Œºg
                
                # Model 2: Area-based with chlorophyll adjustment
                # Base conversion: 1 mm¬≤ ‚âà 10-50 Œºg for Wolffia
                area_mm2 = cell['area_microns_sq'] / 1e6
                biomass_area = area_mm2 * 30  # Mid-range estimate
                
                # Chlorophyll adjustment
                chl_factor = 1 + (cell['chlorophyll_index'] - 0.5) * 0.5
                biomass_area *= chl_factor
                
                # Model 3: Allometric scaling
                # Biomass ‚àù Area^1.5 (based on plant scaling laws)
                biomass_allometric = 0.01 * (cell['area_microns_sq'] ** 1.5) / 1000
                
                # Model 4: ML-based prediction using health and growth stage
                health_factor = {'healthy': 1.2, 'moderate': 1.0, 'stressed': 0.8}
                stage_factor = {'daughter_frond': 0.5, 'young': 0.7, 'mature': 1.0, 'mother_frond': 1.1}
                
                ml_factor = health_factor.get(cell.get('ml_health_status', 'moderate'), 1.0)
                ml_factor *= stage_factor.get(cell.get('ml_growth_stage', 'mature'), 1.0)
                
                biomass_ml = biomass_area * ml_factor
                
                # Ensemble prediction (weighted average)
                weights = [0.2, 0.3, 0.2, 0.3]  # Weights for each model
                final_biomass = (
                    weights[0] * biomass_volume +
                    weights[1] * biomass_area +
                    weights[2] * biomass_allometric +
                    weights[3] * biomass_ml
                )
                
                biomass_estimates.append({
                    'biomass_volume_model': biomass_volume,
                    'biomass_area_model': biomass_area,
                    'biomass_allometric_model': biomass_allometric,
                    'biomass_ml_model': biomass_ml,
                    'biomass_ensemble': final_biomass,
                    'biomass_uncertainty': np.std([biomass_volume, biomass_area, biomass_allometric, biomass_ml])
                })
            
            # Add to dataframe
            biomass_df = pd.DataFrame(biomass_estimates)
            features_df = pd.concat([features_df, biomass_df], axis=1)
            
            return features_df
            
        except Exception as e:
            print(f"Error in biomass prediction: {str(e)}")
            return features_df
            
    def analyze_population_dynamics(self, time_series_results):
        """
        Analyze population dynamics and predict future growth
        """
        if len(time_series_results) < 2:
            return None
            
        try:
            # Extract time series data
            timestamps = [r['timestamp'] for r in time_series_results]
            cell_counts = [r['total_cells'] for r in time_series_results]
            biomass_totals = [r['summary']['total_biomass_ug'] for r in time_series_results]
            
            # Convert to numpy arrays
            time_points = np.arange(len(timestamps))
            counts = np.array(cell_counts)
            biomass = np.array(biomass_totals)
            
            # Fit exponential growth model: N(t) = N0 * e^(rt)
            # Take log to linearize: log(N) = log(N0) + rt
            log_counts = np.log(counts + 1)  # Add 1 to avoid log(0)
            log_biomass = np.log(biomass + 1)
            
            # Linear regression for growth rate
            from scipy import stats
            
            # Cell count growth
            slope_count, intercept_count, r_value_count, p_value_count, std_err_count = stats.linregress(time_points, log_counts)
            growth_rate_count = slope_count
            
            # Biomass growth
            slope_biomass, intercept_biomass, r_value_biomass, p_value_biomass, std_err_biomass = stats.linregress(time_points, log_biomass)
            growth_rate_biomass = slope_biomass
            
            # Calculate doubling time
            doubling_time_count = np.log(2) / growth_rate_count if growth_rate_count > 0 else np.inf
            doubling_time_biomass = np.log(2) / growth_rate_biomass if growth_rate_biomass > 0 else np.inf
            
            # Predict future values
            future_points = np.arange(len(timestamps), len(timestamps) + 5)
            predicted_counts = np.exp(intercept_count + slope_count * future_points)
            predicted_biomass = np.exp(intercept_biomass + slope_biomass * future_points)
            
            # Calculate carrying capacity using logistic model if growth slowing
            if len(counts) > 5:
                # Check for growth deceleration
                growth_rates = np.diff(counts) / counts[:-1]
                if np.mean(growth_rates[-3:]) < np.mean(growth_rates[:3]):
                    # Fit logistic model
                    from scipy.optimize import curve_fit
                    
                    def logistic(t, K, r, t0):
                        return K / (1 + np.exp(-r * (t - t0)))
                    
                    try:
                        popt, _ = curve_fit(logistic, time_points, counts, 
                                          p0=[max(counts) * 2, 0.5, len(counts) / 2],
                                          maxfev=5000)
                        carrying_capacity = popt[0]
                        logistic_rate = popt[1]
                        
                        # Predict with logistic model
                        predicted_counts_logistic = logistic(future_points, *popt)
                    except:
                        carrying_capacity = None
                        predicted_counts_logistic = None
                else:
                    carrying_capacity = None
                    predicted_counts_logistic = None
            else:
                carrying_capacity = None
                predicted_counts_logistic = None
            
            # Population health metrics
            health_percentages = []
            size_distributions = []
            
            for result in time_series_results:
                if 'cell_data' in result:
                    df = result['cell_data']
                    if 'ml_health_status' in df.columns:
                        health_dist = df['ml_health_status'].value_counts(normalize=True).to_dict()
                        health_percentages.append(health_dist.get('healthy', 0))
                    
                    size_distributions.append({
                        'mean': df['area_microns_sq'].mean(),
                        'std': df['area_microns_sq'].std(),
                        'median': df['area_microns_sq'].median()
                    })
            
            # Diversity indices
            diversity_metrics = []
            for result in time_series_results:
                if 'cell_data' in result:
                    df = result['cell_data']
                    if 'ml_cell_type' in df.columns:
                        # Shannon diversity index
                        type_counts = df['ml_cell_type'].value_counts()
                        proportions = type_counts / len(df)
                        shannon = -np.sum(proportions * np.log(proportions + 1e-10))
                        
                        # Simpson diversity index
                        simpson = 1 - np.sum(proportions ** 2)
                        
                        diversity_metrics.append({
                            'shannon': shannon,
                            'simpson': simpson,
                            'richness': len(type_counts)
                        })
            
            population_dynamics = {
                'growth_analysis': {
                    'cell_count_growth_rate': growth_rate_count,
                    'biomass_growth_rate': growth_rate_biomass,
                    'doubling_time_cells': doubling_time_count,
                    'doubling_time_biomass': doubling_time_biomass,
                    'r_squared_count': r_value_count ** 2,
                    'r_squared_biomass': r_value_biomass ** 2,
                    'carrying_capacity': carrying_capacity
                },
                'predictions': {
                    'future_time_points': future_points.tolist(),
                    'predicted_cell_counts': predicted_counts.tolist(),
                    'predicted_biomass': predicted_biomass.tolist(),
                    'predicted_counts_logistic': predicted_counts_logistic.tolist() if predicted_counts_logistic is not None else None
                },
                'population_health': {
                    'health_trend': health_percentages,
                    'size_distributions': size_distributions
                },
                'diversity': diversity_metrics,
                'alerts': self._generate_population_alerts(growth_rate_count, growth_rate_biomass, health_percentages)
            }
            
            return population_dynamics
            
        except Exception as e:
            print(f"Error in population dynamics analysis: {str(e)}")
            return None
            
    def _generate_population_alerts(self, growth_rate_count, growth_rate_biomass, health_percentages):
        """Generate alerts based on population dynamics"""
        alerts = []
        
        # Growth rate alerts
        if growth_rate_count < 0:
            alerts.append({
                'type': 'warning',
                'message': 'Population decline detected',
                'severity': 'high'
            })
        elif growth_rate_count > 1.0:
            alerts.append({
                'type': 'info',
                'message': 'Rapid population growth detected',
                'severity': 'medium'
            })
        
        # Health alerts
        if health_percentages and len(health_percentages) > 1:
            health_trend = health_percentages[-1] - health_percentages[0]
            if health_trend < -0.2:
                alerts.append({
                    'type': 'warning',
                    'message': 'Declining population health',
                    'severity': 'high'
                })
        
        # Biomass alerts
        if abs(growth_rate_biomass - growth_rate_count) > 0.5:
            alerts.append({
                'type': 'info',
                'message': 'Biomass and cell count growth rates diverging',
                'severity': 'medium'
            })
        
        return alerts
        
    def optimize_parameters(self, results_history):
        """
        Auto-optimize analysis parameters based on results
        """
        if len(results_history) < 5:
            return self.get_current_parameters()
            
        try:
            # Collect performance metrics
            segmentation_quality = []
            classification_accuracy = []
            
            for result in results_history:
                if 'cell_data' in result:
                    df = result['cell_data']
                    
                    # Segmentation quality based on shape regularity
                    if 'circularity' in df.columns:
                        # Good segmentation should have consistent circularity
                        circ_std = df['circularity'].std()
                        seg_quality = 1 / (1 + circ_std)
                        segmentation_quality.append(seg_quality)
                    
                    # Classification accuracy based on cluster separation
                    if 'ml_cell_type' in df.columns and len(df) > 10:
                        # Calculate silhouette score
                        feature_cols = ['area_microns_sq', 'chlorophyll_index', 'circularity']
                        X = df[feature_cols].fillna(0)
                        labels = pd.Categorical(df['ml_cell_type']).codes
                        
                        if len(np.unique(labels)) > 1:
                            score = silhouette_score(X, labels)
                            classification_accuracy.append(score)
            
            # Optimize parameters
            optimized_params = self.get_current_parameters()
            
            # Adjust chlorophyll threshold based on classification
            if classification_accuracy:
                mean_accuracy = np.mean(classification_accuracy)
                if mean_accuracy < 0.3:
                    # Poor separation, adjust threshold
                    self.chlorophyll_threshold *= 0.95
                elif mean_accuracy > 0.7:
                    # Good separation, can be more selective
                    self.chlorophyll_threshold *= 1.05
                
                self.chlorophyll_threshold = np.clip(self.chlorophyll_threshold, 0.3, 0.9)
                optimized_params['chlorophyll_threshold'] = self.chlorophyll_threshold
            
            # Adjust minimum cell area based on segmentation quality
            if segmentation_quality:
                mean_quality = np.mean(segmentation_quality)
                if mean_quality < 0.5:
                    # Poor segmentation, increase minimum area
                    self.wolffia_params['min_area_microns'] *= 1.1
                elif mean_quality > 0.8:
                    # Good segmentation, can decrease minimum
                    self.wolffia_params['min_area_microns'] *= 0.9
                
                self.wolffia_params['min_area_microns'] = np.clip(
                    self.wolffia_params['min_area_microns'], 50, 500
                )
                optimized_params['min_area_microns'] = self.wolffia_params['min_area_microns']
            
            return optimized_params
            
        except Exception as e:
            print(f"Error in parameter optimization: {str(e)}")
            return self.get_current_parameters()
            
    def get_current_parameters(self):
        """Get current analysis parameters"""
        return {
            'pixel_to_micron': self.pixel_to_micron,
            'chlorophyll_threshold': self.chlorophyll_threshold,
            'min_area_microns': self.wolffia_params['min_area_microns'],
            'max_area_microns': self.wolffia_params['max_area_microns'],
            'expected_circularity': self.wolffia_params['expected_circularity']
        }
        
    def analyze_single_image(self, image_path, timestamp=None, save_visualization=True):
        """
        Complete enhanced analysis pipeline for a single image
        """
        if timestamp is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
        print(f"\n{'='*60}")
        print(f"Analyzing image: {image_path}")
        print(f"Timestamp: {timestamp}")
        print(f"{'='*60}")
        
        try:
            # Advanced preprocessing
            print("Step 1: Preprocessing image...")
            preprocessed = self.advanced_preprocess_image(image_path)
            if preprocessed is None:
                print("‚ùå Failed to preprocess image")
                return None
            print("‚úÖ Preprocessing complete")
            
            # Multi-method segmentation
            print("\nStep 2: Performing segmentation...")
            labels, segmentation_methods = self.multi_method_segmentation(preprocessed)
            
            if np.max(labels) == 0:
                print("‚ùå No cells detected after segmentation!")
                return None
            
            print(f"‚úÖ Detected {np.max(labels)} potential cells")
            
            # Extract ML features
            print("\nStep 3: Extracting features...")
            features_df = self.extract_ml_features(labels, preprocessed)
            
            if len(features_df) == 0:
                print("‚ùå No features extracted!")
                return None
            print(f"‚úÖ Extracted features for {len(features_df)} cells")
            
            # ML classification
            print("\nStep 4: ML classification...")
            features_df = self.ml_classify_cells(features_df)
            print("‚úÖ Classification complete")
            
            # Biomass prediction
            print("\nStep 5: Predicting biomass...")
            features_df = self.predict_biomass(features_df)
            print("‚úÖ Biomass prediction complete")
        
        # Rest of the method remains the same...
            
            # Add traditional features for compatibility
            features_df['cell_id'] = features_df['label']
            features_df['area_microns_sq'] = features_df['area_microns_sq']
            features_df['mean_chlorophyll_intensity'] = features_df['mean_intensity']  # Use actual intensity
            features_df['biomass_estimate_ug'] = features_df['biomass_ensemble']
            features_df['health_status'] = features_df['ml_health_status'] if 'ml_health_status' in features_df.columns else 'unknown'
            features_df['cell_type'] = features_df['ml_cell_type'] if 'ml_cell_type' in features_df.columns else 'unknown'
            features_df['growth_stage'] = features_df['ml_growth_stage'] if 'ml_growth_stage' in features_df.columns else 'unknown'
            
            if 'ml_cell_type' in features_df.columns:
                cell_type_counts = features_df['ml_cell_type'].value_counts().to_dict()
                features_df['similar_cell_count'] = features_df['ml_cell_type'].map(lambda x: cell_type_counts.get(x, 0) - 1)
            
            # Add metadata
            features_df['timestamp'] = timestamp
            features_df['image_path'] = str(image_path)
            
            # Calculate comprehensive statistics
            summary = self.calculate_enhanced_stats(features_df)
            
            # Create visualizations
            visualizations = {}
            if save_visualization:
                visualizations = self.create_enhanced_visualization(
                    preprocessed, labels, features_df, segmentation_methods,
                    return_base64=True
                )
            
            # Prepare result
            result = {
                'timestamp': timestamp,
                'image_path': str(image_path),
                'cell_data': features_df,
                'summary': summary,
                'total_cells': len(features_df),
                'visualizations': visualizations,
                'segmentation_methods': {k: v.tolist() if isinstance(v, np.ndarray) else v 
                                       for k, v in segmentation_methods.items() if k != 'distance_map'},
                'ml_metrics': {
                    'feature_importance': self.feature_importance.to_dict() if self.feature_importance is not None else None,
                    'anomalies_detected': int(features_df['is_anomaly'].sum()) if 'is_anomaly' in features_df.columns else 0
                }
            }
            
            self.results_history.append(result)
            print(f"Analysis complete: {len(features_df)} cells analyzed with ML enhancement")
            
            return result
            
        except Exception as e:
            print(f"Error in enhanced analysis: {str(e)}")
            import traceback
            traceback.print_exc()
            return None
            
            
            
            
    def analyze_single_image_enhanced(self, image_path, timestamp=None, save_visualization=True, custom_params=None):
        """
        Enhanced complete analysis pipeline with all professional features
        """
        if timestamp is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        self.current_image_path = image_path
        
        print(f"\n{'='*60}")
        print(f"üî¨ BIOIMAGIN Professional Analysis")
        print(f"Image: {image_path}")
        print(f"Timestamp: {timestamp}")
        print(f"{'='*60}")
        
        try:
            # Step 1: Smart preprocessing with top-hat
            print("üìä Step 1: Smart preprocessing with enhanced top-hat filter...")
            preprocessed = self.smart_preprocess_with_tophat(image_path, auto_optimize=True)
            
            if preprocessed is None:
                print("‚ùå Failed to preprocess image")
                return None
            
            print(f"‚úÖ Preprocessing complete (Optimal top-hat size: {preprocessed['optimal_tophat_size']})")
            
            # Also run standard preprocessing for compatibility
            standard_preprocessed = self.advanced_preprocess_image(image_path)
            
            # Merge preprocessing results
            preprocessed.update({
                'hsv': standard_preprocessed['hsv'],
                'lab': standard_preprocessed['lab'],
                'ndvi': standard_preprocessed['ndvi'],
                'gci': standard_preprocessed['gci'],
                'exg': standard_preprocessed['exg'],
                'green_mask': standard_preprocessed['green_mask']
            })
            
            # Step 2: Enhanced segmentation with custom sensitivity
            print("\nüîç Step 2: Multi-method segmentation with ML optimization...")
            
            # Apply custom parameters if provided
            if custom_params:
                min_area = custom_params.get('min_cell_area', 30)
                max_area = custom_params.get('max_cell_area', 8000)
            else:
                min_area = 30
                max_area = 8000
            
            labels, segmentation_methods = self.multi_method_segmentation(
                preprocessed, 
                min_cell_area=min_area, 
                max_cell_area=max_area
            )
            
            if np.max(labels) == 0:
                print("‚ùå No cells detected! Trying with reduced sensitivity...")
                # Try with more lenient parameters
                labels, segmentation_methods = self.multi_method_segmentation(
                    preprocessed, 
                    min_cell_area=20, 
                    max_cell_area=10000
                )
            
            print(f"‚úÖ Detected {np.max(labels)} cells")
            
            # Step 3: Spectral analysis
            print("\nüåà Step 3: Spectral analysis and chlorophyll quantification...")
            spectral_df, spectral_viz = self.analyze_chlorophyll_spectrum(
                preprocessed['original'], 
                labels
            )
            
            print(f"‚úÖ Spectral analysis complete")
            print(f"   Total chlorophyll: {spectral_df['total_chlorophyll_ug'].sum():.2f} Œºg")
            
            # Step 4: ML feature extraction
            print("\nü§ñ Step 4: ML feature extraction and classification...")
            features_df = self.extract_ml_features(labels, preprocessed)
            
            # Add spectral features
            features_df = features_df.merge(
                spectral_df[['cell_id', 'chlorophyll_concentration', 'total_chlorophyll_ug', 
                            'spectral_health', 'evi_mean', 'gli_mean', 'tgi_mean']],
                left_on='label', right_on='cell_id', how='left'
            )
            
            print(f"‚úÖ Extracted features for {len(features_df)} cells")
            
            # Step 5: ML classification with spectral data
            print("\nüß† Step 5: Advanced ML classification...")
            features_df = self.ml_classify_cells(features_df)
            
            # Combine health assessments
            features_df['combined_health'] = features_df.apply(
                lambda row: self._combine_health_assessments(
                    row.get('ml_health_status', 'unknown'),
                    row.get('spectral_health', 'unknown')
                ), axis=1
            )
            
            print("‚úÖ ML classification complete")
            
            # Step 6: Enhanced biomass prediction
            print("\n‚öñÔ∏è Step 6: Multi-model biomass prediction...")
            features_df = self.predict_biomass(features_df)
            
            # Add chlorophyll-based biomass
            features_df['biomass_chlorophyll'] = features_df['total_chlorophyll_ug'] * 8.5  # Empirical factor
            
            # Update ensemble to include chlorophyll model
            features_df['biomass_ensemble_enhanced'] = (
                features_df['biomass_ensemble'] * 0.7 +
                features_df['biomass_chlorophyll'] * 0.3
            )
            
            print("‚úÖ Biomass prediction complete")
            
            # Step 7: Cell tracking preparation
            print("\nüéØ Step 7: Preparing cell tracking data...")
            
            # Add tracking features
            features_df['tracking_id'] = features_df['label'].astype(str) + '_' + timestamp
            features_df['timestamp'] = timestamp
            features_df['image_path'] = str(image_path)
            
            # Calculate green cells
            green_cells = features_df[
                (features_df['chlorophyll_concentration'] > 10) &  # Œºg/cm¬≤
                (features_df['spectral_health'].isin(['healthy', 'very_healthy']))
            ]
            
            print(f"‚úÖ Identified {len(green_cells)} green cells")
            
            # Step 8: Generate comprehensive visualizations
            if save_visualization:
                print("\nüé® Step 8: Creating enhanced visualizations...")
                
                visualizations = self.create_enhanced_visualization(
                    preprocessed, labels, features_df, segmentation_methods,
                    return_base64=True
                )
                
                # Add spectral visualization
                visualizations['spectral_analysis'] = spectral_viz
                
                # Add top-hat visualization
                tophat_viz = self._create_tophat_visualization(preprocessed)
                visualizations['tophat_analysis'] = tophat_viz
                
                print("‚úÖ Visualizations complete")
            else:
                visualizations = {}
            
            # Calculate comprehensive statistics
            summary = self.calculate_professional_stats(features_df, spectral_df)
            
            # Generate reports
            spectral_report = self.generate_spectral_report(spectral_df)
            
            # Prepare final result
            result = {
                'timestamp': timestamp,
                'image_path': str(image_path),
                'cell_data': features_df,
                'spectral_data': spectral_df,
                'summary': summary,
                'total_cells': len(features_df),
                'green_cells': len(green_cells),
                'visualizations': visualizations,
                'segmentation_methods': {k: v.tolist() if isinstance(v, np.ndarray) else v 
                                    for k, v in segmentation_methods.items() if k != 'distance_map'},
                'ml_metrics': {
                    'feature_importance': self.feature_importance.to_dict() if self.feature_importance is not None else None,
                    'anomalies_detected': int(features_df['is_anomaly'].sum()) if 'is_anomaly' in features_df.columns else 0,
                    'model_confidence': self._calculate_model_confidence(features_df)
                },
                'spectral_report': spectral_report,
                'preprocessing_params': {
                    'tophat_size': preprocessed['optimal_tophat_size'],
                    'segmentation_sensitivity': 'adaptive'
                }
            }
            
            # Store for history
            self.results_history.append(result)
            
            print(f"\n{'='*60}")
            print(f"‚úÖ ANALYSIS COMPLETE")
            print(f"   Total cells: {len(features_df)}")
            print(f"   Green cells: {len(green_cells)}")
            print(f"   Total chlorophyll: {spectral_df['total_chlorophyll_ug'].sum():.2f} Œºg")
            print(f"   Total biomass: {features_df['biomass_ensemble_enhanced'].sum():.2f} Œºg")
            print(f"{'='*60}\n")
            
            return result
            
        except Exception as e:
            print(f"‚ùå Error in enhanced analysis: {str(e)}")
            import traceback
            traceback.print_exc()
            return None

    def _combine_health_assessments(self, ml_health, spectral_health):
        """Combine ML and spectral health assessments"""
        health_scores = {
            'healthy': 3, 'very_healthy': 4,
            'moderate': 2, 'stressed': 1, 'unknown': 0
        }
        
        ml_score = health_scores.get(ml_health, 0)
        spectral_score = health_scores.get(spectral_health, 0)
        
        combined_score = (ml_score + spectral_score) / 2
        
        if combined_score >= 3.5:
            return 'very_healthy'
        elif combined_score >= 2.5:
            return 'healthy'
        elif combined_score >= 1.5:
            return 'moderate'
        else:
            return 'stressed'

    def _calculate_model_confidence(self, features_df):
        """Calculate overall model confidence"""
        if len(features_df) == 0:
            return 0
        
        # Factors affecting confidence
        factors = []
        
        # 1. Anomaly rate
        anomaly_rate = features_df['is_anomaly'].sum() / len(features_df) if 'is_anomaly' in features_df.columns else 0
        factors.append(1 - anomaly_rate)
        
        # 2. Health score consistency
        if 'ml_health_score' in features_df.columns:
            health_std = features_df['ml_health_score'].std()
            factors.append(1 / (1 + health_std))
        
        # 3. Biomass uncertainty
        if 'biomass_uncertainty' in features_df.columns:
            mean_uncertainty = features_df['biomass_uncertainty'].mean()
            mean_biomass = features_df['biomass_ensemble'].mean()
            uncertainty_ratio = mean_uncertainty / (mean_biomass + 1e-10)
            factors.append(1 - min(uncertainty_ratio, 1))
        
        # 4. Segmentation quality
        circularity_std = features_df['circularity'].std()
        factors.append(1 / (1 + circularity_std))
        
        return float(np.mean(factors))

    def _create_tophat_visualization(self, preprocessed):
        """Create visualization of top-hat filtering results"""
        fig, axes = plt.subplots(2, 2, figsize=(10, 10))
        
        axes[0, 0].imshow(preprocessed['original'])
        axes[0, 0].set_title('Original Image')
        axes[0, 0].axis('off')
        
        axes[0, 1].imshow(preprocessed['tophat_enhanced'], cmap='gray')
        axes[0, 1].set_title(f'Top-hat Enhanced (size={preprocessed["optimal_tophat_size"]})')
        axes[0, 1].axis('off')
        
        axes[1, 0].imshow(preprocessed['green_enhanced'], cmap='Greens')
        axes[1, 0].set_title('Enhanced Green Channel')
        axes[1, 0].axis('off')
        
        axes[1, 1].imshow(preprocessed['background_mask'], cmap='gray')
        axes[1, 1].set_title('Background Mask')
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        
        buffer = BytesIO()
        plt.savefig(buffer, format='png', dpi=150, bbox_inches='tight')
        buffer.seek(0)
        tophat_viz = base64.b64encode(buffer.getvalue()).decode()
        plt.close()
        
        return tophat_viz

    def calculate_professional_stats(self, features_df, spectral_df):
        """Calculate comprehensive professional statistics"""
        if len(features_df) == 0:
            return {}
        
        stats = self.calculate_enhanced_stats(features_df)
        
        # Add spectral statistics
        stats.update({
            'total_chlorophyll_ug': float(spectral_df['total_chlorophyll_ug'].sum()),
            'mean_chlorophyll_concentration': float(spectral_df['chlorophyll_concentration'].mean()),
            'chlorophyll_per_cell': float(spectral_df['total_chlorophyll_ug'].mean()),
            'spectral_health_distribution': spectral_df['spectral_health'].value_counts().to_dict(),
            'vegetation_indices': {
                'evi': float(spectral_df['evi_mean'].mean()),
                'gli': float(spectral_df['gli_mean'].mean()),
                'tgi': float(spectral_df['tgi_mean'].mean())
            }
        })
        
        # Add green cell statistics
        green_mask = (
            (spectral_df['chlorophyll_concentration'] > 10) &
            (spectral_df['spectral_health'].isin(['healthy', 'very_healthy']))
        )
        
        stats['green_cell_count'] = int(green_mask.sum())
        stats['green_cell_percentage'] = float(green_mask.mean() * 100)
        stats['green_cell_total_chlorophyll'] = float(spectral_df.loc[green_mask, 'total_chlorophyll_ug'].sum())
        
        return stats
    
    
    
    def calculate_enhanced_stats(self, df):
        """Calculate enhanced statistics including ML metrics"""
        if len(df) == 0:
            return {}
            
        try:
            # Traditional statistics
            basic_stats = {
                'total_cell_count': len(df),
                'mean_cell_area_microns': float(df['area_microns_sq'].mean()),
                'std_cell_area_microns': float(df['area_microns_sq'].std()),
                'median_cell_area_microns': float(df['area_microns_sq'].median()),
                'total_biomass_ug': float(df['biomass_ensemble'].sum()),
                'mean_chlorophyll_intensity': float(df['chlorophyll_index'].mean())
            }
            
            # ML-enhanced statistics
            ml_stats = {
                'ml_cell_type_distribution': df['ml_cell_type'].value_counts().to_dict(),
                'ml_health_distribution': df['ml_health_status'].value_counts().to_dict() if 'ml_health_status' in df.columns else {},
                'ml_growth_stage_distribution': df['ml_growth_stage'].value_counts().to_dict() if 'ml_growth_stage' in df.columns else {},
                'anomaly_count': int(df['is_anomaly'].sum()) if 'is_anomaly' in df.columns else 0,
                'mean_health_score': float(df['ml_health_score'].mean()) if 'ml_health_score' in df.columns else 0
            }
            
            # Biomass statistics by model
            biomass_stats = {
                'biomass_volume_total': float(df['biomass_volume_model'].sum()),
                'biomass_area_total': float(df['biomass_area_model'].sum()),
                'biomass_allometric_total': float(df['biomass_allometric_model'].sum()),
                'biomass_ml_total': float(df['biomass_ml_model'].sum()),
                'mean_biomass_uncertainty': float(df['biomass_uncertainty'].mean())
            }
            
            # Spectral analysis statistics
            spectral_stats = {
                'mean_ndvi': float(df['mean_ndvi'].mean()) if 'mean_ndvi' in df.columns else 0,
                'mean_gci': float(df['mean_gci'].mean()) if 'mean_gci' in df.columns else 0,
                'mean_exg': float(df['mean_exg'].mean()) if 'mean_exg' in df.columns else 0,
                'green_red_ratio': float(df['green_red_ratio'].mean()) if 'green_red_ratio' in df.columns else 0
            }
            
            # Color analysis
            color_stats = {
                'mean_hue': float(df['mean_hue'].mean()) if 'mean_hue' in df.columns else 0,
                'mean_saturation': float(df['mean_saturation'].mean()) if 'mean_saturation' in df.columns else 0,
                'mean_lightness': float(df['mean_lightness'].mean()) if 'mean_lightness' in df.columns else 0
            }
            
            # Morphological diversity
            morph_stats = {
                'circularity_mean': float(df['circularity'].mean()),
                'circularity_std': float(df['circularity'].std()),
                'eccentricity_mean': float(df['eccentricity'].mean()),
                'solidity_mean': float(df['solidity'].mean())
            }
            
            # Quality metrics
            quality_metrics = {
                'segmentation_confidence': float(1 - df['eccentricity'].std()),  # Lower std = better
                'population_homogeneity': float(1 / (1 + df['area_microns_sq'].std() / df['area_microns_sq'].mean())),
                'health_index': float(df['ml_health_score'].mean()) if 'ml_health_score' in df.columns else 0.5
            }
            
            # Combine all statistics
            summary = {
                **basic_stats,
                **ml_stats,
                **biomass_stats,
                **spectral_stats,
                **color_stats,
                **morph_stats,
                **quality_metrics,
                'green_cell_count': len(df[df['chlorophyll_index'] > self.chlorophyll_threshold]),
                'healthy_cell_count': len(df[df['ml_health_status'] == 'healthy']) if 'ml_health_status' in df.columns else 0,
                'healthy_cell_percentage': float((df['ml_health_status'] == 'healthy').mean() * 100) if 'ml_health_status' in df.columns else 0
            }
            
            return summary
            
        except Exception as e:
            print(f"Error calculating enhanced statistics: {str(e)}")
            return {}
            
    def create_enhanced_visualization(self, preprocessed, labels, df, segmentation_methods, 
                                    output_path=None, return_base64=False):
        """Create enhanced visualizations with ML insights"""
        try:
            visualizations = {}
            
            # Main analysis figure with more panels
            fig = plt.figure(figsize=(24, 20))
            
            # Create grid layout
            gs = fig.add_gridspec(5, 4, hspace=0.3, wspace=0.3)
            
            # Original image
            ax1 = fig.add_subplot(gs[0, 0])
            ax1.imshow(preprocessed['original'])
            ax1.set_title('Original Image', fontsize=12)
            ax1.axis('off')
            
            # Enhanced chlorophyll
            ax2 = fig.add_subplot(gs[0, 1])
            ax2.imshow(preprocessed['chlorophyll_enhanced'], cmap='Greens')
            ax2.set_title('Enhanced Chlorophyll', fontsize=12)
            ax2.axis('off')
            
            # NDVI
            ax3 = fig.add_subplot(gs[0, 2])
            ax3.imshow(preprocessed['ndvi'], cmap='RdYlGn')
            ax3.set_title('NDVI', fontsize=12)
            ax3.axis('off')
            
            # Final segmentation
            ax4 = fig.add_subplot(gs[0, 3])
            ax4.imshow(preprocessed['original'])
            ax4.imshow(labels, alpha=0.4, cmap='tab20')
            ax4.set_title(f'Final Segmentation ({len(df)} cells)', fontsize=12)
            ax4.axis('off')
            
            # Segmentation methods comparison
            methods = ['binary_otsu', 'binary_adaptive', 'binary_kmeans', 'binary_fz']
            for i, method in enumerate(methods):
                if method in segmentation_methods:
                    ax = fig.add_subplot(gs[1, i])
                    ax.imshow(segmentation_methods[method], cmap='gray')
                    ax.set_title(method.replace('binary_', '').upper(), fontsize=10)
                    ax.axis('off')
            
            if len(df) > 0:
                # ML cell type distribution
                ax5 = fig.add_subplot(gs[2, 0])
                if 'ml_cell_type' in df.columns:
                    type_counts = df['ml_cell_type'].value_counts()
                    colors = plt.cm.viridis(np.linspace(0, 1, len(type_counts)))
                    ax5.pie(type_counts.values, labels=type_counts.index, autopct='%1.1f%%', colors=colors)
                    ax5.set_title('ML Cell Types')
                
                # Health status distribution
                ax6 = fig.add_subplot(gs[2, 1])
                if 'ml_health_status' in df.columns:
                    health_counts = df['ml_health_status'].value_counts()
                    colors_health = {'healthy': '#4CAF50', 'moderate': '#FFC107', 'stressed': '#F44336'}
                    pie_colors = [colors_health.get(status, 'gray') for status in health_counts.index]
                    ax6.pie(health_counts.values, labels=health_counts.index, autopct='%1.1f%%', colors=pie_colors)
                    ax6.set_title('ML Health Status')
                
                # Biomass by model
                ax7 = fig.add_subplot(gs[2, 2])
                biomass_models = ['volume', 'area', 'allometric', 'ml', 'ensemble']
                biomass_values = [
                    df['biomass_volume_model'].sum(),
                    df['biomass_area_model'].sum(),
                    df['biomass_allometric_model'].sum(),
                    df['biomass_ml_model'].sum(),
                    df['biomass_ensemble'].sum()
                ]
                bars = ax7.bar(biomass_models, biomass_values, color=plt.cm.plasma(np.linspace(0, 1, 5)))
                ax7.set_ylabel('Total Biomass (Œºg)')
                ax7.set_title('Biomass by Model')
                ax7.tick_params(axis='x', rotation=45)
                
                # Add value labels on bars
                for bar, value in zip(bars, biomass_values):
                    ax7.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01*max(biomass_values),
                            f'{value:.1f}', ha='center', va='bottom', fontsize=8)
                
                # Growth stage distribution
                ax8 = fig.add_subplot(gs[2, 3])
                if 'ml_growth_stage' in df.columns:
                    stage_counts = df['ml_growth_stage'].value_counts()
                    stage_order = ['daughter_frond', 'young', 'mature', 'mother_frond']
                    stage_counts = stage_counts.reindex(stage_order, fill_value=0)
                    colors_stage = plt.cm.Greens(np.linspace(0.3, 0.9, len(stage_counts)))
                    ax8.bar(stage_counts.index, stage_counts.values, color=colors_stage)
                    ax8.set_xlabel('Growth Stage')
                    ax8.set_ylabel('Count')
                    ax8.set_title('Growth Stage Distribution')
                    ax8.tick_params(axis='x', rotation=45)
                
                # Area vs Chlorophyll scatter with ML classification
                ax9 = fig.add_subplot(gs[3, 0:2])
                if 'ml_cell_type' in df.columns:
                    types = df['ml_cell_type'].unique()
                    colors = plt.cm.tab10(np.linspace(0, 1, len(types)))
                    for i, cell_type in enumerate(types):
                        mask = df['ml_cell_type'] == cell_type
                        ax9.scatter(df.loc[mask, 'area_microns_sq'], 
                                  df.loc[mask, 'chlorophyll_index'],
                                  c=[colors[i]], label=cell_type, alpha=0.6, s=50)
                    ax9.set_xlabel('Cell Area (Œºm¬≤)')
                    ax9.set_ylabel('Chlorophyll Index')
                    ax9.set_title('Area vs Chlorophyll by Cell Type')
                    ax9.legend()
                    ax9.grid(True, alpha=0.3)
                
                # Feature importance
                ax10 = fig.add_subplot(gs[3, 2:4])
                if self.feature_importance is not None and len(self.feature_importance) > 0:
                    top_features = self.feature_importance.head(10)
                    ax10.barh(top_features['feature'], top_features['importance'], color='skyblue')
                    ax10.set_xlabel('Importance')
                    ax10.set_title('Top 10 Feature Importances')
                    ax10.grid(True, alpha=0.3)
                
                # Spectral indices distribution
                ax11 = fig.add_subplot(gs[4, 0])
                spectral_data = {
                    'NDVI': df['mean_ndvi'].mean() if 'mean_ndvi' in df.columns else 0,
                    'GCI': df['mean_gci'].mean() if 'mean_gci' in df.columns else 0,
                    'ExG': df['mean_exg'].mean() if 'mean_exg' in df.columns else 0
                }
                ax11.bar(spectral_data.keys(), spectral_data.values(), color=['green', 'darkgreen', 'lime'])
                ax11.set_ylabel('Mean Value')
                ax11.set_title('Spectral Indices')
                ax11.grid(True, alpha=0.3)
                
                # Anomaly detection visualization
                ax12 = fig.add_subplot(gs[4, 1])
                if 'is_anomaly' in df.columns:
                    anomaly_counts = df['is_anomaly'].value_counts()
                    labels = ['Normal', 'Anomaly']
                    values = [anomaly_counts.get(False, 0), anomaly_counts.get(True, 0)]
                    colors = ['#4CAF50', '#F44336']
                    ax12.pie(values, labels=labels, autopct='%1.1f%%', colors=colors)
                    ax12.set_title('Anomaly Detection')
                
                # Biomass uncertainty
                ax13 = fig.add_subplot(gs[4, 2])
                ax13.hist(df['biomass_uncertainty'], bins=20, alpha=0.7, color='purple', edgecolor='black')
                ax13.set_xlabel('Uncertainty (Œºg)')
                ax13.set_ylabel('Frequency')
                ax13.set_title('Biomass Prediction Uncertainty')
                ax13.grid(True, alpha=0.3)
                
                # Health score distribution
                ax14 = fig.add_subplot(gs[4, 3])
                if 'ml_health_score' in df.columns:
                    ax14.hist(df['ml_health_score'], bins=20, alpha=0.7, color='teal', edgecolor='black')
                    ax14.set_xlabel('Health Score')
                    ax14.set_ylabel('Frequency')
                    ax14.set_title('ML Health Score Distribution')
                    ax14.axvline(df['ml_health_score'].mean(), color='red', linestyle='--', 
                               label=f'Mean: {df["ml_health_score"].mean():.2f}')
                    ax14.legend()
                    ax14.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            if return_base64:
                # Convert main visualization to base64
                buffer = BytesIO()
                plt.savefig(buffer, format='png', dpi=150, bbox_inches='tight')
                buffer.seek(0)
                visualizations['main_analysis'] = base64.b64encode(buffer.getvalue()).decode()
                plt.close()
                
                # Create additional ML-specific visualizations
                # 1. Cell tracking with ML classification
                fig2, ax2 = plt.subplots(figsize=(12, 10))
                ax2.imshow(preprocessed['original'])
                
                if len(df) > 0 and 'ml_health_status' in df.columns:
                    health_colors = {'healthy': 'green', 'moderate': 'yellow', 'stressed': 'red'}
                    
                    for _, cell in df.iterrows():
                        y, x = cell['centroid_y'] / self.pixel_to_micron, cell['centroid_x'] / self.pixel_to_micron
                        color = health_colors.get(cell['ml_health_status'], 'gray')
                        marker_size = np.sqrt(cell['area_microns_sq']) / 10
                        
                        # Draw cell
                        ax2.plot(x, y, 'o', color=color, markersize=marker_size, 
                               markeredgecolor='white', markeredgewidth=1, alpha=0.7)
                        
                        # Add cell ID
                        ax2.text(x, y, str(int(cell['cell_id'])), fontsize=6, color='white',
                               ha='center', va='center', weight='bold')
                        
                        # Mark anomalies
                        if 'is_anomaly' in df.columns and cell['is_anomaly']:
                            ax2.plot(x, y, 'x', color='red', markersize=15, markeredgewidth=2)
                
                ax2.set_title(f'ML Cell Classification - {len(df)} cells', fontsize=14)
                ax2.axis('off')
                
                # Add legend
                from matplotlib.patches import Patch
                legend_elements = [
                    Patch(facecolor='green', label='Healthy'),
                    Patch(facecolor='yellow', label='Moderate'),
                    Patch(facecolor='red', label='Stressed'),
                    Patch(facecolor='none', edgecolor='red', linewidth=2, label='Anomaly')
                ]
                ax2.legend(handles=legend_elements, loc='upper right')
                
                buffer2 = BytesIO()
                plt.savefig(buffer2, format='png', dpi=150, bbox_inches='tight')
                buffer2.seek(0)
                visualizations['ml_cell_tracking'] = base64.b64encode(buffer2.getvalue()).decode()
                plt.close()
                
                # 2. PCA visualization of cell features
                if len(df) > 10:
                    fig3, ax3 = plt.subplots(figsize=(10, 8))
                    
                    feature_cols = ['area_microns_sq', 'circularity', 'chlorophyll_index', 
                                  'mean_ndvi', 'edge_density', 'aspect_ratio']
                    X = df[feature_cols].fillna(0)
                    
                    # Perform PCA
                    from sklearn.decomposition import PCA
                    pca = PCA(n_components=2)
                    X_pca = pca.fit_transform(StandardScaler().fit_transform(X))
                    
                    # Color by health status
                    if 'ml_health_status' in df.columns:
                        health_colors = {'healthy': 'green', 'moderate': 'orange', 'stressed': 'red'}
                        colors = [health_colors.get(status, 'gray') for status in df['ml_health_status']]
                    else:
                        colors = 'blue'
                    
                    scatter = ax3.scatter(X_pca[:, 0], X_pca[:, 1], c=colors, alpha=0.6, s=60)
                    ax3.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')
                    ax3.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')
                    ax3.set_title('PCA of Cell Features')
                    ax3.grid(True, alpha=0.3)
                    
                    buffer3 = BytesIO()
                    plt.savefig(buffer3, format='png', dpi=150, bbox_inches='tight')
                    buffer3.seek(0)
                    visualizations['pca_analysis'] = base64.b64encode(buffer3.getvalue()).decode()
                    plt.close()
                
            elif output_path:
                plt.savefig(output_path, dpi=300, bbox_inches='tight')
                print(f"Enhanced visualization saved to: {output_path}")
                
            return visualizations
            
        except Exception as e:
            print(f"Error creating enhanced visualization: {str(e)}")
            return {}
    
    def analyze_time_series(self, image_paths, timestamps=None):
        """
        Enhanced time series analysis with ML predictions
        """
        if timestamps is None:
            timestamps = [f"t_{i:03d}" for i in range(len(image_paths))]
            
        results = []
        
        print(f"Starting enhanced time series analysis of {len(image_paths)} images...")
        
        for i, (path, timestamp) in enumerate(zip(image_paths, timestamps)):
            print(f"Processing image {i+1}/{len(image_paths)}: {timestamp}")
            result = self.analyze_single_image(path, timestamp, save_visualization=True)
            if result:
                results.append(result)
            else:
                print(f"Failed to analyze image {i+1}")
        
        # Analyze population dynamics
        if len(results) > 1:
            population_dynamics = self.analyze_population_dynamics(results)
            if population_dynamics:
                results[-1]['population_dynamics'] = population_dynamics
            
            # Create time series visualizations
            time_series_viz = self.create_enhanced_time_series_plots(results, return_base64=True)
            if time_series_viz:
                results[-1]['time_series_visualizations'] = time_series_viz
            
            # Optimize parameters based on results
            optimized_params = self.optimize_parameters(results)
            results[-1]['optimized_parameters'] = optimized_params
        
        print(f"Enhanced time series analysis complete: {len(results)} images processed")
        return results
    
    def create_enhanced_time_series_plots(self, results, return_base64=False):
        """Create enhanced time series visualizations with ML insights"""
        try:
            if len(results) < 2:
                return None
                
            visualizations = {}
            
            # Extract time series data
            timestamps = [r['timestamp'] for r in results]
            time_points = np.arange(len(timestamps))
            
            # Basic metrics
            cell_counts = [r['total_cells'] for r in results]
            biomass_totals = [r['summary']['total_biomass_ug'] for r in results]
            
            # ML metrics
            health_scores = []
            anomaly_counts = []
            diversity_indices = []
            
            for r in results:
                if 'summary' in r:
                    health_scores.append(r['summary'].get('mean_health_score', 0))
                    anomaly_counts.append(r['summary'].get('anomaly_count', 0))
                    
                    # Calculate diversity
                    if 'ml_cell_type_distribution' in r['summary']:
                        dist = r['summary']['ml_cell_type_distribution']
                        if dist:
                            total = sum(dist.values())
                            proportions = np.array(list(dist.values())) / total
                            shannon = -np.sum(proportions * np.log(proportions + 1e-10))
                            diversity_indices.append(shannon)
                        else:
                            diversity_indices.append(0)
                    else:
                        diversity_indices.append(0)
            
            # Create comprehensive figure
            fig = plt.figure(figsize=(20, 16))
            gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)
            
            # 1. Cell count and biomass trends
            ax1 = fig.add_subplot(gs[0, :])
            ax1_twin = ax1.twinx()
            
            line1 = ax1.plot(time_points, cell_counts, 'b-o', linewidth=2, markersize=8, label='Cell Count')
            line2 = ax1_twin.plot(time_points, biomass_totals, 'g-s', linewidth=2, markersize=8, label='Total Biomass')
            
            ax1.set_xlabel('Time Point')
            ax1.set_ylabel('Cell Count', color='b')
            ax1_twin.set_ylabel('Total Biomass (Œºg)', color='g')
            ax1.set_title('Population Growth Dynamics', fontsize=14)
            ax1.grid(True, alpha=0.3)
            
            # Add trend lines
            z1 = np.polyfit(time_points, cell_counts, 1)
            p1 = np.poly1d(z1)
            ax1.plot(time_points, p1(time_points), 'b--', alpha=0.5)
            
            z2 = np.polyfit(time_points, biomass_totals, 1)
            p2 = np.poly1d(z2)
            ax1_twin.plot(time_points, p2(time_points), 'g--', alpha=0.5)
            
            # Combined legend
            lines = line1 + line2
            labels = [l.get_label() for l in lines]
            ax1.legend(lines, labels, loc='upper left')
            
            ax1.set_xticks(time_points)
            ax1.set_xticklabels(timestamps, rotation=45)
            
            # 2. Health and anomaly tracking
            ax2 = fig.add_subplot(gs[1, 0])
            ax2_twin = ax2.twinx()
            
            ax2.plot(time_points, health_scores, 'teal', marker='o', linewidth=2, markersize=8)
            ax2_twin.bar(time_points, anomaly_counts, alpha=0.5, color='red', width=0.6)
            
            ax2.set_xlabel('Time Point')
            ax2.set_ylabel('Mean Health Score', color='teal')
            ax2_twin.set_ylabel('Anomaly Count', color='red')
            ax2.set_title('Population Health Monitoring')
            ax2.grid(True, alpha=0.3)
            ax2.set_xticks(time_points)
            ax2.set_xticklabels(timestamps, rotation=45)
            
            # 3. Diversity index
            ax3 = fig.add_subplot(gs[1, 1])
            ax3.plot(time_points, diversity_indices, 'purple', marker='D', linewidth=2, markersize=8)
            ax3.set_xlabel('Time Point')
            ax3.set_ylabel('Shannon Diversity Index')
            ax3.set_title('Population Diversity Over Time')
            ax3.grid(True, alpha=0.3)
            ax3.set_xticks(time_points)
            ax3.set_xticklabels(timestamps, rotation=45)
            
            # 4. Growth stage distribution over time
            ax4 = fig.add_subplot(gs[1, 2])
            growth_stages = ['daughter_frond', 'young', 'mature', 'mother_frond']
            stage_data = {stage: [] for stage in growth_stages}
            
            for r in results:
                if 'summary' in r and 'ml_growth_stage_distribution' in r['summary']:
                    dist = r['summary']['ml_growth_stage_distribution']
                    for stage in growth_stages:
                        stage_data[stage].append(dist.get(stage, 0))
                else:
                    for stage in growth_stages:
                        stage_data[stage].append(0)
            
            # Stacked area chart
            colors = plt.cm.Greens(np.linspace(0.3, 0.9, len(growth_stages)))
            ax4.stackplot(time_points, *[stage_data[stage] for stage in growth_stages], 
                         labels=growth_stages, colors=colors, alpha=0.7)
            ax4.set_xlabel('Time Point')
            ax4.set_ylabel('Cell Count')
            ax4.set_title('Growth Stage Distribution Over Time')
            ax4.legend(loc='upper left')
            ax4.grid(True, alpha=0.3)
            ax4.set_xticks(time_points)
            ax4.set_xticklabels(timestamps, rotation=45)
            
            # 5. Biomass model comparison
            ax5 = fig.add_subplot(gs[2, :])
            model_types = ['biomass_volume_total', 'biomass_area_total', 'biomass_allometric_total', 'biomass_ml_total']
            model_names = ['Volume', 'Area', 'Allometric', 'ML']
            colors = ['blue', 'green', 'orange', 'red']
            
            for model, name, color in zip(model_types, model_names, colors):
                model_data = []
                for r in results:
                    if 'summary' in r and model in r['summary']:
                        model_data.append(r['summary'][model])
                    else:
                        model_data.append(0)
                
                ax5.plot(time_points, model_data, color=color, marker='o', 
                        linewidth=2, markersize=6, label=name, alpha=0.7)
            
            ax5.set_xlabel('Time Point')
            ax5.set_ylabel('Total Biomass (Œºg)')
            ax5.set_title('Biomass Predictions by Model')
            ax5.legend()
            ax5.grid(True, alpha=0.3)
            ax5.set_xticks(time_points)
            ax5.set_xticklabels(timestamps, rotation=45)
            
            # 6. Population dynamics predictions
            if len(results) > 0 and 'population_dynamics' in results[-1]:
                pop_dyn = results[-1]['population_dynamics']
                
                if 'predictions' in pop_dyn:
                    ax6 = fig.add_subplot(gs[3, 0:2])
                    
                    # Historical data
                    ax6.plot(time_points, cell_counts, 'bo-', linewidth=2, markersize=8, label='Observed')
                    
                    # Predictions
                    future_points = pop_dyn['predictions']['future_time_points']
                    predicted_counts = pop_dyn['predictions']['predicted_cell_counts']
                    
                    ax6.plot(future_points, predicted_counts, 'r--o', linewidth=2, 
                            markersize=6, label='Predicted (Exponential)')
                    
                    # Logistic prediction if available
                    if pop_dyn['predictions']['predicted_counts_logistic']:
                        logistic_counts = pop_dyn['predictions']['predicted_counts_logistic']
                        ax6.plot(future_points, logistic_counts, 'g--s', linewidth=2, 
                                markersize=6, label='Predicted (Logistic)')
                    
                    ax6.set_xlabel('Time Point')
                    ax6.set_ylabel('Cell Count')
                    ax6.set_title('Population Growth Prediction')
                    ax6.legend()
                    ax6.grid(True, alpha=0.3)
                    
                    # Add growth metrics text
                    growth_text = f"Growth Rate: {pop_dyn['growth_analysis']['cell_count_growth_rate']:.3f}\n"
                    growth_text += f"Doubling Time: {pop_dyn['growth_analysis']['doubling_time_cells']:.1f}"
                    if pop_dyn['growth_analysis']['carrying_capacity']:
                        growth_text += f"\nCarrying Capacity: {pop_dyn['growth_analysis']['carrying_capacity']:.0f}"
                    
                    ax6.text(0.02, 0.98, growth_text, transform=ax6.transAxes, 
                            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),
                            verticalalignment='top', fontsize=10)
            
            # 7. Quality metrics
            ax7 = fig.add_subplot(gs[3, 2])
            quality_metrics = ['segmentation_confidence', 'population_homogeneity', 'health_index']
            metric_data = {metric: [] for metric in quality_metrics}
            
            for r in results:
                if 'summary' in r:
                    for metric in quality_metrics:
                        metric_data[metric].append(r['summary'].get(metric, 0))
            
            x = np.arange(len(quality_metrics))
            width = 0.2
            
            for i, (timestamp, color) in enumerate(zip(timestamps[:3], ['blue', 'green', 'red'])):
                values = [metric_data[metric][i] if i < len(metric_data[metric]) else 0 
                         for metric in quality_metrics]
                ax7.bar(x + i*width, values, width, label=timestamp, color=color, alpha=0.7)
            
            ax7.set_xlabel('Quality Metric')
            ax7.set_ylabel('Score')
            ax7.set_title('Analysis Quality Metrics')
            ax7.set_xticks(x + width)
            ax7.set_xticklabels(['Segmentation', 'Homogeneity', 'Health'], rotation=45)
            ax7.legend()
            ax7.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            if return_base64:
                buffer = BytesIO()
                plt.savefig(buffer, format='png', dpi=150, bbox_inches='tight')
                buffer.seek(0)
                visualizations['enhanced_time_series'] = base64.b64encode(buffer.getvalue()).decode()
                plt.close()
                
                # Create growth rate analysis plot
                if len(results) > 1 and 'population_dynamics' in results[-1]:
                    fig2, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))
                    
                    # Growth rates
                    growth_rates_cells = []
                    growth_rates_biomass = []
                    
                    for i in range(1, len(results)):
                        cell_rate = (cell_counts[i] - cell_counts[i-1]) / cell_counts[i-1] if cell_counts[i-1] > 0 else 0
                        biomass_rate = (biomass_totals[i] - biomass_totals[i-1]) / biomass_totals[i-1] if biomass_totals[i-1] > 0 else 0
                        growth_rates_cells.append(cell_rate * 100)
                        growth_rates_biomass.append(biomass_rate * 100)
                    
                    periods = [f"{timestamps[i]}-{timestamps[i+1]}" for i in range(len(growth_rates_cells))]
                    
                    # Bar plot of growth rates
                    x = np.arange(len(periods))
                    width = 0.35
                    
                    bars1 = ax1.bar(x - width/2, growth_rates_cells, width, label='Cell Count', color='blue', alpha=0.7)
                    bars2 = ax1.bar(x + width/2, growth_rates_biomass, width, label='Biomass', color='green', alpha=0.7)
                    
                    ax1.set_ylabel('Growth Rate (%)')
                    ax1.set_title('Period-to-Period Growth Rates')
                    ax1.set_xticks(x)
                    ax1.set_xticklabels(periods, rotation=45)
                    ax1.legend()
                    ax1.grid(True, alpha=0.3)
                    ax1.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
                    
                    # Add value labels
                    for bars in [bars1, bars2]:
                        for bar in bars:
                            height = bar.get_height()
                            ax1.text(bar.get_x() + bar.get_width()/2., height,
                                   f'{height:.1f}%', ha='center', va='bottom' if height > 0 else 'top')
                    
                    # Population alerts
                    pop_dyn = results[-1]['population_dynamics']
                    if 'alerts' in pop_dyn and pop_dyn['alerts']:
                        alert_text = "Population Alerts:\n"
                        for alert in pop_dyn['alerts']:
                            alert_text += f"‚Ä¢ {alert['message']} ({alert['severity']})\n"
                        
                        ax2.text(0.5, 0.5, alert_text, transform=ax2.transAxes,
                                bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8),
                                verticalalignment='center', horizontalalignment='center',
                                fontsize=12)
                        ax2.axis('off')
                    else:
                        ax2.text(0.5, 0.5, "No population alerts", transform=ax2.transAxes,
                                verticalalignment='center', horizontalalignment='center',
                                fontsize=12, color='green')
                        ax2.axis('off')
                    
                    plt.tight_layout()
                    
                    buffer2 = BytesIO()
                    plt.savefig(buffer2, format='png', dpi=150, bbox_inches='tight')
                    buffer2.seek(0)
                    visualizations['growth_analysis'] = base64.b64encode(buffer2.getvalue()).decode()
                    plt.close()
                
                return visualizations
            else:
                plt.savefig(f"enhanced_time_series_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png", 
                          dpi=300, bbox_inches='tight')
                plt.show()
                
        except Exception as e:
            print(f"Error creating enhanced time series plots: {str(e)}")
            return None

    def export_enhanced_results(self, result, output_path=None):
            """Export enhanced analysis results with ML insights"""
            try:
                # Define convert_types as a method-level function first
                def convert_types(obj):
                    """Convert numpy/pandas types to JSON-serializable formats"""
                    if isinstance(obj, (np.integer, np.int64, np.int32)):
                        return int(obj)
                    elif isinstance(obj, (np.floating, np.float64, np.float32)):
                        return float(obj)
                    elif isinstance(obj, np.ndarray):
                        return obj.tolist()
                    elif isinstance(obj, pd.Series):
                        return obj.to_list()
                    elif isinstance(obj, pd.DataFrame):
                        return obj.to_dict('records')
                    elif isinstance(obj, dict):
                        return {k: convert_types(v) for k, v in obj.items()}
                    elif isinstance(obj, list):
                        return [convert_types(item) for item in obj]
                    elif isinstance(obj, tuple):
                        return tuple(convert_types(item) for item in obj)
                    elif pd.isna(obj):
                        return None
                    else:
                        return obj
                
                # Prepare comprehensive JSON data
                json_data = {
                    'timestamp': result['timestamp'],
                    'image_path': str(result['image_path']),  # Ensure it's a string
                    'total_cells': int(result['total_cells']),  # Ensure it's an int
                    'summary': result['summary'],
                    'visualizations': result.get('visualizations', {}),
                    'ml_metrics': result.get('ml_metrics', {}),
                    'population_dynamics': result.get('population_dynamics', {}),
                    'optimized_parameters': result.get('optimized_parameters', {}),
                    'analysis_version': '2.0-ML-Enhanced'
                }
                
                # Convert cell data
                if 'cell_data' in result:
                    if isinstance(result['cell_data'], pd.DataFrame):
                        df = result['cell_data'].copy()
                        
                        # Select key columns for export
                        export_columns = [
                            'cell_id', 'area_microns_sq', 'perimeter_microns', 'circularity',
                            'mean_chlorophyll_intensity', 'chlorophyll_index', 'mean_ndvi', 'mean_gci', 
                            'biomass_estimate_ug', 'biomass_ensemble', 'biomass_uncertainty', 
                            'ml_cell_type', 'cell_type', 'ml_health_status', 'health_status',
                            'ml_health_score', 'ml_growth_stage', 'growth_stage', 'is_anomaly',
                            'similar_cell_count', 'centroid_x', 'centroid_y'
                        ]
                        
                        # Filter to only existing columns
                        available_columns = [col for col in export_columns if col in df.columns]
                        export_df = df[available_columns].copy()
                        
                        # Convert DataFrame to records, handling numpy types
                        cells_data = []
                        for idx, row in export_df.iterrows():
                            cell_dict = {}
                            for col in available_columns:
                                value = row[col]
                                # Convert each value individually
                                cell_dict[col] = convert_types(value)
                            cells_data.append(cell_dict)
                        
                        json_data['cells'] = cells_data
                    else:
                        # If cell_data is already a list or other format
                        json_data['cells'] = convert_types(result['cell_data'])
                
                # Apply conversion to the entire json_data structure
                json_data = convert_types(json_data)
                
                # Save to file if path provided
                if output_path:
                    with open(output_path, 'w') as f:
                        json.dump(json_data, f, indent=2)
                    print(f"Enhanced results exported to: {output_path}")
                
                return json_data
                
            except Exception as e:
                print(f"Error exporting enhanced results: {str(e)}")
                import traceback
                traceback.print_exc()
                # Return a minimal valid result on error
                return {
                    'timestamp': result.get('timestamp', ''),
                    'image_path': str(result.get('image_path', '')),
                    'total_cells': 0,
                    'error': str(e),
                    'analysis_version': '2.0-ML-Enhanced'
                }


    # Enhanced helper functions for web integration
def analyze_uploaded_image(image_path, analyzer=None):
    """
    Analyze a single uploaded image with ML enhancement
    """
    if analyzer is None:
        analyzer = WolffiaAnalyzer(pixel_to_micron_ratio=0.5, chlorophyll_threshold=0.6)
    
    try:
        result = analyzer.analyze_single_image(image_path)
        
        if result:
            # Export to JSON format
            json_result = analyzer.export_enhanced_results(result)
            return json_result
        else:
            print(f"Analysis returned no results for {image_path}")
            return None
    except Exception as e:
        print(f"Error in analyze_uploaded_image: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

def analyze_multiple_images(image_paths, timestamps=None, analyzer=None):
    """
    Analyze multiple images with enhanced ML time series analysis
    """
    if analyzer is None:
        analyzer = WolffiaAnalyzer(pixel_to_micron_ratio=0.5, chlorophyll_threshold=0.6)
    
    results = analyzer.analyze_time_series(image_paths, timestamps)
    
    if results:
        # Convert all results to JSON format
        json_results = []
        for result in results:
            json_result = analyzer.export_enhanced_results(result)
            json_results.append(json_result)
        
        return json_results
    
    return None









# improved machine learning capabilities for WolffiaAnalyzer
# Add these methods to your WolffiaAnalyzer class in bioimaging.py

def create_training_interface(self, image_path, existing_labels=None):
    """
    Create an interactive interface for manual cell annotation
    Returns annotated regions for ML training
    """
    import json

    import matplotlib.pyplot as plt
    from matplotlib.patches import Circle, Rectangle
    from matplotlib.widgets import Button, RadioButtons
    
    class AnnotationInterface:
        def __init__(self, image, analyzer):
            self.image = image
            self.analyzer = analyzer
            self.annotations = []
            self.current_class = 'healthy'
            self.fig, self.ax = plt.subplots(figsize=(12, 10))
            self.ax.imshow(image)
            
            # Setup UI elements
            self.setup_ui()
            self.connect_events()
            
        def setup_ui(self):
            # Class selection radio buttons
            rax = plt.axes([0.85, 0.7, 0.12, 0.15])
            self.radio = RadioButtons(rax, ('healthy', 'stressed', 'dead', 'debris'))
            self.radio.on_clicked(self.set_class)
            
            # Save button
            save_ax = plt.axes([0.85, 0.05, 0.1, 0.04])
            self.save_btn = Button(save_ax, 'Save')
            self.save_btn.on_clicked(self.save_annotations)
            
        def connect_events(self):
            self.cid_press = self.fig.canvas.mpl_connect('button_press_event', self.on_click)
            self.cid_key = self.fig.canvas.mpl_connect('key_press_event', self.on_key)
            
        def on_click(self, event):
            if event.inaxes != self.ax:
                return
                
            # Add annotation point
            x, y = int(event.xdata), int(event.ydata)
            self.annotations.append({
                'x': x,
                'y': y,
                'class': self.current_class,
                'timestamp': datetime.now().isoformat()
            })
            
            # Visual feedback
            circle = Circle((x, y), 10, fill=False, 
                            color=self.get_class_color(self.current_class), 
                            linewidth=2)
            self.ax.add_patch(circle)
            self.ax.text(x+12, y, self.current_class[:3], fontsize=8)
            self.fig.canvas.draw()
            
        def get_class_color(self, class_name):
            colors = {
                'healthy': 'green',
                'stressed': 'orange', 
                'dead': 'red',
                'debris': 'gray'
            }
            return colors.get(class_name, 'blue')
            
        def set_class(self, label):
            self.current_class = label
            
        def save_annotations(self, event):
            # Save annotations to file
            os.makedirs('annotations', exist_ok=True)
            filename = os.path.join('annotations', f"annotations_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
            with open(filename, 'w') as f:
                json.dump({
                    'image_path': str(self.analyzer.current_image_path),
                    'annotations': self.annotations,
                    'metadata': {
                        'pixel_to_micron': self.analyzer.pixel_to_micron,
                        'timestamp': datetime.now().isoformat()
                    }
                }, f, indent=2)
            print(f"Annotations saved to {filename}")
            plt.close()
            
        def on_key(self, event):
            if event.key == 'escape':
                plt.close()
                
    # Load and preprocess the image
    if isinstance(image_path, str):
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    else:
        image = image_path
        
    self.current_image_path = image_path
    
    # Create and show interface
    interface = AnnotationInterface(image, self)
    plt.show()
    
    return interface.annotations

def train_from_annotations(self, annotation_files):
    """
    Train ML models from manual annotations
    """
    all_features = []
    all_labels = []
    
    for ann_file in annotation_files:
        with open(ann_file, 'r') as f:
            data = json.load(f)
            
        # Load and process image
        image_path = data['image_path']
        preprocessed = self.advanced_preprocess_image(image_path)
        
        # Extract features around each annotation
        for ann in data['annotations']:
            x, y = ann['x'], ann['y']
            
            # Extract patch around annotation
            patch_size = 50
            y1 = max(0, y - patch_size)
            y2 = min(preprocessed['original'].shape[0], y + patch_size)
            x1 = max(0, x - patch_size)
            x2 = min(preprocessed['original'].shape[1], x + patch_size)
            
            # Calculate features for patch
            patch_features = self._extract_patch_features(
                preprocessed, x1, x2, y1, y2
            )
            
            all_features.append(patch_features)
            all_labels.append(ann['class'])
    
    # Train classifier
    X = np.array(all_features)
    y = np.array(all_labels)
    
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # Train enhanced classifier
    self.ml_classifier.fit(X_train, y_train)
    
    # Evaluate
    score = self.ml_classifier.score(X_test, y_test)
    print(f"Classifier accuracy: {score:.3f}")
    
    # Save trained model
    import joblib
    os.makedirs('models', exist_ok=True)
    joblib.dump(self.ml_classifier, 'models/wolffia_classifier_trained.pkl')
    
    return score

def _extract_patch_features(self, preprocessed, x1, x2, y1, y2):
    """Extract features from image patch"""
    patch_chlorophyll = preprocessed['chlorophyll_enhanced'][y1:y2, x1:x2]
    patch_ndvi = preprocessed['ndvi'][y1:y2, x1:x2]
    patch_hsv = preprocessed['hsv'][y1:y2, x1:x2]
    
    features = [
        np.mean(patch_chlorophyll),
        np.std(patch_chlorophyll),
        np.mean(patch_ndvi),
        np.std(patch_ndvi),
        np.mean(patch_hsv[:,:,0]),  # Hue
        np.mean(patch_hsv[:,:,1]),  # Saturation
        np.mean(patch_hsv[:,:,2]),  # Value
        np.percentile(patch_chlorophyll, 75),
        np.percentile(patch_chlorophyll, 25),
        # Texture features
        np.mean(np.gradient(patch_chlorophyll)[0]**2),
        np.mean(np.gradient(patch_chlorophyll)[1]**2)
    ]
    
    return features

def smart_preprocess_with_tophat(self, image_path, auto_optimize=True):
    """
    Enhanced preprocessing with intelligent top-hat filtering
    """
    # Load image
    if isinstance(image_path, str):
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    else:
        image = image_path
    
    original = image.copy()
    
    # Convert to grayscale for morphological operations
    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    
    # Auto-detect optimal structuring element size
    if auto_optimize:
        optimal_size = self._find_optimal_tophat_size(gray)
    else:
        optimal_size = 30
    
    # Multi-scale top-hat transform
    tophat_results = []
    for size in [optimal_size//2, optimal_size, optimal_size*2]:
        selem = disk(size)
        
        # White top-hat (bright features on dark background)
        white_tophat = cv2.morphologyEx(gray, cv2.MORPH_TOPHAT, selem)
        
        # Black top-hat (dark features on bright background)
        black_tophat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, selem)
        
        # Combined result
        enhanced = gray + white_tophat - black_tophat
        tophat_results.append(enhanced)
    
    # Combine multi-scale results
    combined_tophat = np.mean(tophat_results, axis=0).astype(np.uint8)
    
    # Adaptive histogram equalization on top-hat result
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    enhanced_tophat = clahe.apply(combined_tophat)
    
    # Extract enhanced green channel
    green_channel = image[:,:,1].astype(np.float32) / 255.0
    
    # Apply top-hat enhancement to green channel
    green_enhanced = green_channel * (enhanced_tophat / 255.0)
    
    # Chlorophyll-specific enhancement
    # Use color deconvolution for chlorophyll
    chlorophyll_stain = np.array([[0.460, 0.570, 0.680]])  # Chlorophyll absorption
    
    # Normalize image
    img_normalized = image.astype(np.float32) / 255.0
    img_flat = img_normalized.reshape(-1, 3)
    
    # Project onto chlorophyll vector
    chlorophyll_projection = np.dot(img_flat, chlorophyll_stain.T)
    chlorophyll_map = chlorophyll_projection.reshape(gray.shape)
    
    # Enhance chlorophyll signal
    chlorophyll_enhanced = np.clip(chlorophyll_map * 2, 0, 1)
    
    # Intelligent background removal
    background_mask = self._intelligent_background_detection(
        image, enhanced_tophat, chlorophyll_enhanced
    )
    
    # Apply background mask
    green_enhanced = green_enhanced * background_mask
    chlorophyll_enhanced = chlorophyll_enhanced * background_mask
    
    # Return enhanced results
    return {
        'original': original,
        'gray': gray,
        'tophat_enhanced': enhanced_tophat,
        'green_enhanced': green_enhanced,
        'chlorophyll_enhanced': chlorophyll_enhanced,
        'background_mask': background_mask,
        'optimal_tophat_size': optimal_size
    }

def _find_optimal_tophat_size(self, gray_image):
    """
    Automatically determine optimal structuring element size
    """
    # Estimate cell sizes using FFT
    f_transform = np.fft.fft2(gray_image)
    f_shift = np.fft.fftshift(f_transform)
    magnitude_spectrum = np.abs(f_shift)
    
    # Find dominant frequencies
    radial_profile = self._radial_profile(magnitude_spectrum)
    
    # Find peaks in radial profile
    from scipy.signal import find_peaks
    peaks, _ = find_peaks(radial_profile, height=np.max(radial_profile)*0.1)
    
    if len(peaks) > 1:
        # Estimate feature size from frequency
        dominant_freq = peaks[1]  # Skip DC component
        feature_size = gray_image.shape[0] / dominant_freq
        optimal_size = int(feature_size / 2)
    else:
        # Default fallback
        optimal_size = 30
    
    return np.clip(optimal_size, 10, 100)

def _radial_profile(self, data):
    """Calculate radial profile of 2D data"""
    center = np.array(data.shape) // 2
    y, x = np.ogrid[:data.shape[0], :data.shape[1]]
    r = np.sqrt((x - center[1])**2 + (y - center[0])**2)
    r = r.astype(int)
    
    tbin = np.bincount(r.ravel(), data.ravel())
    nr = np.bincount(r.ravel())
    radialprofile = tbin / nr
    
    return radialprofile

def _intelligent_background_detection(self, image, tophat, chlorophyll):
    """
    Intelligently detect and remove background
    """
    # Method 1: Otsu on tophat image
    _, binary1 = cv2.threshold(tophat, 0, 1, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    
    # Method 2: Chlorophyll threshold
    chlorophyll_norm = (chlorophyll - np.min(chlorophyll)) / (np.max(chlorophyll) - np.min(chlorophyll))
    binary2 = chlorophyll_norm > 0.1
    
    # Method 3: Color-based (HSV)
    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
    # Green hue range
    lower_green = np.array([35, 40, 40])
    upper_green = np.array([85, 255, 255])
    binary3 = cv2.inRange(hsv, lower_green, upper_green) > 0
    
    # Combine methods
    combined = (binary1 + binary2 + binary3) >= 2
    
    # Morphological cleanup
    combined = morphology.remove_small_holes(combined, area_threshold=100)
    combined = morphology.remove_small_objects(combined, min_size=50)
    
    return combined.astype(np.float32)

def analyze_chlorophyll_spectrum(self, image, labels):
    """
    Advanced spectral analysis for chlorophyll quantification
    """
    # RGB to approximate spectral bands
    red = image[:,:,0].astype(np.float32) / 255.0
    green = image[:,:,1].astype(np.float32) / 255.0
    blue = image[:,:,2].astype(np.float32) / 255.0
    
    # Calculate various vegetation indices
    indices = {}
    
    # 1. Enhanced Vegetation Index (EVI) approximation
    indices['evi'] = 2.5 * ((green - red) / (green + 6*red - 7.5*blue + 1))
    
    # 2. Green Leaf Index (GLI)
    indices['gli'] = (2*green - red - blue) / (2*green + red + blue + 1e-10)
    
    # 3. Chlorophyll Index Green (CIg)
    indices['cig'] = (green / (red + 1e-10)) - 1
    
    # 4. Modified Chlorophyll Absorption Ratio Index (MCARI)
    # Using RGB approximation
    indices['mcari'] = ((green - red) - 0.2 * (green - blue)) * (green / (red + 1e-10))
    
    # 5. Triangular Greenness Index (TGI)
    indices['tgi'] = green - 0.39*red - 0.61*blue
    
    # Analyze per cell
    cell_spectral_data = []
    
    for prop in measure.regionprops(labels):
        cell_mask = labels == prop.label
        
        cell_data = {
            'cell_id': prop.label,
            'area_microns_sq': prop.area * (self.pixel_to_micron ** 2),
            'centroid': prop.centroid
        }
        
        # Extract spectral features for this cell
        for idx_name, idx_map in indices.items():
            cell_values = idx_map[cell_mask]
            cell_data[f'{idx_name}_mean'] = np.mean(cell_values)
            cell_data[f'{idx_name}_std'] = np.std(cell_values)
            cell_data[f'{idx_name}_max'] = np.max(cell_values)
        
        # RGB channel statistics
        cell_data['red_mean'] = np.mean(red[cell_mask])
        cell_data['green_mean'] = np.mean(green[cell_mask])
        cell_data['blue_mean'] = np.mean(blue[cell_mask])
        
        # Chlorophyll concentration estimation
        # Based on empirical relationship with green/red ratio
        gr_ratio = cell_data['green_mean'] / (cell_data['red_mean'] + 1e-10)
        
        # Chlorophyll a+b concentration (Œºg/cm¬≤)
        # Using calibration curve: Chl = a * (G/R)^b
        a, b = 12.7, 1.5  # Empirical constants for Wolffia
        cell_data['chlorophyll_concentration'] = a * (gr_ratio ** b)
        
        # Total chlorophyll content (Œºg)
        area_cm2 = cell_data['area_microns_sq'] / 1e8  # Convert Œºm¬≤ to cm¬≤
        cell_data['total_chlorophyll_ug'] = cell_data['chlorophyll_concentration'] * area_cm2
        
        # Health classification based on spectral signature
        health_score = (
            0.3 * cell_data['evi_mean'] +
            0.2 * cell_data['gli_mean'] +
            0.2 * cell_data['cig_mean'] +
            0.3 * cell_data['tgi_mean']
        )
        
        if health_score > 0.7:
            cell_data['spectral_health'] = 'very_healthy'
        elif health_score > 0.5:
            cell_data['spectral_health'] = 'healthy'
        elif health_score > 0.3:
            cell_data['spectral_health'] = 'moderate'
        else:
            cell_data['spectral_health'] = 'stressed'
        
        cell_spectral_data.append(cell_data)
    
    # Create wavelength simulation visualization
    wavelength_viz = self._create_wavelength_visualization(
        image, labels, cell_spectral_data
    )
    
    return pd.DataFrame(cell_spectral_data), wavelength_viz

def _create_wavelength_visualization(self, image, labels, spectral_data):
    """
    Create visualization of spectral analysis
    """
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    
    # Original image with cell outlines
    ax = axes[0, 0]
    ax.imshow(image)
    ax.contour(labels > 0, colors='white', linewidths=1)
    ax.set_title('Original with Cell Boundaries')
    ax.axis('off')
    
    # Chlorophyll concentration heatmap
    ax = axes[0, 1]
    chlor_map = np.zeros_like(labels, dtype=np.float32)
    for cell in spectral_data:
        mask = labels == cell['cell_id']
        chlor_map[mask] = cell['chlorophyll_concentration']
    
    im = ax.imshow(chlor_map, cmap='Greens', vmin=0)
    ax.set_title('Chlorophyll Concentration (Œºg/cm¬≤)')
    ax.axis('off')
    plt.colorbar(im, ax=ax)
    
    # Health status map
    ax = axes[0, 2]
    health_map = np.zeros_like(labels, dtype=np.float32)
    health_colors = {
        'very_healthy': 4,
        'healthy': 3,
        'moderate': 2,
        'stressed': 1
    }
    
    for cell in spectral_data:
        mask = labels == cell['cell_id']
        health_map[mask] = health_colors.get(cell['spectral_health'], 0)
    
    im = ax.imshow(health_map, cmap='RdYlGn', vmin=0, vmax=4)
    ax.set_title('Cell Health Status')
    ax.axis('off')
    
    # Spectral indices
    indices_to_show = ['evi', 'gli', 'tgi']
    for i, idx_name in enumerate(indices_to_show):
        ax = axes[1, i]
        idx_map = np.zeros_like(labels, dtype=np.float32)
        
        for cell in spectral_data:
            mask = labels == cell['cell_id']
            idx_map[mask] = cell[f'{idx_name}_mean']
        
        im = ax.imshow(idx_map, cmap='viridis')
        ax.set_title(f'{idx_name.upper()} Index')
        ax.axis('off')
        plt.colorbar(im, ax=ax)
    
    plt.tight_layout()
    
    # Convert to base64
    buffer = BytesIO()
    plt.savefig(buffer, format='png', dpi=150, bbox_inches='tight')
    buffer.seek(0)
    wavelength_viz_base64 = base64.b64encode(buffer.getvalue()).decode()
    plt.close()
    
    return wavelength_viz_base64

def generate_spectral_report(self, spectral_df):
    """
    Generate detailed spectral analysis report
    """
    report = []
    report.append("SPECTRAL ANALYSIS REPORT")
    report.append("=" * 50)
    report.append(f"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append("")
    
    # Overall statistics
    report.append("CHLOROPHYLL STATISTICS")
    report.append("-" * 30)
    report.append(f"Total Cells Analyzed: {len(spectral_df)}")
    report.append(f"Mean Chlorophyll Concentration: {spectral_df['chlorophyll_concentration'].mean():.2f} Œºg/cm¬≤")
    report.append(f"Total Chlorophyll Content: {spectral_df['total_chlorophyll_ug'].sum():.2f} Œºg")
    report.append("")
    
    # Health distribution
    report.append("HEALTH DISTRIBUTION (Spectral)")
    report.append("-" * 30)
    health_dist = spectral_df['spectral_health'].value_counts()
    for health, count in health_dist.items():
        percentage = (count / len(spectral_df)) * 100
        report.append(f"{health}: {count} cells ({percentage:.1f}%)")
    report.append("")
    
    # Vegetation indices
    report.append("VEGETATION INDICES (Mean Values)")
    report.append("-" * 30)
    indices = ['evi', 'gli', 'cig', 'tgi']
    for idx in indices:
        mean_val = spectral_df[f'{idx}_mean'].mean()
        std_val = spectral_df[f'{idx}_mean'].std()
        report.append(f"{idx.upper()}: {mean_val:.3f} ¬± {std_val:.3f}")
    report.append("")
    
    # Wavelength approximations
    report.append("SPECTRAL BAND ANALYSIS")
    report.append("-" * 30)
    report.append(f"Red (660nm) Reflectance: {spectral_df['red_mean'].mean():.3f}")
    report.append(f"Green (550nm) Reflectance: {spectral_df['green_mean'].mean():.3f}")
    report.append(f"Blue (450nm) Reflectance: {spectral_df['blue_mean'].mean():.3f}")
    report.append(f"Green/Red Ratio: {(spectral_df['green_mean']/spectral_df['red_mean']).mean():.3f}")
    
    return "\n".join(report)









#  usage
if __name__ == "__main__":
    print("=" * 80)
    print("BIOIMAGIN - Enhanced Wolffia Analysis System v2.0")
    print("=" * 80)
    print("\nFeatures:")
    print("‚úì Multi-method segmentation (Otsu, Adaptive, K-means, Felzenszwalb, SLIC)")
    print("‚úì Machine Learning cell classification")
    print("‚úì Advanced biomass prediction models")
    print("‚úì Population dynamics analysis")
    print("‚úì Anomaly detection")
    print("‚úì Automated parameter optimization")
    print("‚úì Spectral analysis (NDVI, GCI, ExG)")
    print("‚úì Growth stage classification")
    print("‚úì Predictive growth modeling")
    print("‚úì Feature importance analysis")
    print("‚úì Real-time quality metrics")
    print("\nSystem ready for production use.")
    print("=" * 80)