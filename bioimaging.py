# bioimaging.py - Enhanced Production-Grade Wolffia Analysis System

import base64
import json
import os
import warnings
from datetime import datetime
from io import BytesIO

import cv2
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from scipy import ndimage, signal, stats
from skimage import feature, filters, measure, morphology, restoration, segmentation
from skimage.color import rgb2gray, rgb2hsv, rgb2lab
from skimage.exposure import adjust_gamma, equalize_adapthist
from skimage.filters import (
    gaussian,
    threshold_local,
    threshold_multiotsu,
    threshold_otsu,
)
from skimage.morphology import (
    closing,
    disk,
    opening,
    remove_small_objects,
    white_tophat,
)
from skimage.segmentation import clear_border, felzenszwalb, slic, watershed
from sklearn.cluster import DBSCAN, KMeans
from sklearn.decomposition import PCA
from sklearn.ensemble import IsolationForest, RandomForestClassifier
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler

matplotlib.use('Agg')
warnings.filterwarnings('ignore')

class WolffiaAnalyzer:
    """
    Production-grade Wolffia bioimage analysis pipeline with ML enhancement
    """
    
    def __init__(self, pixel_to_micron_ratio=1.0, chlorophyll_threshold=0.6):
        self.pixel_to_micron = pixel_to_micron_ratio
        self.chlorophyll_threshold = chlorophyll_threshold
        self.results_history = []
        self.ml_classifier = None
        self.anomaly_detector = None
        self.feature_importance = None
        
        # Enhanced wavelength data for spectral analysis
        self.wavelength_data = {
            'red': {'nm': 660, 'chlorophyll_absorption': 0.85},
            'green': {'nm': 550, 'chlorophyll_absorption': 0.15},
            'blue': {'nm': 450, 'chlorophyll_absorption': 0.65},
            'nir': {'nm': 850, 'chlorophyll_absorption': 0.05}  # Near-infrared if available
        }
        
        # Wolffia-specific parameters
        self.wolffia_params = {
            'min_area_microns': 30,  # Minimum area in Î¼mÂ²
            'max_area_microns': 12000,  # Maximum area in Î¼mÂ²
            'expected_circularity': 0.85,  # Expected circularity for healthy cells
            'chlorophyll_peaks': [435, 670],  # Chlorophyll a absorption peaks
            'growth_rate_range': [0.1, 0.5],  # Daily growth rate range
            'doubling_time_hours': [24, 96]  # Cell doubling time range
        }
        
        # Initialize ML components
        self._initialize_ml_components()
        
    def _initialize_ml_components(self):
        """Initialize machine learning components"""
        # Random Forest for cell classification
        self.ml_classifier = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            n_jobs=-1
        )
        
        # Isolation Forest for anomaly detection
        self.anomaly_detector = IsolationForest(
            contamination=0.1,
            random_state=42,
            n_jobs=-1
        )
        
    def assess_image_quality(self, image):
        """
        Professional AI-based image quality assessment for bioimage analysis
        Returns quality metrics and recommended preprocessing strategy
        """
        try:
            if len(image.shape) == 3:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            else:
                gray = image.copy()
            
            # 1. Contrast and brightness assessment
            contrast = np.std(gray)
            brightness = np.mean(gray)
            dynamic_range = np.ptp(gray)  # Peak-to-peak (max - min)
            
            # 2. Noise assessment using Laplacian variance
            noise_score = cv2.Laplacian(gray, cv2.CV_64F).var()
            
            # 3. Blur assessment using gradient magnitude
            grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
            grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
            blur_score = np.mean(np.sqrt(grad_x**2 + grad_y**2))
            
            # 4. Illumination uniformity
            h, w = gray.shape
            center_region = gray[h//4:3*h//4, w//4:3*w//4]
            edge_region = np.concatenate([
                gray[:h//4, :].flatten(),
                gray[3*h//4:, :].flatten(),
                gray[:, :w//4].flatten(),
                gray[:, 3*w//4:].flatten()
            ])
            illumination_uniformity = 1.0 - abs(np.mean(center_region) - np.mean(edge_region)) / 255.0
            
            # 5. Green content assessment (specific for Wolffia)
            if len(image.shape) == 3:
                hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
                green_mask = cv2.inRange(hsv, (40, 30, 30), (80, 255, 255))
                green_content = np.sum(green_mask > 0) / (h * w)
            else:
                green_content = 0.0
            
            # 6. Overall quality scoring
            quality_metrics = {
                'contrast': min(contrast / 50.0, 1.0),  # Normalize to 0-1
                'brightness_optimal': 1.0 - abs(brightness - 127.5) / 127.5,
                'dynamic_range': min(dynamic_range / 255.0, 1.0),
                'sharpness': min(noise_score / 100.0, 1.0),
                'focus': min(blur_score / 50.0, 1.0),
                'illumination': illumination_uniformity,
                'green_content': min(green_content * 5.0, 1.0),  # Boost importance
            }
            
            # Calculate overall quality score (weighted average)
            weights = {
                'contrast': 0.15,
                'brightness_optimal': 0.15,
                'dynamic_range': 0.10,
                'sharpness': 0.15,
                'focus': 0.15,
                'illumination': 0.15,
                'green_content': 0.15
            }
            
            overall_quality = sum(quality_metrics[k] * weights[k] for k in quality_metrics.keys())
            
            # Determine quality category and strategy
            if overall_quality >= 0.7:
                quality_category = 'excellent'
                strategy = 'minimal'
            elif overall_quality >= 0.5:
                quality_category = 'good'
                strategy = 'standard'
            elif overall_quality >= 0.3:
                quality_category = 'fair'
                strategy = 'enhanced'
            else:
                quality_category = 'poor'
                strategy = 'aggressive'
            
            return {
                'overall_quality': overall_quality,
                'category': quality_category,
                'strategy': strategy,
                'metrics': quality_metrics,
                'recommendations': self._get_enhancement_recommendations(quality_metrics)
            }
            
        except Exception as e:
            print(f"Error in quality assessment: {str(e)}")
            return {
                'overall_quality': 0.5,
                'category': 'unknown',
                'strategy': 'standard',
                'metrics': {},
                'recommendations': ['standard_preprocessing']
            }
    
    def _get_enhancement_recommendations(self, metrics):
        """
        Generate specific enhancement recommendations based on quality metrics
        """
        recommendations = []
        
        if metrics.get('contrast', 0) < 0.4:
            recommendations.append('contrast_enhancement')
        if metrics.get('brightness_optimal', 0) < 0.6:
            recommendations.append('brightness_correction')
        if metrics.get('sharpness', 0) < 0.5:
            recommendations.append('noise_reduction')
        if metrics.get('focus', 0) < 0.5:
            recommendations.append('sharpening')
        if metrics.get('illumination', 0) < 0.6:
            recommendations.append('illumination_correction')
        if metrics.get('green_content', 0) < 0.3:
            recommendations.append('color_enhancement')
            
        if not recommendations:
            recommendations.append('standard_preprocessing')
            
        return recommendations
    
    def intelligent_preprocess_image(self, image_path, quality_assessment=None):
        """
        Intelligent preprocessing that adapts based on image quality assessment
        """
        try:
            # Load image
            if isinstance(image_path, str):
                image = cv2.imread(image_path)
                if image is None:
                    raise ValueError(f"Could not load image from {image_path}")
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            else:
                image = image_path
            
            original = image.copy()
            
            # Assess quality if not provided
            if quality_assessment is None:
                quality_assessment = self.assess_image_quality(image)
            
            print(f"ðŸ” Image quality: {quality_assessment['category']} (score: {quality_assessment['overall_quality']:.3f})")
            print(f"ðŸ“‹ Enhancement strategy: {quality_assessment['strategy']}")
            
            # Apply adaptive preprocessing based on quality
            if quality_assessment['strategy'] == 'minimal':
                enhanced = self._minimal_preprocessing(image)
            elif quality_assessment['strategy'] == 'standard':
                enhanced = self._standard_preprocessing(image)
            elif quality_assessment['strategy'] == 'enhanced':
                enhanced = self._enhanced_preprocessing(image)
            else:  # aggressive
                enhanced = self._aggressive_preprocessing(image)
            
            # Apply specific recommendations
            for rec in quality_assessment['recommendations']:
                enhanced = self._apply_specific_enhancement(enhanced, rec, quality_assessment['metrics'])
            
            # Final quality validation
            final_quality = self.assess_image_quality(enhanced)
            improvement = final_quality['overall_quality'] - quality_assessment['overall_quality']
            
            print(f"âœ… Processing complete. Quality improvement: {improvement:+.3f}")
            
            # Prepare preprocessed data structure
            preprocessed_data = self._create_preprocessed_data_structure(enhanced, original)
            
            return preprocessed_data
            
        except Exception as e:
            print(f"âŒ Error in intelligent preprocessing: {str(e)}")
            # Fallback to standard preprocessing
            return self.advanced_preprocess_image(image_path)
    
    def _minimal_preprocessing(self, image):
        """Minimal processing for excellent quality images"""
        enhanced = image.copy()
        
        # Light denoising only
        enhanced = cv2.bilateralFilter(enhanced, 5, 20, 20)
        
        # Slight contrast enhancement
        enhanced = cv2.convertScaleAbs(enhanced, alpha=1.05, beta=5)
        
        return enhanced
    
    def _standard_preprocessing(self, image):
        """Standard processing for good quality images"""
        enhanced = image.copy()
        
        # Moderate denoising
        enhanced = cv2.bilateralFilter(enhanced, 7, 50, 50)
        
        # Standard contrast enhancement
        lab = cv2.cvtColor(enhanced, cv2.COLOR_RGB2LAB)
        lab[:,:,0] = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8)).apply(lab[:,:,0])
        enhanced = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)
        
        return enhanced
    
    def _enhanced_preprocessing(self, image):
        """Enhanced processing for fair quality images"""
        enhanced = image.copy()
        
        # Strong denoising
        enhanced = cv2.bilateralFilter(enhanced, 9, 75, 75)
        
        # Illumination correction using top-hat
        gray = cv2.cvtColor(enhanced, cv2.COLOR_RGB2GRAY)
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (50, 50))
        background = cv2.morphologyEx(gray, cv2.MORPH_TOPHAT, kernel)
        
        # Apply correction to each channel
        for i in range(3):
            enhanced[:,:,i] = cv2.add(enhanced[:,:,i], background)
        
        # Strong contrast enhancement
        lab = cv2.cvtColor(enhanced, cv2.COLOR_RGB2LAB)
        lab[:,:,0] = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(6,6)).apply(lab[:,:,0])
        enhanced = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)
        
        return enhanced
    
    def _aggressive_preprocessing(self, image):
        """Aggressive processing for poor quality images"""
        enhanced = image.copy()
        
        # Multi-step denoising
        enhanced = cv2.bilateralFilter(enhanced, 11, 100, 100)
        enhanced = cv2.medianBlur(enhanced, 5)
        
        # Illumination correction with large kernel
        gray = cv2.cvtColor(enhanced, cv2.COLOR_RGB2GRAY)
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (80, 80))
        background = cv2.morphologyEx(gray, cv2.MORPH_TOPHAT, kernel)
        
        # Apply strong correction
        for i in range(3):
            bg_scaled = (background * 1.5).astype(enhanced.dtype)
            enhanced[:,:,i] = cv2.add(enhanced[:,:,i], bg_scaled)
        
        # Aggressive contrast and brightness correction
        enhanced = cv2.convertScaleAbs(enhanced, alpha=1.2, beta=10)
        
        # Multiple CLAHE applications
        lab = cv2.cvtColor(enhanced, cv2.COLOR_RGB2LAB)
        lab[:,:,0] = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(4,4)).apply(lab[:,:,0])
        enhanced = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)
        
        # Color space enhancement for green content
        hsv = cv2.cvtColor(enhanced, cv2.COLOR_RGB2HSV)
        hsv[:,:,1] = cv2.multiply(hsv[:,:,1], 1.3)  # Increase saturation
        enhanced = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)
        
        return enhanced
    
    def _apply_specific_enhancement(self, image, recommendation, metrics):
        """Apply specific enhancement based on recommendation"""
        enhanced = image.copy()
        
        if recommendation == 'contrast_enhancement':
            enhanced = cv2.convertScaleAbs(enhanced, alpha=1.3, beta=0)
        
        elif recommendation == 'brightness_correction':
            brightness = metrics.get('brightness_optimal', 0.5)
            if brightness < 0.5:  # Too dark
                enhanced = cv2.convertScaleAbs(enhanced, alpha=1.0, beta=20)
            else:  # Too bright
                enhanced = cv2.convertScaleAbs(enhanced, alpha=1.0, beta=-20)
        
        elif recommendation == 'noise_reduction':
            enhanced = cv2.bilateralFilter(enhanced, 9, 80, 80)
        
        elif recommendation == 'sharpening':
            kernel = np.array([[-1,-1,-1],[-1,9,-1],[-1,-1,-1]])
            for i in range(3):
                enhanced[:,:,i] = cv2.filter2D(enhanced[:,:,i], -1, kernel)
        
        elif recommendation == 'illumination_correction':
            gray = cv2.cvtColor(enhanced, cv2.COLOR_RGB2GRAY)
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (60, 60))
            background = cv2.morphologyEx(gray, cv2.MORPH_TOPHAT, kernel)
            for i in range(3):
                enhanced[:,:,i] = cv2.add(enhanced[:,:,i], background)
        
        elif recommendation == 'color_enhancement':
            hsv = cv2.cvtColor(enhanced, cv2.COLOR_RGB2HSV)
            hsv[:,:,1] = cv2.multiply(hsv[:,:,1], 1.2)  # Boost saturation
            enhanced = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)
        
        return enhanced
    
    def _create_preprocessed_data_structure(self, enhanced, original):
        """Create the standard preprocessed data structure matching expected format"""
        # Convert to different color spaces
        gray = rgb2gray(enhanced)
        hsv = rgb2hsv(enhanced)
        lab = rgb2lab(enhanced)
        
        # Calculate vegetation indices
        ndvi = self._calculate_ndvi(enhanced)
        green_mask = self._create_green_mask(enhanced, hsv)
        
        # Create additional required fields to match original preprocessing format
        gray_corrected = gray.copy()  # Use gray as gray_corrected
        green_channel = enhanced[:,:,1] if len(enhanced.shape) == 3 else gray
        chlorophyll_enhanced = ndvi * green_mask  # Simplified chlorophyll proxy
        
        # Calculate additional vegetation indices
        gci = self._calculate_gci(enhanced)
        exg = self._calculate_exg(enhanced)
        
        return {
            'original': original,
            'enhanced': enhanced,
            'gray': gray,
            'gray_corrected': gray_corrected,  # Required by multi_method_segmentation
            'green_channel': green_channel,
            'chlorophyll_enhanced': chlorophyll_enhanced,  # Required by multi_method_segmentation
            'hsv': hsv,
            'lab': lab,
            'ndvi': ndvi,
            'gci': gci,
            'exg': exg,
            'green_mask': green_mask
        }
    
    def _calculate_gci(self, image):
        """Calculate Green Chlorophyll Index"""
        if len(image.shape) == 3:
            green = image[:,:,1].astype(np.float64)
            red = image[:,:,0].astype(np.float64)
            # GCI = (Green / Red) - 1
            denominator = red.copy()
            denominator[denominator == 0] = 1e-10
            gci = (green / denominator) - 1
            return np.clip(gci, -1, 5)
        else:
            return np.zeros_like(image)
    
    def _calculate_exg(self, image):
        """Calculate Excess Green Index"""
        if len(image.shape) == 3:
            r, g, b = image[:,:,0], image[:,:,1], image[:,:,2]
            # ExG = 2*G - R - B
            exg = 2 * g.astype(np.float64) - r.astype(np.float64) - b.astype(np.float64)
            return np.clip(exg, -1, 1)
        else:
            return np.zeros_like(image)
    
    def _calculate_ndvi(self, image):
        """Calculate NDVI from RGB image"""
        red = image[:,:,0].astype(np.float64)
        green = image[:,:,1].astype(np.float64)
        
        # Avoid division by zero
        denominator = red + green
        denominator[denominator == 0] = 1e-10
        
        ndvi = (green - red) / denominator
        return np.clip(ndvi, -1, 1)
    
    def _create_green_mask(self, image, hsv):
        """Create green mask for vegetation detection"""
        # HSV-based green detection
        green_lower = np.array([40, 30, 30])
        green_upper = np.array([80, 255, 255])
        
        # Convert to HSV 0-255 range
        hsv_255 = (hsv * 255).astype(np.uint8)
        mask = cv2.inRange(hsv_255, green_lower, green_upper)
        
        return mask.astype(bool)
    
    def smart_targeted_preprocess(self, image_path):
        """
        Smart preprocessing that only uses intensive quality assessment when standard methods fail
        """
        try:
            # First, try standard preprocessing
            preprocessed = self.advanced_preprocess_image(image_path)
            
            if preprocessed is None:
                print("âš ï¸ Standard preprocessing failed, trying intelligent approach...")
                return self.intelligent_preprocess_image(image_path)
            
            # Quick check: does standard preprocessing give good green detection?
            green_coverage = np.sum(preprocessed['green_mask']) / preprocessed['green_mask'].size
            
            # If green coverage is very low, the image might need special handling
            if green_coverage < 0.001:  # Less than 0.1% green
                print(f"âš ï¸ Low green coverage ({green_coverage:.4f}), trying enhanced preprocessing...")
                
                # Try enhanced preprocessing for challenging images
                enhanced_preprocessed = self.enhanced_wolffia_preprocess(image_path)
                if enhanced_preprocessed is not None:
                    return enhanced_preprocessed
            
            print(f"âœ… Standard preprocessing sufficient (green coverage: {green_coverage:.4f})")
            return preprocessed
            
        except Exception as e:
            print(f"âŒ Error in smart preprocessing: {str(e)}")
            return self.advanced_preprocess_image(image_path)
    
    def enhanced_wolffia_preprocess(self, image_path):
        """
        Enhanced preprocessing specifically designed for challenging Wolffia images
        (WhatsApp compressed, low contrast, etc.)
        """
        try:
            # Load image
            if isinstance(image_path, str):
                image = cv2.imread(image_path)
                if image is None:
                    raise ValueError(f"Could not load image from {image_path}")
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            else:
                image = image_path
            
            original = image.copy()
            print("ðŸ”§ Enhanced Wolffia preprocessing for challenging image...")
            
            # Step 1: Aggressive denoising for compressed images
            enhanced = cv2.bilateralFilter(image, 15, 100, 100)
            
            # Step 2: Color space enhancement for better green detection
            hsv = cv2.cvtColor(enhanced, cv2.COLOR_RGB2HSV)
            
            # Enhance saturation to make greens more prominent
            hsv[:,:,1] = cv2.multiply(hsv[:,:,1], 1.5)
            enhanced = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)
            
            # Step 3: Contrast enhancement with multiple methods
            lab = cv2.cvtColor(enhanced, cv2.COLOR_RGB2LAB)
            lab[:,:,0] = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8)).apply(lab[:,:,0])
            enhanced = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)
            
            # Step 4: Create more sensitive green detection
            enhanced_green_mask = self._create_sensitive_green_mask(enhanced)
            
            # Step 5: If still no green detected, try alternative detection
            if np.sum(enhanced_green_mask) < 100:  # Very low threshold
                print("âš ï¸ Still low green detection, trying alternative methods...")
                enhanced_green_mask = self._alternative_cell_detection(enhanced)
            
            # Create preprocessed data structure
            gray = rgb2gray(enhanced)
            hsv_final = rgb2hsv(enhanced)
            lab_final = rgb2lab(enhanced)
            
            # Enhanced vegetation indices
            ndvi = self._calculate_enhanced_ndvi(enhanced)
            gci = self._calculate_gci(enhanced)
            exg = self._calculate_exg(enhanced)
            
            return {
                'original': original,
                'enhanced': enhanced,
                'gray': gray,
                'gray_corrected': gray,
                'green_channel': enhanced[:,:,1],
                'chlorophyll_enhanced': ndvi * enhanced_green_mask,
                'hsv': hsv_final,
                'lab': lab_final,
                'ndvi': ndvi,
                'gci': gci,
                'exg': exg,
                'green_mask': enhanced_green_mask
            }
            
        except Exception as e:
            print(f"âŒ Error in enhanced Wolffia preprocessing: {str(e)}")
            return None
    
    def _create_sensitive_green_mask(self, image):
        """
        Create a more sensitive green mask for challenging images
        """
        # Method 1: Expanded HSV range for green
        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
        
        # More permissive green range
        green_lower = np.array([30, 20, 20])  # Expanded range
        green_upper = np.array([90, 255, 255])
        mask1 = cv2.inRange(hsv, green_lower, green_upper)
        
        # Method 2: Green channel enhancement
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        green_channel = image[:,:,1]
        
        # Green is stronger than red and blue
        green_dominance = (green_channel > image[:,:,0] * 0.8) & (green_channel > image[:,:,2] * 0.8)
        
        # Method 3: Vegetation-like colors (including yellowish-green)
        lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
        # Negative 'a' channel indicates green
        green_lab = lab[:,:,1] < 128  # Less than neutral in a-channel
        
        # Combine methods
        combined_mask = mask1.astype(bool) | green_dominance | green_lab
        
        # Clean up the mask
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
        combined_mask = cv2.morphologyEx(combined_mask.astype(np.uint8), cv2.MORPH_OPEN, kernel)
        combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_CLOSE, kernel)
        
        return combined_mask.astype(bool)
    
    def _alternative_cell_detection(self, image):
        """
        Alternative cell detection for images where green detection fails
        """
        print("ðŸ” Trying alternative cell detection methods...")
        
        # Method 1: Circular/oval objects detection
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        
        # Edge detection
        edges = cv2.Canny(gray, 50, 150)
        
        # Find contours
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        # Create mask from circular contours
        mask = np.zeros_like(gray, dtype=bool)
        
        for contour in contours:
            area = cv2.contourArea(contour)
            if 50 < area < 5000:  # Reasonable cell size range
                # Check if contour is roughly circular
                perimeter = cv2.arcLength(contour, True)
                if perimeter > 0:
                    circularity = 4 * np.pi * area / (perimeter * perimeter)
                    if circularity > 0.3:  # Reasonably circular
                        cv2.fillPoly(mask, [contour], True)
        
        # Method 2: Blob detection
        # Set up blob detector parameters
        params = cv2.SimpleBlobDetector_Params()
        params.filterByArea = True
        params.minArea = 50
        params.maxArea = 5000
        params.filterByCircularity = True
        params.minCircularity = 0.3
        params.filterByConvexity = True
        params.minConvexity = 0.5
        
        detector = cv2.SimpleBlobDetector_create(params)
        keypoints = detector.detect(gray)
        
        # Add blob areas to mask
        for kp in keypoints:
            x, y = int(kp.pt[0]), int(kp.pt[1])
            radius = int(kp.size / 2)
            cv2.circle(mask, (x, y), radius, True, -1)
        
        print(f"âœ… Alternative detection found {np.sum(mask)} pixels")
        return mask
    
    def _calculate_enhanced_ndvi(self, image):
        """
        Calculate enhanced NDVI with better handling for compressed images
        """
        if len(image.shape) == 3:
            # Use green and red channels
            green = image[:,:,1].astype(np.float64)
            red = image[:,:,0].astype(np.float64)
            
            # Smooth to reduce compression artifacts
            green = cv2.GaussianBlur(green, (3, 3), 0)
            red = cv2.GaussianBlur(red, (3, 3), 0)
            
            # Avoid division by zero
            denominator = green + red
            denominator[denominator == 0] = 1e-10
            
            ndvi = (green - red) / denominator
            return np.clip(ndvi, -1, 1)
        else:
            return np.zeros_like(image)
    
    def create_accurate_cell_visualization(self, original_image, labels, features_df=None):
        """
        Create accurate cell highlighting visualization with proper borders
        """
        try:
            # Create visualization image
            vis_img = original_image.copy()
            
            # Get unique cell labels (excluding background)
            unique_labels = np.unique(labels)
            unique_labels = unique_labels[unique_labels > 0]
            
            print(f"ðŸŽ¨ Creating visualization for {len(unique_labels)} detected cells...")
            
            # Define colors for different cell types/conditions
            colors = {
                'detected': (0, 255, 0),      # Green for detected cells
                'green_cell': (0, 255, 0),    # Bright green for green cells
                'normal_cell': (255, 255, 0), # Yellow for normal cells
                'small_cell': (255, 165, 0),  # Orange for small cells
                'large_cell': (255, 0, 255)   # Magenta for large cells
            }
            
            for label_id in unique_labels:
                # Get cell mask
                cell_mask = (labels == label_id)
                
                # Find contours of the cell
                contours, _ = cv2.findContours(
                    cell_mask.astype(np.uint8), 
                    cv2.RETR_EXTERNAL, 
                    cv2.CHAIN_APPROX_SIMPLE
                )
                
                # Determine cell color based on properties
                cell_color = colors['detected']  # Default
                
                if features_df is not None and len(features_df) >= label_id:
                    try:
                        cell_row = features_df.iloc[label_id - 1]
                        area = cell_row.get('area_microns_sq', 0)
                        is_green = cell_row.get('is_green_cell', False)
                        
                        if is_green:
                            cell_color = colors['green_cell']
                        elif area < 50:
                            cell_color = colors['small_cell']
                        elif area > 1000:
                            cell_color = colors['large_cell']
                        else:
                            cell_color = colors['normal_cell']
                    except:
                        pass
                
                # Draw cell boundaries
                for contour in contours:
                    # Draw thick border
                    cv2.drawContours(vis_img, [contour], -1, cell_color, 3)
                    
                    # Draw thinner inner border for better visibility
                    cv2.drawContours(vis_img, [contour], -1, (255, 255, 255), 1)
                
                # Add cell number/ID
                if len(contours) > 0:
                    # Get centroid of largest contour
                    largest_contour = max(contours, key=cv2.contourArea)
                    M = cv2.moments(largest_contour)
                    if M['m00'] != 0:
                        cx = int(M['m10'] / M['m00'])
                        cy = int(M['m01'] / M['m00'])
                        
                        # Draw cell ID
                        cv2.putText(vis_img, str(label_id), (cx-10, cy+5), 
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                        cv2.putText(vis_img, str(label_id), (cx-10, cy+5), 
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)
            
            # Add legend
            legend_y = 30
            cv2.putText(vis_img, f"Detected: {len(unique_labels)} cells", 
                       (10, legend_y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            cv2.putText(vis_img, f"Detected: {len(unique_labels)} cells", 
                       (10, legend_y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 1)
            
            # Add color legend
            legend_items = [
                ("Green cells", colors['green_cell']),
                ("Normal cells", colors['normal_cell']),
                ("Small cells", colors['small_cell']),
                ("Large cells", colors['large_cell'])
            ]
            
            for i, (label, color) in enumerate(legend_items):
                y_pos = vis_img.shape[0] - 120 + i * 25
                # Draw color box
                cv2.rectangle(vis_img, (10, y_pos-10), (30, y_pos+5), color, -1)
                cv2.rectangle(vis_img, (10, y_pos-10), (30, y_pos+5), (255, 255, 255), 1)
                # Draw text
                cv2.putText(vis_img, label, (35, y_pos), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
                cv2.putText(vis_img, label, (35, y_pos), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)
            
            return vis_img
            
        except Exception as e:
            print(f"âŒ Error creating cell visualization: {str(e)}")
            return original_image
    
    def _enhanced_segmentation_fallback(self, preprocessed):
        """
        Enhanced fallback segmentation methods for difficult images
        """
        print("ðŸ”§ Trying enhanced segmentation fallback methods...")
        
        try:
            # Method 1: More aggressive Otsu with morphological opening
            gray = preprocessed['gray']
            green_mask = preprocessed['green_mask']
            
            # Histogram stretching
            gray_stretched = cv2.normalize(gray, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
            
            # Multiple Otsu thresholds with lower sensitivity
            thresholds = threshold_multiotsu(gray_stretched, classes=2)
            binary = gray_stretched > thresholds[0] * 0.7  # Lower threshold
            
            # Combine with green mask
            combined = binary & green_mask
            
            # Morphological operations
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
            combined = cv2.morphologyEx(combined.astype(np.uint8), cv2.MORPH_OPEN, kernel)
            combined = cv2.morphologyEx(combined, cv2.MORPH_CLOSE, kernel)
            
            # Remove small objects
            combined = remove_small_objects(combined.astype(bool), min_size=10)
            
            if np.sum(combined) > 50:  # Found something
                # Watershed segmentation
                distance = ndimage.distance_transform_edt(combined)
                if np.max(distance) > 0:
                    # Use distance peaks as markers
                    from scipy.ndimage import maximum_filter
                    local_maxima = distance == maximum_filter(distance, size=5)
                    local_maxima = local_maxima & (distance > np.max(distance)*0.3)
                    coords = np.where(local_maxima)
                    if len(coords[0]) > 0:
                        markers = np.zeros_like(distance, dtype=int)
                        markers[coords] = range(1, len(coords[0]) + 1)
                        labels = segmentation.watershed(-distance, markers, mask=combined)
                        
                        if np.max(labels) > 0:
                            print(f"âœ… Fallback Method 1 succeeded: {np.max(labels)} regions")
                            return labels, {'fallback_otsu_watershed': True}
            
            # Method 2: Adaptive thresholding with different parameters
            gray_uint8 = (gray * 255).astype(np.uint8)
            
            for block_size in [11, 15, 25, 35]:
                for offset in [0.05, 0.1, 0.15]:
                    try:
                        adaptive = threshold_local(gray_uint8, block_size=block_size, offset=offset, method='gaussian')
                        binary_adaptive = gray_uint8 > adaptive
                        
                        # Combine with green mask
                        combined_adaptive = binary_adaptive & green_mask
                        
                        if np.sum(combined_adaptive) > 30:
                            # Clean up
                            combined_adaptive = remove_small_objects(combined_adaptive, min_size=8)
                            
                            if np.sum(combined_adaptive) > 0:
                                # Simple connected components
                                labels = measure.label(combined_adaptive)
                                
                                if np.max(labels) > 0:
                                    print(f"âœ… Fallback Method 2 succeeded: {np.max(labels)} regions (block_size={block_size}, offset={offset})")
                                    return labels, {'fallback_adaptive': {'block_size': block_size, 'offset': offset}}
                    except Exception as e:
                        continue
            
            # Method 3: Simple green-based segmentation
            if len(preprocessed['original'].shape) == 3:
                # Extract green channel and enhance
                green_channel = preprocessed['original'][:,:,1]
                green_enhanced = cv2.equalizeHist((green_channel * 255).astype(np.uint8))
                
                # Simple threshold on enhanced green
                _, green_binary = cv2.threshold(green_enhanced, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
                green_binary = green_binary.astype(bool)
                
                # Clean up
                green_binary = remove_small_objects(green_binary, min_size=5)
                
                if np.sum(green_binary) > 0:
                    labels = measure.label(green_binary)
                    if np.max(labels) > 0:
                        print(f"âœ… Fallback Method 3 succeeded: {np.max(labels)} regions (green channel)")
                        return labels, {'fallback_green_channel': True}
            
            print("âŒ All fallback methods failed")
            return np.zeros_like(gray, dtype=np.int32), {}
            
        except Exception as e:
            print(f"âŒ Error in enhanced segmentation fallback: {str(e)}")
            return np.zeros_like(preprocessed['gray'], dtype=np.int32), {}
        
    def advanced_preprocess_image(self, image_path, enhance_contrast=True, denoise=True):
        """
        Advanced preprocessing optimized for petri dish Wolffia images
        """
        try:
            # Load image
            if isinstance(image_path, str):
                image = cv2.imread(image_path)
                if image is None:
                    raise ValueError(f"Could not load image from {image_path}")
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            else:
                image = image_path
                
            original = image.copy()
            
            # Multi-scale denoising
            if denoise:
                image = restoration.denoise_bilateral(
                    image, 
                    sigma_color=0.05, 
                    sigma_spatial=15,
                    channel_axis=-1
                )
            
            # Color space conversions
            gray = rgb2gray(image)
            hsv = rgb2hsv(image)
            lab = rgb2lab(image)
            
            # Illumination correction using white top-hat
            selem = disk(30)
            background = morphology.white_tophat(gray, selem)
            gray_corrected = gray - background
            gray_corrected = np.clip(gray_corrected, 0, 1)
            
            # Enhanced chlorophyll detection using multiple color spaces
            # Extract channels
            h_channel = hsv[:, :, 0]
            s_channel = hsv[:, :, 1]
            v_channel = hsv[:, :, 2]
            l_channel = lab[:, :, 0] / 100.0
            a_channel = (lab[:, :, 1] + 128) / 255.0
            b_channel = (lab[:, :, 2] + 128) / 255.0
            
            # Green color detection in HSV (Hue: 60-140 degrees)
            green_mask_hsv = (
                (h_channel >= 60/360) & (h_channel <= 140/360) &
                (s_channel >= 0.2) & (v_channel >= 0.2)
            )
            
            # Green detection in LAB (negative a values indicate green)
            green_mask_lab = (a_channel < 0.45) & (l_channel > 0.2)
            
            # Combine masks
            green_mask = green_mask_hsv | green_mask_lab
            
            # Chlorophyll index calculation
            red_channel = image[:, :, 0] / 255.0
            green_channel = image[:, :, 1] / 255.0
            blue_channel = image[:, :, 2] / 255.0
            nir_channel = green_channel  # Approximation when NIR not available
            
            # Multiple vegetation indices
            # Normalized Difference Vegetation Index (NDVI) approximation
            ndvi = (nir_channel - red_channel) / (nir_channel + red_channel + 1e-10)
            
            # Green Chlorophyll Index (GCI)
            gci = (nir_channel / green_channel) - 1
            
            # Excess Green Index (ExG)
            exg = 2 * green_channel - red_channel - blue_channel
            
            # Combined chlorophyll index
            chlorophyll_enhanced = np.maximum.reduce([ndvi, gci, exg])
            chlorophyll_enhanced = np.clip(chlorophyll_enhanced, 0, 1)
            
            # Apply green mask to focus on Wolffia
            chlorophyll_enhanced = np.where(green_mask, chlorophyll_enhanced, chlorophyll_enhanced * 0.3)
            green_channel = np.where(green_mask, green_channel, 0)
            gray = np.where(green_mask, gray, 0)
            
            # Adaptive contrast enhancement
            if enhance_contrast:
                gray_corrected = equalize_adapthist(gray_corrected, clip_limit=0.03)
                chlorophyll_enhanced = equalize_adapthist(chlorophyll_enhanced, clip_limit=0.03)
            
            # Edge-preserving smoothing
            gray_smoothed = filters.median(gray_corrected, disk(2))
            chlorophyll_smoothed = filters.median(chlorophyll_enhanced, disk(2))
            
            return {
                'original': original,
                'gray': gray_smoothed,
                'gray_corrected': gray_corrected,
                'green_channel': green_channel * green_mask,
                'chlorophyll_enhanced': chlorophyll_smoothed,
                'hsv': hsv,
                'lab': lab,
                'green_mask': green_mask,
                'ndvi': ndvi,
                'gci': gci,
                'exg': exg
            }
            
        except Exception as e:
            print(f"Error in advanced preprocessing: {str(e)}")
            return None
            
    def multi_method_segmentation(self, preprocessed_data, min_cell_area=30, max_cell_area=8000):
        """
        Robust multi-method segmentation tuned for broad sensitivity
        """
        try:
            gray = preprocessed_data['gray_corrected']
            chlorophyll = preprocessed_data['chlorophyll_enhanced']
            green_mask = preprocessed_data['green_mask']

            # Method 1: Multi-Otsu thresholding
            thresholds = threshold_multiotsu(gray, classes=3)
            regions = np.digitize(gray, bins=thresholds)
            binary_otsu = (regions >= 1) & green_mask

            # Method 2: Adaptive thresholding with multiple block sizes
            block_sizes = [21, 35, 51]
            adaptive_masks = [(gray > threshold_local(gray, block_size=b, method='gaussian', offset=0.02)) & green_mask for b in block_sizes]
            binary_adaptive = np.logical_or.reduce(adaptive_masks)

            # Method 3: K-means clustering
            lab = preprocessed_data['lab']
            pixels = lab.reshape(-1, 3)
            sample_indices = np.random.choice(len(pixels), min(10000, len(pixels)), replace=False)
            kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
            kmeans.fit(pixels[sample_indices])
            labels_kmeans = kmeans.predict(pixels).reshape(gray.shape)
            green_clusters = [i for i in range(5) if np.mean(green_mask[labels_kmeans == i]) > 0.5]
            binary_kmeans = np.isin(labels_kmeans, green_clusters)

            # Method 4: Felzenszwalb
            segments_fz = felzenszwalb(preprocessed_data['original'], scale=100, sigma=0.5, min_size=50)
            binary_fz = np.zeros_like(gray, dtype=bool)
            for sid in np.unique(segments_fz):
                mask = segments_fz == sid
                if np.mean(green_mask[mask]) > 0.7 and np.sum(mask) >= min_cell_area:
                    binary_fz |= mask

            # Method 5: SLIC
            segments_slic = slic(preprocessed_data['original'], n_segments=500, compactness=10, start_label=1)
            binary_slic = np.zeros_like(gray, dtype=bool)
            for sid in np.unique(segments_slic):
                mask = segments_slic == sid
                if np.mean(chlorophyll[mask]) > 0.2 and np.sum(mask) >= min_cell_area:
                    binary_slic |= mask

            # Lenient combination
            combined_binary = (
                binary_otsu.astype(int) * 2 +
                binary_adaptive.astype(int) +
                binary_kmeans.astype(int) * 2 +
                binary_fz.astype(int) +
                binary_slic.astype(int)
            ) >= 2

            if np.sum(combined_binary) < 10:
                print("âš ï¸ Combined mask too weak. Trying intelligent fallback cascade...")
                
                # Fallback 1: Reduced chlorophyll threshold
                fallback1 = green_mask & (chlorophyll > 0.05)  # Lower threshold
                if np.sum(fallback1) > 10:
                    combined_binary = fallback1
                    print(f"âœ… Fallback 1 succeeded with {np.sum(combined_binary)} pixels (low chlorophyll)")
                else:
                    # Fallback 2: Pure green mask with morphological opening
                    fallback2 = opening(green_mask, disk(1))
                    if np.sum(fallback2) > 5:
                        combined_binary = fallback2
                        print(f"âœ… Fallback 2 succeeded with {np.sum(combined_binary)} pixels (pure green)")
                    else:
                        # Fallback 3: Aggressive NDVI-based detection
                        if 'ndvi' in preprocessed_data:
                            ndvi = preprocessed_data['ndvi']
                            fallback3 = ndvi > 0.1  # Very low NDVI threshold
                            if np.sum(fallback3) > 5:
                                combined_binary = fallback3
                                print(f"âœ… Fallback 3 succeeded with {np.sum(combined_binary)} pixels (NDVI-based)")
                            else:
                                # Fallback 4: Simple brightness-based detection
                                gray_normalized = (gray - np.min(gray)) / (np.max(gray) - np.min(gray))
                                fallback4 = gray_normalized > 0.3  # Low brightness threshold
                                if np.sum(fallback4) > 3:
                                    combined_binary = fallback4
                                    print(f"âœ… Fallback 4 succeeded with {np.sum(combined_binary)} pixels (brightness-based)")
                                else:
                                    print("âŒ All intelligent fallbacks failed: no viable regions")
                                    return np.zeros_like(gray, dtype=np.int32), {'failed_fallbacks': True}

            # Morphological refinement
            combined_binary = remove_small_objects(combined_binary, min_size=20)
            combined_binary = clear_border(combined_binary)
            combined_binary = opening(combined_binary, disk(2))
            combined_binary = closing(combined_binary, disk(3))
            combined_binary = ndimage.binary_fill_holes(combined_binary)

            # Watershed segmentation
            distance = ndimage.distance_transform_edt(combined_binary)
            print("ðŸ” Distance map max:", np.max(distance))

            threshold_abs = 0.3
            local_maxima = feature.peak_local_max(
                distance,
                min_distance=max(1, int(np.sqrt(min_cell_area / np.pi))),
                threshold_abs=threshold_abs,
                exclude_border=False
            )

            local_maxi_mask = np.zeros_like(distance, dtype=bool)
            if local_maxima.size > 0:
                local_maxi_mask[tuple(local_maxima.T)] = True
            else:
                print("âš ï¸ No local maxima â€” trying h-maxima fallback...")
                local_maxi_mask = morphology.h_maxima(distance, h=0.1)
                if np.sum(local_maxi_mask) == 0:
                    print("âš ï¸ Still no markers â€” trying region centroid fallback...")
                    temp_labels = measure.label(combined_binary)
                    for prop in measure.regionprops(temp_labels):
                        if prop.area > 5:
                            r, c = map(int, np.round(prop.centroid))
                            if 0 <= r < local_maxi_mask.shape[0] and 0 <= c < local_maxi_mask.shape[1]:
                                local_maxi_mask[r, c] = True
                    if np.sum(local_maxi_mask) == 0:
                        print("âŒ Final fallback failed: No markers for watershed")
                        return np.zeros_like(distance, dtype=np.int32), {}

            print("ðŸ§© Markers for watershed:", np.sum(local_maxi_mask))
            markers = measure.label(local_maxi_mask)
            labels = watershed(-distance, markers, mask=combined_binary)
            print("ðŸ”¬ Regions from watershed:", np.max(labels))

            # âœ… No merging or filtering â€“ use all watershed labels directly
            filtered_labels = labels.copy()

            print("âœ… Final region count (no filtering):", np.max(filtered_labels))
            print("[DEBUG] Binary mask pixel counts:")
            print(" - Otsu:", np.sum(binary_otsu))
            print(" - Adaptive:", np.sum(binary_adaptive))
            print(" - K-means:", np.sum(binary_kmeans))
            print(" - Felzenszwalb:", np.sum(binary_fz))
            print(" - SLIC:", np.sum(binary_slic))
            print(" - Combined:", np.sum(combined_binary))
            print(" - Green mask coverage:", np.sum(green_mask))
            print(" - Chlorophyll max:", np.max(chlorophyll))
            print(" - Distance map max:", np.max(distance))

            return filtered_labels, {
                'binary_otsu': binary_otsu,
                'binary_adaptive': binary_adaptive,
                'binary_kmeans': binary_kmeans,
                'binary_fz': binary_fz,
                'binary_slic': binary_slic,
                'combined_binary': combined_binary,
                'distance_map': distance
            }

        except Exception as e:
            print(f"Error in multi-method segmentation: {str(e)}")
            return np.zeros_like(preprocessed_data['gray_corrected'], dtype=np.int32), {}

    def _merge_oversegmented_cells(self, labels, distance_map, min_distance_ratio=0.5):
        """
        Optional post-processing to merge over-segmented regions
        (based on watershed distance minima proximity)
        """
        try:
            from scipy.spatial import distance
            from skimage.segmentation import relabel_sequential

            props = measure.regionprops(labels)
            centroids = [prop.centroid for prop in props]

            # Compute pairwise distances
            dists = distance.squareform(distance.pdist(centroids))
            np.fill_diagonal(dists, np.inf)

            merge_map = {}
            merged_labels = set()

            for i, row in enumerate(dists):
                j = np.argmin(row)
                if row[j] < min_distance_ratio * np.mean(distance_map.shape):
                    li, lj = props[i].label, props[j].label
                    if li not in merged_labels and lj not in merged_labels:
                        merge_map[li] = lj
                        merged_labels.add(li)

            for li, lj in merge_map.items():
                labels[labels == li] = lj

            # Relabel to ensure sequential integers
            new_labels, _, _ = relabel_sequential(labels)
            return new_labels

        except Exception as e:
            print(f"Error in merging oversegmented cells: {str(e)}")
            return labels


    def extract_ml_features(self, labels, preprocessed_data):
        """
        Extract comprehensive features for ML analysis
        """
        try:
            original = preprocessed_data['original']
            chlorophyll = preprocessed_data['chlorophyll_enhanced']
            ndvi = preprocessed_data['ndvi']
            gci = preprocessed_data['gci']
            exg = preprocessed_data['exg']
            hsv = preprocessed_data['hsv']
            lab = preprocessed_data['lab']
            
            props = measure.regionprops(labels, intensity_image=chlorophyll)
            
            all_features = []
            
            for prop in props:
                # Basic morphological features
                area_microns = prop.area * (self.pixel_to_micron ** 2)
                perimeter = prop.perimeter * self.pixel_to_micron
                
                # Shape features
                features = {
                    # Size features
                    'area_microns_sq': area_microns,
                    'perimeter_microns': perimeter,
                    'equivalent_diameter': prop.equivalent_diameter * self.pixel_to_micron,
                    'major_axis': prop.major_axis_length * self.pixel_to_micron,
                    'minor_axis': prop.minor_axis_length * self.pixel_to_micron,
                    
                    # Shape descriptors
                    'circularity': (4 * np.pi * prop.area) / (prop.perimeter ** 2) if prop.perimeter > 0 else 0,
                    'eccentricity': prop.eccentricity,
                    'solidity': prop.solidity,
                    'aspect_ratio': prop.major_axis_length / prop.minor_axis_length if prop.minor_axis_length > 0 else 1,
                    'roundness': 4 * prop.area / (np.pi * prop.major_axis_length ** 2) if prop.major_axis_length > 0 else 0,
                    'convexity': prop.convex_area / prop.area if prop.area > 0 else 0,
                    
                    # Texture features
                    'mean_intensity': prop.mean_intensity,
                    'max_intensity': prop.max_intensity,
                    'min_intensity': prop.min_intensity,
                    'intensity_std': np.std(chlorophyll[labels == prop.label]),
                    
                    # Moments
                    'hu_moment_1': prop.moments_hu[0],
                    'hu_moment_2': prop.moments_hu[1],
                    'hu_moment_3': prop.moments_hu[2],
                }
                
                # Extract region pixels
                cell_mask = labels == prop.label
                
                # Color features from different color spaces
                if np.any(cell_mask):
                    # RGB features
                    rgb_pixels = original[cell_mask]
                    features['mean_red'] = np.mean(rgb_pixels[:, 0]) / 255.0
                    features['mean_green'] = np.mean(rgb_pixels[:, 1]) / 255.0
                    features['mean_blue'] = np.mean(rgb_pixels[:, 2]) / 255.0
                    features['std_red'] = np.std(rgb_pixels[:, 0]) / 255.0
                    features['std_green'] = np.std(rgb_pixels[:, 1]) / 255.0
                    features['std_blue'] = np.std(rgb_pixels[:, 2]) / 255.0
                    
                    # HSV features
                    hsv_pixels = hsv[cell_mask]
                    features['mean_hue'] = np.mean(hsv_pixels[:, 0])
                    features['mean_saturation'] = np.mean(hsv_pixels[:, 1])
                    features['mean_value'] = np.mean(hsv_pixels[:, 2])
                    features['std_hue'] = np.std(hsv_pixels[:, 0])
                    features['std_saturation'] = np.std(hsv_pixels[:, 1])
                    
                    # LAB features
                    lab_pixels = lab[cell_mask]
                    features['mean_lightness'] = np.mean(lab_pixels[:, 0])
                    features['mean_a'] = np.mean(lab_pixels[:, 1])
                    features['mean_b'] = np.mean(lab_pixels[:, 2])
                    
                    # Vegetation indices
                    features['mean_ndvi'] = np.mean(ndvi[cell_mask])
                    features['mean_gci'] = np.mean(gci[cell_mask])
                    features['mean_exg'] = np.mean(exg[cell_mask])
                    features['std_ndvi'] = np.std(ndvi[cell_mask])
                    
                    # Spectral features
                    features['green_red_ratio'] = features['mean_green'] / (features['mean_red'] + 1e-10)
                    features['green_blue_ratio'] = features['mean_green'] / (features['mean_blue'] + 1e-10)
                    features['chlorophyll_index'] = features['mean_green'] - 0.5 * (features['mean_red'] + features['mean_blue'])
                    
                    # Edge features
                    gray = preprocessed_data.get('gray')
                    if gray is None:
                        raise ValueError("Preprocessed data does not contain 'gray'.")
                    edges = filters.sobel(gray[cell_mask])
                    features['edge_density'] = np.mean(edges)
                    features['edge_std'] = np.std(edges)
                
                # Add region properties
                features['centroid_x'] = prop.centroid[1] * self.pixel_to_micron
                features['centroid_y'] = prop.centroid[0] * self.pixel_to_micron
                features['orientation'] = prop.orientation
                features['label'] = prop.label
                
                all_features.append(features)
            
            return pd.DataFrame(all_features)
            
        except Exception as e:
            print(f"Error in ML feature extraction: {str(e)}")
            return pd.DataFrame()
            
    def ml_classify_cells(self, features_df):
        """
        Machine learning based cell classification
        """
        if len(features_df) == 0:
            return features_df
            
        try:
            # Prepare features for ML
            feature_columns = [
                'area_microns_sq', 'circularity', 'eccentricity', 'solidity',
                'mean_intensity', 'mean_ndvi', 'mean_gci', 'chlorophyll_index',
                'green_red_ratio', 'edge_density', 'aspect_ratio', 'convexity'
            ]
            
            X = features_df[feature_columns].fillna(0)
            
            # Scale features
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            # Unsupervised clustering for cell types
            n_clusters = min(5, max(2, len(features_df) // 10))
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            clusters = kmeans.fit_predict(X_scaled)
            
            # Analyze clusters
            cluster_stats = []
            for i in range(n_clusters):
                mask = clusters == i
                if np.any(mask):
                    cluster_features = X[mask]
                    cluster_stats.append({
                        'cluster': i,
                        'mean_area': cluster_features['area_microns_sq'].mean(),
                        'mean_chlorophyll': cluster_features['chlorophyll_index'].mean(),
                        'mean_circularity': cluster_features['circularity'].mean(),
                        'count': np.sum(mask)
                    })
            
            cluster_df = pd.DataFrame(cluster_stats).sort_values('mean_area')
            
            # Assign cell types based on cluster characteristics
            cell_types = ['tiny', 'small', 'medium', 'large', 'extra_large']
            cluster_to_type = {}
            for i, row in cluster_df.iterrows():
                type_index = min(i, len(cell_types) - 1)
                cluster_to_type[row['cluster']] = cell_types[type_index]
            
            features_df['ml_cell_type'] = [cluster_to_type[c] for c in clusters]
            
            # Health classification based on multiple factors
            health_scores = np.zeros(len(features_df))
            
            # Circularity score (closer to expected is better)
            circ_score = 1 - np.abs(features_df['circularity'] - self.wolffia_params['expected_circularity'])
            
            # Chlorophyll score
            chl_score = features_df['chlorophyll_index'] / features_df['chlorophyll_index'].max()
            
            # Size score (penalize extremes)
            size_mean = features_df['area_microns_sq'].mean()
            size_std = features_df['area_microns_sq'].std()
            size_z = np.abs((features_df['area_microns_sq'] - size_mean) / size_std)
            size_score = 1 / (1 + size_z)
            
            # Combined health score
            health_scores = (circ_score * 0.3 + chl_score * 0.5 + size_score * 0.2)
            
            # Classify health
            features_df['ml_health_score'] = health_scores
            features_df['ml_health_status'] = pd.cut(
                health_scores,
                bins=[0, 0.4, 0.7, 1.0],
                labels=['stressed', 'moderate', 'healthy']
            )
            
            # Anomaly detection
            if self.anomaly_detector is not None:
                anomalies = self.anomaly_detector.fit_predict(X_scaled)
                features_df['is_anomaly'] = anomalies == -1
            
            # Growth stage prediction based on size and chlorophyll
            growth_stages = []
            for _, row in features_df.iterrows():
                if row['area_microns_sq'] < 500:
                    stage = 'daughter_frond'
                elif row['area_microns_sq'] < 2000:
                    stage = 'young'
                elif row['area_microns_sq'] < 5000:
                    stage = 'mature'
                else:
                    stage = 'mother_frond'
                growth_stages.append(stage)
            
            features_df['ml_growth_stage'] = growth_stages
            
            # Calculate feature importance
            if len(features_df) > 10:
                # Use Random Forest to determine feature importance
                y = health_scores > 0.7  # Binary classification: healthy vs not
                rf = RandomForestClassifier(n_estimators=50, random_state=42)
                rf.fit(X, y)
                
                self.feature_importance = pd.DataFrame({
                    'feature': feature_columns,
                    'importance': rf.feature_importances_
                }).sort_values('importance', ascending=False)
            
            return features_df
            
        except Exception as e:
            print(f"Error in ML classification: {str(e)}")
            return features_df
            
    def predict_biomass(self, features_df):
        """
        Advanced biomass prediction using multiple models
        """
        if len(features_df) == 0:
            return features_df
            
        try:
            # Multiple biomass estimation models
            biomass_estimates = []
            
            for _, cell in features_df.iterrows():
                # Model 1: Volume-based estimation
                # Assume ellipsoid shape
                a = cell['major_axis'] / 2  # Semi-major axis
                b = cell['minor_axis'] / 2  # Semi-minor axis
                c = b * 0.7  # Thickness approximation
                volume = (4/3) * np.pi * a * b * c
                
                # Dry weight estimation (Î¼g)
                # Based on Wolffia literature: ~0.05-0.1 mg/mmÂ³
                biomass_volume = volume * 0.075 / 1000  # Convert to Î¼g
                
                # Model 2: Area-based with chlorophyll adjustment
                # Base conversion: 1 mmÂ² â‰ˆ 10-50 Î¼g for Wolffia
                area_mm2 = cell['area_microns_sq'] / 1e6
                biomass_area = area_mm2 * 30  # Mid-range estimate
                
                # Chlorophyll adjustment
                chl_factor = 1 + (cell['chlorophyll_index'] - 0.5) * 0.5
                biomass_area *= chl_factor
                
                # Model 3: Allometric scaling
                # Biomass âˆ Area^1.5 (based on plant scaling laws)
                biomass_allometric = 0.01 * (cell['area_microns_sq'] ** 1.5) / 1000
                
                # Model 4: ML-based prediction using health and growth stage
                health_factor = {'healthy': 1.2, 'moderate': 1.0, 'stressed': 0.8}
                stage_factor = {'daughter_frond': 0.5, 'young': 0.7, 'mature': 1.0, 'mother_frond': 1.1}
                
                ml_factor = health_factor.get(cell.get('ml_health_status', 'moderate'), 1.0)
                ml_factor *= stage_factor.get(cell.get('ml_growth_stage', 'mature'), 1.0)
                
                biomass_ml = biomass_area * ml_factor
                
                # Ensemble prediction (weighted average)
                weights = [0.2, 0.3, 0.2, 0.3]  # Weights for each model
                final_biomass = (
                    weights[0] * biomass_volume +
                    weights[1] * biomass_area +
                    weights[2] * biomass_allometric +
                    weights[3] * biomass_ml
                )
                
                biomass_estimates.append({
                    'biomass_volume_model': biomass_volume,
                    'biomass_area_model': biomass_area,
                    'biomass_allometric_model': biomass_allometric,
                    'biomass_ml_model': biomass_ml,
                    'biomass_ensemble': final_biomass,
                    'biomass_uncertainty': np.std([biomass_volume, biomass_area, biomass_allometric, biomass_ml])
                })
            
            # Add to dataframe
            biomass_df = pd.DataFrame(biomass_estimates)
            features_df = pd.concat([features_df, biomass_df], axis=1)
            
            return features_df
            
        except Exception as e:
            print(f"Error in biomass prediction: {str(e)}")
            return features_df
            
    def analyze_population_dynamics(self, time_series_results):
        """
        Analyze population dynamics and predict future growth
        """
        if len(time_series_results) < 2:
            return None
            
        try:
            # Extract time series data
            timestamps = [r['timestamp'] for r in time_series_results]
            cell_counts = [r['total_cells'] for r in time_series_results]
            biomass_totals = [r['summary']['total_biomass_ug'] for r in time_series_results]
            
            # Convert to numpy arrays
            time_points = np.arange(len(timestamps))
            counts = np.array(cell_counts)
            biomass = np.array(biomass_totals)
            
            # Fit exponential growth model: N(t) = N0 * e^(rt)
            # Take log to linearize: log(N) = log(N0) + rt
            log_counts = np.log(counts + 1)  # Add 1 to avoid log(0)
            log_biomass = np.log(biomass + 1)
            
            # Linear regression for growth rate
            from scipy import stats
            
            # Cell count growth
            slope_count, intercept_count, r_value_count, p_value_count, std_err_count = stats.linregress(time_points, log_counts)
            growth_rate_count = slope_count
            
            # Biomass growth
            slope_biomass, intercept_biomass, r_value_biomass, p_value_biomass, std_err_biomass = stats.linregress(time_points, log_biomass)
            growth_rate_biomass = slope_biomass
            
            # Calculate doubling time
            doubling_time_count = np.log(2) / growth_rate_count if growth_rate_count > 0 else np.inf
            doubling_time_biomass = np.log(2) / growth_rate_biomass if growth_rate_biomass > 0 else np.inf
            
            # Predict future values
            future_points = np.arange(len(timestamps), len(timestamps) + 5)
            predicted_counts = np.exp(intercept_count + slope_count * future_points)
            predicted_biomass = np.exp(intercept_biomass + slope_biomass * future_points)
            
            # Calculate carrying capacity using logistic model if growth slowing
            if len(counts) > 5:
                # Check for growth deceleration
                growth_rates = np.diff(counts) / counts[:-1]
                if np.mean(growth_rates[-3:]) < np.mean(growth_rates[:3]):
                    # Fit logistic model
                    from scipy.optimize import curve_fit
                    
                    def logistic(t, K, r, t0):
                        return K / (1 + np.exp(-r * (t - t0)))
                    
                    try:
                        popt, _ = curve_fit(logistic, time_points, counts, 
                                          p0=[max(counts) * 2, 0.5, len(counts) / 2],
                                          maxfev=5000)
                        carrying_capacity = popt[0]
                        logistic_rate = popt[1]
                        
                        # Predict with logistic model
                        predicted_counts_logistic = logistic(future_points, *popt)
                    except:
                        carrying_capacity = None
                        predicted_counts_logistic = None
                else:
                    carrying_capacity = None
                    predicted_counts_logistic = None
            else:
                carrying_capacity = None
                predicted_counts_logistic = None
            
            # Population health metrics
            health_percentages = []
            size_distributions = []
            
            for result in time_series_results:
                if 'cell_data' in result:
                    df = result['cell_data']
                    if 'ml_health_status' in df.columns:
                        health_dist = df['ml_health_status'].value_counts(normalize=True).to_dict()
                        health_percentages.append(health_dist.get('healthy', 0))
                    
                    size_distributions.append({
                        'mean': df['area_microns_sq'].mean(),
                        'std': df['area_microns_sq'].std(),
                        'median': df['area_microns_sq'].median()
                    })
            
            # Diversity indices
            diversity_metrics = []
            for result in time_series_results:
                if 'cell_data' in result:
                    df = result['cell_data']
                    if 'ml_cell_type' in df.columns:
                        # Shannon diversity index
                        type_counts = df['ml_cell_type'].value_counts()
                        proportions = type_counts / len(df)
                        shannon = -np.sum(proportions * np.log(proportions + 1e-10))
                        
                        # Simpson diversity index
                        simpson = 1 - np.sum(proportions ** 2)
                        
                        diversity_metrics.append({
                            'shannon': shannon,
                            'simpson': simpson,
                            'richness': len(type_counts)
                        })
            
            population_dynamics = {
                'growth_analysis': {
                    'cell_count_growth_rate': growth_rate_count,
                    'biomass_growth_rate': growth_rate_biomass,
                    'doubling_time_cells': doubling_time_count,
                    'doubling_time_biomass': doubling_time_biomass,
                    'r_squared_count': r_value_count ** 2,
                    'r_squared_biomass': r_value_biomass ** 2,
                    'carrying_capacity': carrying_capacity
                },
                'predictions': {
                    'future_time_points': future_points.tolist(),
                    'predicted_cell_counts': predicted_counts.tolist(),
                    'predicted_biomass': predicted_biomass.tolist(),
                    'predicted_counts_logistic': predicted_counts_logistic.tolist() if predicted_counts_logistic is not None else None
                },
                'population_health': {
                    'health_trend': health_percentages,
                    'size_distributions': size_distributions
                },
                'diversity': diversity_metrics,
                'alerts': self._generate_population_alerts(growth_rate_count, growth_rate_biomass, health_percentages)
            }
            
            return population_dynamics
            
        except Exception as e:
            print(f"Error in population dynamics analysis: {str(e)}")
            return None
            
    def _generate_population_alerts(self, growth_rate_count, growth_rate_biomass, health_percentages):
        """Generate alerts based on population dynamics"""
        alerts = []
        
        # Growth rate alerts
        if growth_rate_count < 0:
            alerts.append({
                'type': 'warning',
                'message': 'Population decline detected',
                'severity': 'high'
            })
        elif growth_rate_count > 1.0:
            alerts.append({
                'type': 'info',
                'message': 'Rapid population growth detected',
                'severity': 'medium'
            })
        
        # Health alerts
        if health_percentages and len(health_percentages) > 1:
            health_trend = health_percentages[-1] - health_percentages[0]
            if health_trend < -0.2:
                alerts.append({
                    'type': 'warning',
                    'message': 'Declining population health',
                    'severity': 'high'
                })
        
        # Biomass alerts
        if abs(growth_rate_biomass - growth_rate_count) > 0.5:
            alerts.append({
                'type': 'info',
                'message': 'Biomass and cell count growth rates diverging',
                'severity': 'medium'
            })
        
        return alerts
        
    def optimize_parameters(self, results_history):
        """
        Auto-optimize analysis parameters based on results
        """
        if len(results_history) < 5:
            return self.get_current_parameters()
            
        try:
            # Collect performance metrics
            segmentation_quality = []
            classification_accuracy = []
            
            for result in results_history:
                if 'cell_data' in result:
                    df = result['cell_data']
                    
                    # Segmentation quality based on shape regularity
                    if 'circularity' in df.columns:
                        # Good segmentation should have consistent circularity
                        circ_std = df['circularity'].std()
                        seg_quality = 1 / (1 + circ_std)
                        segmentation_quality.append(seg_quality)
                    
                    # Classification accuracy based on cluster separation
                    if 'ml_cell_type' in df.columns and len(df) > 10:
                        # Calculate silhouette score
                        feature_cols = ['area_microns_sq', 'chlorophyll_index', 'circularity']
                        X = df[feature_cols].fillna(0)
                        labels = pd.Categorical(df['ml_cell_type']).codes
                        
                        if len(np.unique(labels)) > 1:
                            score = silhouette_score(X, labels)
                            classification_accuracy.append(score)
            
            # Optimize parameters
            optimized_params = self.get_current_parameters()
            
            # Adjust chlorophyll threshold based on classification
            if classification_accuracy:
                mean_accuracy = np.mean(classification_accuracy)
                if mean_accuracy < 0.3:
                    # Poor separation, adjust threshold
                    self.chlorophyll_threshold *= 0.95
                elif mean_accuracy > 0.7:
                    # Good separation, can be more selective
                    self.chlorophyll_threshold *= 1.05
                
                self.chlorophyll_threshold = np.clip(self.chlorophyll_threshold, 0.3, 0.9)
                optimized_params['chlorophyll_threshold'] = self.chlorophyll_threshold
            
            # Adjust minimum cell area based on segmentation quality
            if segmentation_quality:
                mean_quality = np.mean(segmentation_quality)
                if mean_quality < 0.5:
                    # Poor segmentation, increase minimum area
                    self.wolffia_params['min_area_microns'] *= 1.1
                elif mean_quality > 0.8:
                    # Good segmentation, can decrease minimum
                    self.wolffia_params['min_area_microns'] *= 0.9
                
                self.wolffia_params['min_area_microns'] = np.clip(
                    self.wolffia_params['min_area_microns'], 50, 500
                )
                optimized_params['min_area_microns'] = self.wolffia_params['min_area_microns']
            
            return optimized_params
            
        except Exception as e:
            print(f"Error in parameter optimization: {str(e)}")
            return self.get_current_parameters()
            
    def get_current_parameters(self):
        """Get current analysis parameters"""
        return {
            'pixel_to_micron': self.pixel_to_micron,
            'chlorophyll_threshold': self.chlorophyll_threshold,
            'min_area_microns': self.wolffia_params['min_area_microns'],
            'max_area_microns': self.wolffia_params['max_area_microns'],
            'expected_circularity': self.wolffia_params['expected_circularity']
        }
        
    def analyze_single_image(self, image_path, timestamp=None, save_visualization=True):
        """
        Complete enhanced analysis pipeline for a single image
        """
        if timestamp is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
        print(f"\n{'='*60}")
        print(f"Analyzing image: {image_path}")
        print(f"Timestamp: {timestamp}")
        print(f"{'='*60}")
        
        try:
            # Smart preprocessing - use intelligent only when needed
            print("ðŸ” Step 1: Smart preprocessing with targeted quality assessment...")
            preprocessed = self.smart_targeted_preprocess(image_path)
            if preprocessed is None:
                print("âŒ Failed to preprocess image")
                return None
            print("âœ… Preprocessing complete")
            
            # Multi-method segmentation with intelligent fallbacks
            print("\nðŸ” Step 2: Performing intelligent segmentation...")
            labels, segmentation_methods = self.multi_method_segmentation(preprocessed)
            
            # Enhanced fallback strategy for failed segmentation
            if np.max(labels) == 0:
                print("âš ï¸ Initial segmentation found no cells. Trying enhanced fallback methods...")
                labels, segmentation_methods = self._enhanced_segmentation_fallback(preprocessed)
                
                if np.max(labels) == 0:
                    print("âŒ Enhanced fallback also failed. Trying aggressive preprocessing...")
                    # Try with more aggressive preprocessing
                    aggressive_preprocessed = self._aggressive_preprocessing(preprocessed['original'])
                    aggressive_data = self._create_preprocessed_data_structure(aggressive_preprocessed, preprocessed['original'])
                    labels, segmentation_methods = self.multi_method_segmentation(aggressive_data)
                    
                    if np.max(labels) == 0:
                        print("âŒ All segmentation methods failed. No cells detected.")
                        return None
                    else:
                        print(f"âœ… Aggressive preprocessing succeeded! Found {np.max(labels)} potential cells")
                        preprocessed = aggressive_data
                else:
                    print(f"âœ… Enhanced fallback succeeded! Found {np.max(labels)} potential cells")
            
            print(f"âœ… Detected {np.max(labels)} potential cells")
            
            # Extract ML features
            print("\nStep 3: Extracting features...")
            features_df = self.extract_ml_features(labels, preprocessed)
            
            if len(features_df) == 0:
                print("âŒ No features extracted!")
                return None
            print(f"âœ… Extracted features for {len(features_df)} cells")
            
            # ML classification
            print("\nStep 4: ML classification...")
            features_df = self.ml_classify_cells(features_df)
            print("âœ… Classification complete")
            
            # Biomass prediction
            print("\nStep 5: Predicting biomass...")
            features_df = self.predict_biomass(features_df)
            print("âœ… Biomass prediction complete")
        
        # Rest of the method remains the same...
            
            # Add traditional features for compatibility
            features_df['cell_id'] = features_df['label']
            features_df['area_microns_sq'] = features_df['area_microns_sq']
            features_df['mean_chlorophyll_intensity'] = features_df['mean_intensity']  # Use actual intensity
            features_df['biomass_estimate_ug'] = features_df['biomass_ensemble']
            features_df['health_status'] = features_df['ml_health_status'] if 'ml_health_status' in features_df.columns else 'unknown'
            features_df['cell_type'] = features_df['ml_cell_type'] if 'ml_cell_type' in features_df.columns else 'unknown'
            features_df['growth_stage'] = features_df['ml_growth_stage'] if 'ml_growth_stage' in features_df.columns else 'unknown'
            
            if 'ml_cell_type' in features_df.columns:
                cell_type_counts = features_df['ml_cell_type'].value_counts().to_dict()
                features_df['similar_cell_count'] = features_df['ml_cell_type'].map(lambda x: cell_type_counts.get(x, 0) - 1)
            
            # Add metadata
            features_df['timestamp'] = timestamp
            features_df['image_path'] = str(image_path)
            
            # Calculate comprehensive statistics
            summary = self.calculate_enhanced_stats(features_df)
            
            # Create visualizations
            visualizations = {}
            if save_visualization:
                visualizations = self.create_enhanced_visualization(
                    preprocessed, labels, features_df, segmentation_methods,
                    return_base64=True
                )
            
            # Prepare result
            result = {
                'timestamp': timestamp,
                'image_path': str(image_path),
                'cell_data': features_df,
                'summary': summary,
                'total_cells': len(features_df),
                'visualizations': visualizations,
                'segmentation_methods': {k: v.tolist() if isinstance(v, np.ndarray) else v 
                                       for k, v in segmentation_methods.items() if k != 'distance_map'},
                'ml_metrics': {
                    'feature_importance': self.feature_importance.to_dict() if self.feature_importance is not None else None,
                    'anomalies_detected': int(features_df['is_anomaly'].sum()) if 'is_anomaly' in features_df.columns else 0
                }
            }
            
            self.results_history.append(result)
            print(f"Analysis complete: {len(features_df)} cells analyzed with ML enhancement")
            
            return result
            
        except Exception as e:
            print(f"Error in enhanced analysis: {str(e)}")
            import traceback
            traceback.print_exc()
            return None
            
            
            
            
    def analyze_single_image_enhanced(self, image_path, timestamp=None, save_visualization=True, custom_params=None):
        """
        Enhanced complete analysis pipeline with all professional features
        """
        if timestamp is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        self.current_image_path = image_path
        
        print(f"\n{'='*60}")
        print(f"ðŸ”¬ BIOIMAGIN Professional Analysis")
        print(f"Image: {image_path}")
        print(f"Timestamp: {timestamp}")
        print(f"{'='*60}")
        
        try:
            # Step 1: Smart preprocessing with top-hat
            print("ðŸ“Š Step 1: Smart preprocessing with enhanced top-hat filter...")
            preprocessed = self.smart_preprocess_with_tophat(image_path, auto_optimize=True)
            
            if preprocessed is None:
                print("âŒ Failed to preprocess image")
                return None
            
            print(f"âœ… Preprocessing complete (Optimal top-hat size: {preprocessed['optimal_tophat_size']})")
            
            # Also run standard preprocessing for compatibility
            standard_preprocessed = self.advanced_preprocess_image(image_path)
            
            # Merge preprocessing results
            preprocessed.update({
                'hsv': standard_preprocessed['hsv'],
                'lab': standard_preprocessed['lab'],
                'ndvi': standard_preprocessed['ndvi'],
                'gci': standard_preprocessed['gci'],
                'exg': standard_preprocessed['exg'],
                'green_mask': standard_preprocessed['green_mask']
            })
            
            # Step 2: Enhanced segmentation with custom sensitivity
            print("\nðŸ” Step 2: Multi-method segmentation with ML optimization...")
            
            # Apply custom parameters if provided
            if custom_params:
                min_area = custom_params.get('min_cell_area', 30)
                max_area = custom_params.get('max_cell_area', 8000)
            else:
                min_area = 30
                max_area = 8000
            
            labels, segmentation_methods = self.multi_method_segmentation(
                preprocessed, 
                min_cell_area=min_area, 
                max_cell_area=max_area
            )
            
            if np.max(labels) == 0:
                print("âŒ No cells detected! Trying with reduced sensitivity...")
                # Try with more lenient parameters
                labels, segmentation_methods = self.multi_method_segmentation(
                    preprocessed, 
                    min_cell_area=20, 
                    max_cell_area=10000
                )
            
            print(f"âœ… Detected {np.max(labels)} cells")
            
            # Step 3: Spectral analysis
            print("\nðŸŒˆ Step 3: Spectral analysis and chlorophyll quantification...")
            spectral_df, spectral_viz = self.analyze_chlorophyll_spectrum(
                preprocessed['original'], 
                labels
            )
            
            print(f"âœ… Spectral analysis complete")
            print(f"   Total chlorophyll: {spectral_df['total_chlorophyll_ug'].sum():.2f} Î¼g")
            
            # Step 4: ML feature extraction
            print("\nðŸ¤– Step 4: ML feature extraction and classification...")
            features_df = self.extract_ml_features(labels, preprocessed)
            
            # Add spectral features
            features_df = features_df.merge(
                spectral_df[['cell_id', 'chlorophyll_concentration', 'total_chlorophyll_ug', 
                            'spectral_health', 'evi_mean', 'gli_mean', 'tgi_mean']],
                left_on='label', right_on='cell_id', how='left'
            )
            
            print(f"âœ… Extracted features for {len(features_df)} cells")
            
            # Step 5: ML classification with spectral data
            print("\nðŸ§  Step 5: Advanced ML classification...")
            features_df = self.ml_classify_cells(features_df)
            
            # Combine health assessments
            features_df['combined_health'] = features_df.apply(
                lambda row: self._combine_health_assessments(
                    row.get('ml_health_status', 'unknown'),
                    row.get('spectral_health', 'unknown')
                ), axis=1
            )
            
            print("âœ… ML classification complete")
            
            # Step 6: Enhanced biomass prediction
            print("\nâš–ï¸ Step 6: Multi-model biomass prediction...")
            features_df = self.predict_biomass(features_df)
            
            # Add chlorophyll-based biomass
            features_df['biomass_chlorophyll'] = features_df['total_chlorophyll_ug'] * 8.5  # Empirical factor
            
            # Update ensemble to include chlorophyll model
            features_df['biomass_ensemble_enhanced'] = (
                features_df['biomass_ensemble'] * 0.7 +
                features_df['biomass_chlorophyll'] * 0.3
            )
            
            print("âœ… Biomass prediction complete")
            
            # Step 7: Cell tracking preparation
            print("\nðŸŽ¯ Step 7: Preparing cell tracking data...")
            
            # Add tracking features
            features_df['tracking_id'] = features_df['label'].astype(str) + '_' + timestamp
            features_df['timestamp'] = timestamp
            features_df['image_path'] = str(image_path)
            
            # Calculate green cells
            green_cells = features_df[
                (features_df['chlorophyll_concentration'] > 10) &  # Î¼g/cmÂ²
                (features_df['spectral_health'].isin(['healthy', 'very_healthy']))
            ]
            
            print(f"âœ… Identified {len(green_cells)} green cells")
            
            # Step 8: Generate comprehensive visualizations
            if save_visualization:
                print("\nðŸŽ¨ Step 8: Creating enhanced visualizations...")
                
                visualizations = self.create_enhanced_visualization(
                    preprocessed, labels, features_df, segmentation_methods,
                    return_base64=True
                )
                
                # Add spectral visualization
                visualizations['spectral_analysis'] = spectral_viz
                
                # Add top-hat visualization
                tophat_viz = self._create_tophat_visualization(preprocessed)
                visualizations['tophat_analysis'] = tophat_viz
                
                print("âœ… Visualizations complete")
            else:
                visualizations = {}
            
            # Calculate comprehensive statistics
            summary = self.calculate_professional_stats(features_df, spectral_df)
            
            # Generate reports
            spectral_report = self.generate_spectral_report(spectral_df)
            
            # Prepare final result
            result = {
                'timestamp': timestamp,
                'image_path': str(image_path),
                'cell_data': features_df,
                'spectral_data': spectral_df,
                'summary': summary,
                'total_cells': len(features_df),
                'green_cells': len(green_cells),
                'visualizations': visualizations,
                'segmentation_methods': {k: v.tolist() if isinstance(v, np.ndarray) else v 
                                    for k, v in segmentation_methods.items() if k != 'distance_map'},
                'ml_metrics': {
                    'feature_importance': self.feature_importance.to_dict() if self.feature_importance is not None else None,
                    'anomalies_detected': int(features_df['is_anomaly'].sum()) if 'is_anomaly' in features_df.columns else 0,
                    'model_confidence': self._calculate_model_confidence(features_df)
                },
                'spectral_report': spectral_report,
                'preprocessing_params': {
                    'tophat_size': preprocessed['optimal_tophat_size'],
                    'segmentation_sensitivity': 'adaptive'
                }
            }
            
            # Store for history
            self.results_history.append(result)
            
            print(f"\n{'='*60}")
            print(f"âœ… ANALYSIS COMPLETE")
            print(f"   Total cells: {len(features_df)}")
            print(f"   Green cells: {len(green_cells)}")
            print(f"   Total chlorophyll: {spectral_df['total_chlorophyll_ug'].sum():.2f} Î¼g")
            print(f"   Total biomass: {features_df['biomass_ensemble_enhanced'].sum():.2f} Î¼g")
            print(f"{'='*60}\n")
            
            return result
            
        except Exception as e:
            print(f"âŒ Error in enhanced analysis: {str(e)}")
            import traceback
            traceback.print_exc()
            return None

    def _combine_health_assessments(self, ml_health, spectral_health):
        """Combine ML and spectral health assessments"""
        health_scores = {
            'healthy': 3, 'very_healthy': 4,
            'moderate': 2, 'stressed': 1, 'unknown': 0
        }
        
        ml_score = health_scores.get(ml_health, 0)
        spectral_score = health_scores.get(spectral_health, 0)
        
        combined_score = (ml_score + spectral_score) / 2
        
        if combined_score >= 3.5:
            return 'very_healthy'
        elif combined_score >= 2.5:
            return 'healthy'
        elif combined_score >= 1.5:
            return 'moderate'
        else:
            return 'stressed'

    def _calculate_model_confidence(self, features_df):
        """Calculate overall model confidence"""
        if len(features_df) == 0:
            return 0
        
        # Factors affecting confidence
        factors = []
        
        # 1. Anomaly rate
        anomaly_rate = features_df['is_anomaly'].sum() / len(features_df) if 'is_anomaly' in features_df.columns else 0
        factors.append(1 - anomaly_rate)
        
        # 2. Health score consistency
        if 'ml_health_score' in features_df.columns:
            health_std = features_df['ml_health_score'].std()
            factors.append(1 / (1 + health_std))
        
        # 3. Biomass uncertainty
        if 'biomass_uncertainty' in features_df.columns:
            mean_uncertainty = features_df['biomass_uncertainty'].mean()
            mean_biomass = features_df['biomass_ensemble'].mean()
            uncertainty_ratio = mean_uncertainty / (mean_biomass + 1e-10)
            factors.append(1 - min(uncertainty_ratio, 1))
        
        # 4. Segmentation quality
        circularity_std = features_df['circularity'].std()
        factors.append(1 / (1 + circularity_std))
        
        return float(np.mean(factors))

    def _create_tophat_visualization(self, preprocessed):
        """Create visualization of top-hat filtering results"""
        fig, axes = plt.subplots(2, 2, figsize=(10, 10))
        
        axes[0, 0].imshow(preprocessed['original'])
        axes[0, 0].set_title('Original Image')
        axes[0, 0].axis('off')
        
        axes[0, 1].imshow(preprocessed['tophat_enhanced'], cmap='gray')
        axes[0, 1].set_title(f'Top-hat Enhanced (size={preprocessed["optimal_tophat_size"]})')
        axes[0, 1].axis('off')
        
        axes[1, 0].imshow(preprocessed['green_enhanced'], cmap='Greens')
        axes[1, 0].set_title('Enhanced Green Channel')
        axes[1, 0].axis('off')
        
        axes[1, 1].imshow(preprocessed['background_mask'], cmap='gray')
        axes[1, 1].set_title('Background Mask')
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        
        buffer = BytesIO()
        plt.savefig(buffer, format='png', dpi=150, bbox_inches='tight')
        buffer.seek(0)
        tophat_viz = base64.b64encode(buffer.getvalue()).decode()
        plt.close()
        
        return tophat_viz

    def calculate_professional_stats(self, features_df, spectral_df):
        """Calculate comprehensive professional statistics"""
        if len(features_df) == 0:
            return {}
        
        stats = self.calculate_enhanced_stats(features_df)
        
        # Add spectral statistics
        stats.update({
            'total_chlorophyll_ug': float(spectral_df['total_chlorophyll_ug'].sum()),
            'mean_chlorophyll_concentration': float(spectral_df['chlorophyll_concentration'].mean()),
            'chlorophyll_per_cell': float(spectral_df['total_chlorophyll_ug'].mean()),
            'spectral_health_distribution': spectral_df['spectral_health'].value_counts().to_dict(),
            'vegetation_indices': {
                'evi': float(spectral_df['evi_mean'].mean()),
                'gli': float(spectral_df['gli_mean'].mean()),
                'tgi': float(spectral_df['tgi_mean'].mean())
            }
        })
        
        # Add green cell statistics
        green_mask = (
            (spectral_df['chlorophyll_concentration'] > 10) &
            (spectral_df['spectral_health'].isin(['healthy', 'very_healthy']))
        )
        
        stats['green_cell_count'] = int(green_mask.sum())
        stats['green_cell_percentage'] = float(green_mask.mean() * 100)
        stats['green_cell_total_chlorophyll'] = float(spectral_df.loc[green_mask, 'total_chlorophyll_ug'].sum())
        
        return stats
    
    
    
    def calculate_enhanced_stats(self, df):
        """Calculate enhanced statistics including ML metrics"""
        if len(df) == 0:
            return {}
            
        try:
            # Traditional statistics
            basic_stats = {
                'total_cell_count': len(df),
                'mean_cell_area_microns': float(df['area_microns_sq'].mean()),
                'std_cell_area_microns': float(df['area_microns_sq'].std()),
                'median_cell_area_microns': float(df['area_microns_sq'].median()),
                'total_biomass_ug': float(df['biomass_ensemble'].sum()),
                'mean_chlorophyll_intensity': float(df['chlorophyll_index'].mean())
            }
            
            # ML-enhanced statistics
            ml_stats = {
                'ml_cell_type_distribution': df['ml_cell_type'].value_counts().to_dict(),
                'ml_health_distribution': df['ml_health_status'].value_counts().to_dict() if 'ml_health_status' in df.columns else {},
                'ml_growth_stage_distribution': df['ml_growth_stage'].value_counts().to_dict() if 'ml_growth_stage' in df.columns else {},
                'anomaly_count': int(df['is_anomaly'].sum()) if 'is_anomaly' in df.columns else 0,
                'mean_health_score': float(df['ml_health_score'].mean()) if 'ml_health_score' in df.columns else 0
            }
            
            # Biomass statistics by model
            biomass_stats = {
                'biomass_volume_total': float(df['biomass_volume_model'].sum()),
                'biomass_area_total': float(df['biomass_area_model'].sum()),
                'biomass_allometric_total': float(df['biomass_allometric_model'].sum()),
                'biomass_ml_total': float(df['biomass_ml_model'].sum()),
                'mean_biomass_uncertainty': float(df['biomass_uncertainty'].mean())
            }
            
            # Spectral analysis statistics
            spectral_stats = {
                'mean_ndvi': float(df['mean_ndvi'].mean()) if 'mean_ndvi' in df.columns else 0,
                'mean_gci': float(df['mean_gci'].mean()) if 'mean_gci' in df.columns else 0,
                'mean_exg': float(df['mean_exg'].mean()) if 'mean_exg' in df.columns else 0,
                'green_red_ratio': float(df['green_red_ratio'].mean()) if 'green_red_ratio' in df.columns else 0
            }
            
            # Color analysis
            color_stats = {
                'mean_hue': float(df['mean_hue'].mean()) if 'mean_hue' in df.columns else 0,
                'mean_saturation': float(df['mean_saturation'].mean()) if 'mean_saturation' in df.columns else 0,
                'mean_lightness': float(df['mean_lightness'].mean()) if 'mean_lightness' in df.columns else 0
            }
            
            # Morphological diversity
            morph_stats = {
                'circularity_mean': float(df['circularity'].mean()),
                'circularity_std': float(df['circularity'].std()),
                'eccentricity_mean': float(df['eccentricity'].mean()),
                'solidity_mean': float(df['solidity'].mean())
            }
            
            # Quality metrics
            quality_metrics = {
                'segmentation_confidence': float(1 - df['eccentricity'].std()),  # Lower std = better
                'population_homogeneity': float(1 / (1 + df['area_microns_sq'].std() / df['area_microns_sq'].mean())),
                'health_index': float(df['ml_health_score'].mean()) if 'ml_health_score' in df.columns else 0.5
            }
            
            # Combine all statistics
            summary = {
                **basic_stats,
                **ml_stats,
                **biomass_stats,
                **spectral_stats,
                **color_stats,
                **morph_stats,
                **quality_metrics,
                'green_cell_count': len(df[df['chlorophyll_index'] > self.chlorophyll_threshold]),
                'healthy_cell_count': len(df[df['ml_health_status'] == 'healthy']) if 'ml_health_status' in df.columns else 0,
                'healthy_cell_percentage': float((df['ml_health_status'] == 'healthy').mean() * 100) if 'ml_health_status' in df.columns else 0
            }
            
            return summary
            
        except Exception as e:
            print(f"Error calculating enhanced statistics: {str(e)}")
            return {}
            
    def create_enhanced_visualization(self, preprocessed, labels, df, segmentation_methods, 
                                    output_path=None, return_base64=False):
        """Create enhanced visualizations with ML insights"""
        try:
            visualizations = {}
            
            # Main analysis figure with more panels
            fig = plt.figure(figsize=(24, 20))
            
            # Create grid layout
            gs = fig.add_gridspec(5, 4, hspace=0.3, wspace=0.3)
            
            # Original image
            ax1 = fig.add_subplot(gs[0, 0])
            ax1.imshow(preprocessed['original'])
            ax1.set_title('Original Image', fontsize=12)
            ax1.axis('off')
            
            # Enhanced chlorophyll
            ax2 = fig.add_subplot(gs[0, 1])
            ax2.imshow(preprocessed['chlorophyll_enhanced'], cmap='Greens')
            ax2.set_title('Enhanced Chlorophyll', fontsize=12)
            ax2.axis('off')
            
            # NDVI
            ax3 = fig.add_subplot(gs[0, 2])
            ax3.imshow(preprocessed['ndvi'], cmap='RdYlGn')
            ax3.set_title('NDVI', fontsize=12)
            ax3.axis('off')
            
            # Final segmentation
            ax4 = fig.add_subplot(gs[0, 3])
            ax4.imshow(preprocessed['original'])
            ax4.imshow(labels, alpha=0.4, cmap='tab20')
            ax4.set_title(f'Final Segmentation ({len(df)} cells)', fontsize=12)
            ax4.axis('off')
            
            # Segmentation methods comparison
            methods = ['binary_otsu', 'binary_adaptive', 'binary_kmeans', 'binary_fz']
            for i, method in enumerate(methods):
                if method in segmentation_methods:
                    ax = fig.add_subplot(gs[1, i])
                    ax.imshow(segmentation_methods[method], cmap='gray')
                    ax.set_title(method.replace('binary_', '').upper(), fontsize=10)
                    ax.axis('off')
            
            if len(df) > 0:
                # ML cell type distribution
                ax5 = fig.add_subplot(gs[2, 0])
                if 'ml_cell_type' in df.columns:
                    type_counts = df['ml_cell_type'].value_counts()
                    colors = plt.cm.viridis(np.linspace(0, 1, len(type_counts)))
                    ax5.pie(type_counts.values, labels=type_counts.index, autopct='%1.1f%%', colors=colors)
                    ax5.set_title('ML Cell Types')
                
                # Health status distribution
                ax6 = fig.add_subplot(gs[2, 1])
                if 'ml_health_status' in df.columns:
                    health_counts = df['ml_health_status'].value_counts()
                    colors_health = {'healthy': '#4CAF50', 'moderate': '#FFC107', 'stressed': '#F44336'}
                    pie_colors = [colors_health.get(status, 'gray') for status in health_counts.index]
                    ax6.pie(health_counts.values, labels=health_counts.index, autopct='%1.1f%%', colors=pie_colors)
                    ax6.set_title('ML Health Status')
                
                # Biomass by model
                ax7 = fig.add_subplot(gs[2, 2])
                biomass_models = ['volume', 'area', 'allometric', 'ml', 'ensemble']
                biomass_values = [
                    df['biomass_volume_model'].sum(),
                    df['biomass_area_model'].sum(),
                    df['biomass_allometric_model'].sum(),
                    df['biomass_ml_model'].sum(),
                    df['biomass_ensemble'].sum()
                ]
                bars = ax7.bar(biomass_models, biomass_values, color=plt.cm.plasma(np.linspace(0, 1, 5)))
                ax7.set_ylabel('Total Biomass (Î¼g)')
                ax7.set_title('Biomass by Model')
                ax7.tick_params(axis='x', rotation=45)
                
                # Add value labels on bars
                for bar, value in zip(bars, biomass_values):
                    ax7.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01*max(biomass_values),
                            f'{value:.1f}', ha='center', va='bottom', fontsize=8)
                
                # Growth stage distribution
                ax8 = fig.add_subplot(gs[2, 3])
                if 'ml_growth_stage' in df.columns:
                    stage_counts = df['ml_growth_stage'].value_counts()
                    stage_order = ['daughter_frond', 'young', 'mature', 'mother_frond']
                    stage_counts = stage_counts.reindex(stage_order, fill_value=0)
                    colors_stage = plt.cm.Greens(np.linspace(0.3, 0.9, len(stage_counts)))
                    ax8.bar(stage_counts.index, stage_counts.values, color=colors_stage)
                    ax8.set_xlabel('Growth Stage')
                    ax8.set_ylabel('Count')
                    ax8.set_title('Growth Stage Distribution')
                    ax8.tick_params(axis='x', rotation=45)
                
                # Area vs Chlorophyll scatter with ML classification
                ax9 = fig.add_subplot(gs[3, 0:2])
                if 'ml_cell_type' in df.columns:
                    types = df['ml_cell_type'].unique()
                    colors = plt.cm.tab10(np.linspace(0, 1, len(types)))
                    for i, cell_type in enumerate(types):
                        mask = df['ml_cell_type'] == cell_type
                        ax9.scatter(df.loc[mask, 'area_microns_sq'], 
                                  df.loc[mask, 'chlorophyll_index'],
                                  c=[colors[i]], label=cell_type, alpha=0.6, s=50)
                    ax9.set_xlabel('Cell Area (Î¼mÂ²)')
                    ax9.set_ylabel('Chlorophyll Index')
                    ax9.set_title('Area vs Chlorophyll by Cell Type')
                    ax9.legend()
                    ax9.grid(True, alpha=0.3)
                
                # Feature importance
                ax10 = fig.add_subplot(gs[3, 2:4])
                if self.feature_importance is not None and len(self.feature_importance) > 0:
                    top_features = self.feature_importance.head(10)
                    ax10.barh(top_features['feature'], top_features['importance'], color='skyblue')
                    ax10.set_xlabel('Importance')
                    ax10.set_title('Top 10 Feature Importances')
                    ax10.grid(True, alpha=0.3)
                
                # Spectral indices distribution
                ax11 = fig.add_subplot(gs[4, 0])
                spectral_data = {
                    'NDVI': df['mean_ndvi'].mean() if 'mean_ndvi' in df.columns else 0,
                    'GCI': df['mean_gci'].mean() if 'mean_gci' in df.columns else 0,
                    'ExG': df['mean_exg'].mean() if 'mean_exg' in df.columns else 0
                }
                ax11.bar(spectral_data.keys(), spectral_data.values(), color=['green', 'darkgreen', 'lime'])
                ax11.set_ylabel('Mean Value')
                ax11.set_title('Spectral Indices')
                ax11.grid(True, alpha=0.3)
                
                # Anomaly detection visualization
                ax12 = fig.add_subplot(gs[4, 1])
                if 'is_anomaly' in df.columns:
                    anomaly_counts = df['is_anomaly'].value_counts()
                    labels = ['Normal', 'Anomaly']
                    values = [anomaly_counts.get(False, 0), anomaly_counts.get(True, 0)]
                    colors = ['#4CAF50', '#F44336']
                    ax12.pie(values, labels=labels, autopct='%1.1f%%', colors=colors)
                    ax12.set_title('Anomaly Detection')
                
                # Biomass uncertainty
                ax13 = fig.add_subplot(gs[4, 2])
                ax13.hist(df['biomass_uncertainty'], bins=20, alpha=0.7, color='purple', edgecolor='black')
                ax13.set_xlabel('Uncertainty (Î¼g)')
                ax13.set_ylabel('Frequency')
                ax13.set_title('Biomass Prediction Uncertainty')
                ax13.grid(True, alpha=0.3)
                
                # Health score distribution
                ax14 = fig.add_subplot(gs[4, 3])
                if 'ml_health_score' in df.columns:
                    ax14.hist(df['ml_health_score'], bins=20, alpha=0.7, color='teal', edgecolor='black')
                    ax14.set_xlabel('Health Score')
                    ax14.set_ylabel('Frequency')
                    ax14.set_title('ML Health Score Distribution')
                    ax14.axvline(df['ml_health_score'].mean(), color='red', linestyle='--', 
                               label=f'Mean: {df["ml_health_score"].mean():.2f}')
                    ax14.legend()
                    ax14.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            if return_base64:
                # Convert main visualization to base64
                buffer = BytesIO()
                plt.savefig(buffer, format='png', dpi=150, bbox_inches='tight')
                buffer.seek(0)
                visualizations['main_analysis'] = base64.b64encode(buffer.getvalue()).decode()
                plt.close()
                
                # Create additional ML-specific visualizations
                # 1. Cell tracking with ML classification
                fig2, ax2 = plt.subplots(figsize=(12, 10))
                ax2.imshow(preprocessed['original'])
                
                if len(df) > 0 and 'ml_health_status' in df.columns:
                    health_colors = {'healthy': 'green', 'moderate': 'yellow', 'stressed': 'red'}
                    
                    for _, cell in df.iterrows():
                        y, x = cell['centroid_y'] / self.pixel_to_micron, cell['centroid_x'] / self.pixel_to_micron
                        color = health_colors.get(cell['ml_health_status'], 'gray')
                        marker_size = np.sqrt(cell['area_microns_sq']) / 10
                        
                        # Draw cell
                        ax2.plot(x, y, 'o', color=color, markersize=marker_size, 
                               markeredgecolor='white', markeredgewidth=1, alpha=0.7)
                        
                        # Add cell ID
                        ax2.text(x, y, str(int(cell['cell_id'])), fontsize=6, color='white',
                               ha='center', va='center', weight='bold')
                        
                        # Mark anomalies
                        if 'is_anomaly' in df.columns and cell['is_anomaly']:
                            ax2.plot(x, y, 'x', color='red', markersize=15, markeredgewidth=2)
                
                ax2.set_title(f'ML Cell Classification - {len(df)} cells', fontsize=14)
                ax2.axis('off')
                
                # Add legend
                from matplotlib.patches import Patch
                legend_elements = [
                    Patch(facecolor='green', label='Healthy'),
                    Patch(facecolor='yellow', label='Moderate'),
                    Patch(facecolor='red', label='Stressed'),
                    Patch(facecolor='none', edgecolor='red', linewidth=2, label='Anomaly')
                ]
                ax2.legend(handles=legend_elements, loc='upper right')
                
                buffer2 = BytesIO()
                plt.savefig(buffer2, format='png', dpi=150, bbox_inches='tight')
                buffer2.seek(0)
                visualizations['ml_cell_tracking'] = base64.b64encode(buffer2.getvalue()).decode()
                plt.close()
                
                # 2. PCA visualization of cell features
                if len(df) > 10:
                    fig3, ax3 = plt.subplots(figsize=(10, 8))
                    
                    feature_cols = ['area_microns_sq', 'circularity', 'chlorophyll_index', 
                                  'mean_ndvi', 'edge_density', 'aspect_ratio']
                    X = df[feature_cols].fillna(0)
                    
                    # Perform PCA
                    from sklearn.decomposition import PCA
                    pca = PCA(n_components=2)
                    X_pca = pca.fit_transform(StandardScaler().fit_transform(X))
                    
                    # Color by health status
                    if 'ml_health_status' in df.columns:
                        health_colors = {'healthy': 'green', 'moderate': 'orange', 'stressed': 'red'}
                        colors = [health_colors.get(status, 'gray') for status in df['ml_health_status']]
                    else:
                        colors = 'blue'
                    
                    scatter = ax3.scatter(X_pca[:, 0], X_pca[:, 1], c=colors, alpha=0.6, s=60)
                    ax3.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')
                    ax3.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')
                    ax3.set_title('PCA of Cell Features')
                    ax3.grid(True, alpha=0.3)
                    
                    buffer3 = BytesIO()
                    plt.savefig(buffer3, format='png', dpi=150, bbox_inches='tight')
                    buffer3.seek(0)
                    visualizations['pca_analysis'] = base64.b64encode(buffer3.getvalue()).decode()
                    plt.close()
                
            elif output_path:
                plt.savefig(output_path, dpi=300, bbox_inches='tight')
                print(f"Enhanced visualization saved to: {output_path}")
                
            return visualizations
            
        except Exception as e:
            print(f"Error creating enhanced visualization: {str(e)}")
            return {}
    
    def analyze_time_series(self, image_paths, timestamps=None):
        """
        Enhanced time series analysis with ML predictions
        """
        if timestamps is None:
            timestamps = [f"t_{i:03d}" for i in range(len(image_paths))]
            
        results = []
        
        print(f"Starting enhanced time series analysis of {len(image_paths)} images...")
        
        for i, (path, timestamp) in enumerate(zip(image_paths, timestamps)):
            print(f"Processing image {i+1}/{len(image_paths)}: {timestamp}")
            result = self.analyze_single_image(path, timestamp, save_visualization=True)
            if result:
                results.append(result)
            else:
                print(f"Failed to analyze image {i+1}")
        
        # Analyze population dynamics
        if len(results) > 1:
            population_dynamics = self.analyze_population_dynamics(results)
            if population_dynamics:
                results[-1]['population_dynamics'] = population_dynamics
            
            # Create time series visualizations
            time_series_viz = self.create_enhanced_time_series_plots(results, return_base64=True)
            if time_series_viz:
                results[-1]['time_series_visualizations'] = time_series_viz
            
            # Optimize parameters based on results
            optimized_params = self.optimize_parameters(results)
            results[-1]['optimized_parameters'] = optimized_params
        
        print(f"Enhanced time series analysis complete: {len(results)} images processed")
        return results
    
    def create_enhanced_time_series_plots(self, results, return_base64=False):
        """Create enhanced time series visualizations with ML insights"""
        try:
            if len(results) < 2:
                return None
                
            visualizations = {}
            
            # Extract time series data
            timestamps = [r['timestamp'] for r in results]
            time_points = np.arange(len(timestamps))
            
            # Basic metrics
            cell_counts = [r['total_cells'] for r in results]
            biomass_totals = [r['summary']['total_biomass_ug'] for r in results]
            
            # ML metrics
            health_scores = []
            anomaly_counts = []
            diversity_indices = []
            
            for r in results:
                if 'summary' in r:
                    health_scores.append(r['summary'].get('mean_health_score', 0))
                    anomaly_counts.append(r['summary'].get('anomaly_count', 0))
                    
                    # Calculate diversity
                    if 'ml_cell_type_distribution' in r['summary']:
                        dist = r['summary']['ml_cell_type_distribution']
                        if dist:
                            total = sum(dist.values())
                            proportions = np.array(list(dist.values())) / total
                            shannon = -np.sum(proportions * np.log(proportions + 1e-10))
                            diversity_indices.append(shannon)
                        else:
                            diversity_indices.append(0)
                    else:
                        diversity_indices.append(0)
            
            # Create comprehensive figure
            fig = plt.figure(figsize=(20, 16))
            gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)
            
            # 1. Cell count and biomass trends
            ax1 = fig.add_subplot(gs[0, :])
            ax1_twin = ax1.twinx()
            
            line1 = ax1.plot(time_points, cell_counts, 'b-o', linewidth=2, markersize=8, label='Cell Count')
            line2 = ax1_twin.plot(time_points, biomass_totals, 'g-s', linewidth=2, markersize=8, label='Total Biomass')
            
            ax1.set_xlabel('Time Point')
            ax1.set_ylabel('Cell Count', color='b')
            ax1_twin.set_ylabel('Total Biomass (Î¼g)', color='g')
            ax1.set_title('Population Growth Dynamics', fontsize=14)
            ax1.grid(True, alpha=0.3)
            
            # Add trend lines
            z1 = np.polyfit(time_points, cell_counts, 1)
            p1 = np.poly1d(z1)
            ax1.plot(time_points, p1(time_points), 'b--', alpha=0.5)
            
            z2 = np.polyfit(time_points, biomass_totals, 1)
            p2 = np.poly1d(z2)
            ax1_twin.plot(time_points, p2(time_points), 'g--', alpha=0.5)
            
            # Combined legend
            lines = line1 + line2
            labels = [l.get_label() for l in lines]
            ax1.legend(lines, labels, loc='upper left')
            
            ax1.set_xticks(time_points)
            ax1.set_xticklabels(timestamps, rotation=45)
            
            # 2. Health and anomaly tracking
            ax2 = fig.add_subplot(gs[1, 0])
            ax2_twin = ax2.twinx()
            
            ax2.plot(time_points, health_scores, 'teal', marker='o', linewidth=2, markersize=8)
            ax2_twin.bar(time_points, anomaly_counts, alpha=0.5, color='red', width=0.6)
            
            ax2.set_xlabel('Time Point')
            ax2.set_ylabel('Mean Health Score', color='teal')
            ax2_twin.set_ylabel('Anomaly Count', color='red')
            ax2.set_title('Population Health Monitoring')
            ax2.grid(True, alpha=0.3)
            ax2.set_xticks(time_points)
            ax2.set_xticklabels(timestamps, rotation=45)
            
            # 3. Diversity index
            ax3 = fig.add_subplot(gs[1, 1])
            ax3.plot(time_points, diversity_indices, 'purple', marker='D', linewidth=2, markersize=8)
            ax3.set_xlabel('Time Point')
            ax3.set_ylabel('Shannon Diversity Index')
            ax3.set_title('Population Diversity Over Time')
            ax3.grid(True, alpha=0.3)
            ax3.set_xticks(time_points)
            ax3.set_xticklabels(timestamps, rotation=45)
            
            # 4. Growth stage distribution over time
            ax4 = fig.add_subplot(gs[1, 2])
            growth_stages = ['daughter_frond', 'young', 'mature', 'mother_frond']
            stage_data = {stage: [] for stage in growth_stages}
            
            for r in results:
                if 'summary' in r and 'ml_growth_stage_distribution' in r['summary']:
                    dist = r['summary']['ml_growth_stage_distribution']
                    for stage in growth_stages:
                        stage_data[stage].append(dist.get(stage, 0))
                else:
                    for stage in growth_stages:
                        stage_data[stage].append(0)
            
            # Stacked area chart
            colors = plt.cm.Greens(np.linspace(0.3, 0.9, len(growth_stages)))
            ax4.stackplot(time_points, *[stage_data[stage] for stage in growth_stages], 
                         labels=growth_stages, colors=colors, alpha=0.7)
            ax4.set_xlabel('Time Point')
            ax4.set_ylabel('Cell Count')
            ax4.set_title('Growth Stage Distribution Over Time')
            ax4.legend(loc='upper left')
            ax4.grid(True, alpha=0.3)
            ax4.set_xticks(time_points)
            ax4.set_xticklabels(timestamps, rotation=45)
            
            # 5. Biomass model comparison
            ax5 = fig.add_subplot(gs[2, :])
            model_types = ['biomass_volume_total', 'biomass_area_total', 'biomass_allometric_total', 'biomass_ml_total']
            model_names = ['Volume', 'Area', 'Allometric', 'ML']
            colors = ['blue', 'green', 'orange', 'red']
            
            for model, name, color in zip(model_types, model_names, colors):
                model_data = []
                for r in results:
                    if 'summary' in r and model in r['summary']:
                        model_data.append(r['summary'][model])
                    else:
                        model_data.append(0)
                
                ax5.plot(time_points, model_data, color=color, marker='o', 
                        linewidth=2, markersize=6, label=name, alpha=0.7)
            
            ax5.set_xlabel('Time Point')
            ax5.set_ylabel('Total Biomass (Î¼g)')
            ax5.set_title('Biomass Predictions by Model')
            ax5.legend()
            ax5.grid(True, alpha=0.3)
            ax5.set_xticks(time_points)
            ax5.set_xticklabels(timestamps, rotation=45)
            
            # 6. Population dynamics predictions
            if len(results) > 0 and 'population_dynamics' in results[-1]:
                pop_dyn = results[-1]['population_dynamics']
                
                if 'predictions' in pop_dyn:
                    ax6 = fig.add_subplot(gs[3, 0:2])
                    
                    # Historical data
                    ax6.plot(time_points, cell_counts, 'bo-', linewidth=2, markersize=8, label='Observed')
                    
                    # Predictions
                    future_points = pop_dyn['predictions']['future_time_points']
                    predicted_counts = pop_dyn['predictions']['predicted_cell_counts']
                    
                    ax6.plot(future_points, predicted_counts, 'r--o', linewidth=2, 
                            markersize=6, label='Predicted (Exponential)')
                    
                    # Logistic prediction if available
                    if pop_dyn['predictions']['predicted_counts_logistic']:
                        logistic_counts = pop_dyn['predictions']['predicted_counts_logistic']
                        ax6.plot(future_points, logistic_counts, 'g--s', linewidth=2, 
                                markersize=6, label='Predicted (Logistic)')
                    
                    ax6.set_xlabel('Time Point')
                    ax6.set_ylabel('Cell Count')
                    ax6.set_title('Population Growth Prediction')
                    ax6.legend()
                    ax6.grid(True, alpha=0.3)
                    
                    # Add growth metrics text
                    growth_text = f"Growth Rate: {pop_dyn['growth_analysis']['cell_count_growth_rate']:.3f}\n"
                    growth_text += f"Doubling Time: {pop_dyn['growth_analysis']['doubling_time_cells']:.1f}"
                    if pop_dyn['growth_analysis']['carrying_capacity']:
                        growth_text += f"\nCarrying Capacity: {pop_dyn['growth_analysis']['carrying_capacity']:.0f}"
                    
                    ax6.text(0.02, 0.98, growth_text, transform=ax6.transAxes, 
                            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),
                            verticalalignment='top', fontsize=10)
            
            # 7. Quality metrics
            ax7 = fig.add_subplot(gs[3, 2])
            quality_metrics = ['segmentation_confidence', 'population_homogeneity', 'health_index']
            metric_data = {metric: [] for metric in quality_metrics}
            
            for r in results:
                if 'summary' in r:
                    for metric in quality_metrics:
                        metric_data[metric].append(r['summary'].get(metric, 0))
            
            x = np.arange(len(quality_metrics))
            width = 0.2
            
            for i, (timestamp, color) in enumerate(zip(timestamps[:3], ['blue', 'green', 'red'])):
                values = [metric_data[metric][i] if i < len(metric_data[metric]) else 0 
                         for metric in quality_metrics]
                ax7.bar(x + i*width, values, width, label=timestamp, color=color, alpha=0.7)
            
            ax7.set_xlabel('Quality Metric')
            ax7.set_ylabel('Score')
            ax7.set_title('Analysis Quality Metrics')
            ax7.set_xticks(x + width)
            ax7.set_xticklabels(['Segmentation', 'Homogeneity', 'Health'], rotation=45)
            ax7.legend()
            ax7.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            if return_base64:
                buffer = BytesIO()
                plt.savefig(buffer, format='png', dpi=150, bbox_inches='tight')
                buffer.seek(0)
                visualizations['enhanced_time_series'] = base64.b64encode(buffer.getvalue()).decode()
                plt.close()
                
                # Create growth rate analysis plot
                if len(results) > 1 and 'population_dynamics' in results[-1]:
                    fig2, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))
                    
                    # Growth rates
                    growth_rates_cells = []
                    growth_rates_biomass = []
                    
                    for i in range(1, len(results)):
                        cell_rate = (cell_counts[i] - cell_counts[i-1]) / cell_counts[i-1] if cell_counts[i-1] > 0 else 0
                        biomass_rate = (biomass_totals[i] - biomass_totals[i-1]) / biomass_totals[i-1] if biomass_totals[i-1] > 0 else 0
                        growth_rates_cells.append(cell_rate * 100)
                        growth_rates_biomass.append(biomass_rate * 100)
                    
                    periods = [f"{timestamps[i]}-{timestamps[i+1]}" for i in range(len(growth_rates_cells))]
                    
                    # Bar plot of growth rates
                    x = np.arange(len(periods))
                    width = 0.35
                    
                    bars1 = ax1.bar(x - width/2, growth_rates_cells, width, label='Cell Count', color='blue', alpha=0.7)
                    bars2 = ax1.bar(x + width/2, growth_rates_biomass, width, label='Biomass', color='green', alpha=0.7)
                    
                    ax1.set_ylabel('Growth Rate (%)')
                    ax1.set_title('Period-to-Period Growth Rates')
                    ax1.set_xticks(x)
                    ax1.set_xticklabels(periods, rotation=45)
                    ax1.legend()
                    ax1.grid(True, alpha=0.3)
                    ax1.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
                    
                    # Add value labels
                    for bars in [bars1, bars2]:
                        for bar in bars:
                            height = bar.get_height()
                            ax1.text(bar.get_x() + bar.get_width()/2., height,
                                   f'{height:.1f}%', ha='center', va='bottom' if height > 0 else 'top')
                    
                    # Population alerts
                    pop_dyn = results[-1]['population_dynamics']
                    if 'alerts' in pop_dyn and pop_dyn['alerts']:
                        alert_text = "Population Alerts:\n"
                        for alert in pop_dyn['alerts']:
                            alert_text += f"â€¢ {alert['message']} ({alert['severity']})\n"
                        
                        ax2.text(0.5, 0.5, alert_text, transform=ax2.transAxes,
                                bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8),
                                verticalalignment='center', horizontalalignment='center',
                                fontsize=12)
                        ax2.axis('off')
                    else:
                        ax2.text(0.5, 0.5, "No population alerts", transform=ax2.transAxes,
                                verticalalignment='center', horizontalalignment='center',
                                fontsize=12, color='green')
                        ax2.axis('off')
                    
                    plt.tight_layout()
                    
                    buffer2 = BytesIO()
                    plt.savefig(buffer2, format='png', dpi=150, bbox_inches='tight')
                    buffer2.seek(0)
                    visualizations['growth_analysis'] = base64.b64encode(buffer2.getvalue()).decode()
                    plt.close()
                
                return visualizations
            else:
                plt.savefig(f"enhanced_time_series_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png", 
                          dpi=300, bbox_inches='tight')
                plt.show()
                
        except Exception as e:
            print(f"Error creating enhanced time series plots: {str(e)}")
            return None

    def export_enhanced_results(self, result, output_path=None):
            """Export enhanced analysis results with ML insights"""
            try:
                # Define convert_types as a method-level function first
                def convert_types(obj):
                    """Convert numpy/pandas types to JSON-serializable formats"""
                    if isinstance(obj, (np.integer, np.int64, np.int32)):
                        return int(obj)
                    elif isinstance(obj, (np.floating, np.float64, np.float32)):
                        return float(obj)
                    elif isinstance(obj, np.ndarray):
                        return obj.tolist()
                    elif isinstance(obj, pd.Series):
                        return obj.to_list()
                    elif isinstance(obj, pd.DataFrame):
                        return obj.to_dict('records')
                    elif isinstance(obj, dict):
                        return {k: convert_types(v) for k, v in obj.items()}
                    elif isinstance(obj, list):
                        return [convert_types(item) for item in obj]
                    elif isinstance(obj, tuple):
                        return tuple(convert_types(item) for item in obj)
                    elif pd.isna(obj):
                        return None
                    else:
                        return obj
                
                # Prepare comprehensive JSON data
                json_data = {
                    'timestamp': result['timestamp'],
                    'image_path': str(result['image_path']),  # Ensure it's a string
                    'total_cells': int(result['total_cells']),  # Ensure it's an int
                    'summary': result['summary'],
                    'visualizations': result.get('visualizations', {}),
                    'ml_metrics': result.get('ml_metrics', {}),
                    'population_dynamics': result.get('population_dynamics', {}),
                    'optimized_parameters': result.get('optimized_parameters', {}),
                    'analysis_version': '2.0-ML-Enhanced'
                }
                
                # Convert cell data
                if 'cell_data' in result:
                    if isinstance(result['cell_data'], pd.DataFrame):
                        df = result['cell_data'].copy()
                        
                        # Select key columns for export
                        export_columns = [
                            'cell_id', 'area_microns_sq', 'perimeter_microns', 'circularity',
                            'mean_chlorophyll_intensity', 'chlorophyll_index', 'mean_ndvi', 'mean_gci', 
                            'biomass_estimate_ug', 'biomass_ensemble', 'biomass_uncertainty', 
                            'ml_cell_type', 'cell_type', 'ml_health_status', 'health_status',
                            'ml_health_score', 'ml_growth_stage', 'growth_stage', 'is_anomaly',
                            'similar_cell_count', 'centroid_x', 'centroid_y'
                        ]
                        
                        # Filter to only existing columns
                        available_columns = [col for col in export_columns if col in df.columns]
                        export_df = df[available_columns].copy()
                        
                        # Convert DataFrame to records, handling numpy types
                        cells_data = []
                        for idx, row in export_df.iterrows():
                            cell_dict = {}
                            for col in available_columns:
                                value = row[col]
                                # Convert each value individually
                                cell_dict[col] = convert_types(value)
                            cells_data.append(cell_dict)
                        
                        json_data['cells'] = cells_data
                    else:
                        # If cell_data is already a list or other format
                        json_data['cells'] = convert_types(result['cell_data'])
                
                # Apply conversion to the entire json_data structure
                json_data = convert_types(json_data)
                
                # Save to file if path provided
                if output_path:
                    with open(output_path, 'w') as f:
                        json.dump(json_data, f, indent=2)
                    print(f"Enhanced results exported to: {output_path}")
                
                return json_data
                
            except Exception as e:
                print(f"Error exporting enhanced results: {str(e)}")
                import traceback
                traceback.print_exc()
                # Return a minimal valid result on error
                return {
                    'timestamp': result.get('timestamp', ''),
                    'image_path': str(result.get('image_path', '')),
                    'total_cells': 0,
                    'error': str(e),
                    'analysis_version': '2.0-ML-Enhanced'
                }


    def _find_optimal_tophat_size(self, gray_image):
        """
        Automatically determine optimal structuring element size
        """
        # Estimate cell sizes using FFT
        f_transform = np.fft.fft2(gray_image)
        f_shift = np.fft.fftshift(f_transform)
        magnitude_spectrum = np.abs(f_shift)
        
        # Find dominant frequencies
        radial_profile = self._radial_profile(magnitude_spectrum)
        
        # Find peaks in radial profile
        from scipy.signal import find_peaks
        peaks, _ = find_peaks(radial_profile, height=np.max(radial_profile)*0.1)
        
        if len(peaks) > 1:
            # Estimate feature size from frequency
            dominant_freq = peaks[1]  # Skip DC component
            feature_size = gray_image.shape[0] / dominant_freq
            optimal_size = int(feature_size / 2)
        else:
            # Default fallback
            optimal_size = 30
        
        return np.clip(optimal_size, 10, 100)

    def _radial_profile(self, data):
        """Calculate radial profile of 2D data"""
        center = np.array(data.shape) // 2
        y, x = np.ogrid[:data.shape[0], :data.shape[1]]
        r = np.sqrt((x - center[1])**2 + (y - center[0])**2)
        r = r.astype(int)
        
        tbin = np.bincount(r.ravel(), data.ravel())
        nr = np.bincount(r.ravel())
        radialprofile = tbin / nr
        
        return radialprofile

    def _intelligent_background_detection(self, image, tophat, chlorophyll):
        """
        Intelligently detect and remove background
        """
        # Method 1: Otsu on tophat image
        _, binary1 = cv2.threshold(tophat, 0, 1, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        
        # Method 2: Chlorophyll threshold
        chlorophyll_norm = (chlorophyll - np.min(chlorophyll)) / (np.max(chlorophyll) - np.min(chlorophyll))
        binary2 = chlorophyll_norm > 0.1
        
        # Method 3: Color-based (HSV)
        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
        # Green hue range
        lower_green = np.array([35, 40, 40])
        upper_green = np.array([85, 255, 255])
        binary3 = cv2.inRange(hsv, lower_green, upper_green) > 0
        
        # Combine methods
        combined = (binary1 + binary2 + binary3) >= 2
        
        # Morphological cleanup
        combined = morphology.remove_small_holes(combined, area_threshold=100)
        combined = morphology.remove_small_objects(combined, min_size=50)
        
        return combined.astype(np.float32)




    def _create_wavelength_visualization(self, image, labels, spectral_data):
        """
        Create visualization of spectral analysis
        """
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        
        # Original image with cell outlines
        ax = axes[0, 0]
        ax.imshow(image)
        ax.contour(labels > 0, colors='white', linewidths=1)
        ax.set_title('Original with Cell Boundaries')
        ax.axis('off')
        
        # Chlorophyll concentration heatmap
        ax = axes[0, 1]
        chlor_map = np.zeros_like(labels, dtype=np.float32)
        for cell in spectral_data:
            mask = labels == cell['cell_id']
            chlor_map[mask] = cell['chlorophyll_concentration']
        
        im = ax.imshow(chlor_map, cmap='Greens', vmin=0)
        ax.set_title('Chlorophyll Concentration (Î¼g/cmÂ²)')
        ax.axis('off')
        plt.colorbar(im, ax=ax)
        
        # Health status map
        ax = axes[0, 2]
        health_map = np.zeros_like(labels, dtype=np.float32)
        health_colors = {
            'very_healthy': 4,
            'healthy': 3,
            'moderate': 2,
            'stressed': 1
        }
        
        for cell in spectral_data:
            mask = labels == cell['cell_id']
            health_map[mask] = health_colors.get(cell['spectral_health'], 0)
        
        im = ax.imshow(health_map, cmap='RdYlGn', vmin=0, vmax=4)
        ax.set_title('Cell Health Status')
        ax.axis('off')
        
        # Spectral indices
        indices_to_show = ['evi', 'gli', 'tgi']
        for i, idx_name in enumerate(indices_to_show):
            ax = axes[1, i]
            idx_map = np.zeros_like(labels, dtype=np.float32)
            
            for cell in spectral_data:
                mask = labels == cell['cell_id']
                idx_map[mask] = cell[f'{idx_name}_mean']
            
            im = ax.imshow(idx_map, cmap='viridis')
            ax.set_title(f'{idx_name.upper()} Index')
            ax.axis('off')
            plt.colorbar(im, ax=ax)
        
        plt.tight_layout()
        
        # Convert to base64
        buffer = BytesIO()
        plt.savefig(buffer, format='png', dpi=150, bbox_inches='tight')
        buffer.seek(0)
        wavelength_viz_base64 = base64.b64encode(buffer.getvalue()).decode()
        plt.close()
        
        return wavelength_viz_base64

# Enhanced helper functions for web integration
def analyze_uploaded_image(image_path, analyzer=None):
    """
    Analyze a single uploaded image with ML enhancement
    """
    if analyzer is None:
        analyzer = WolffiaAnalyzer(pixel_to_micron_ratio=0.5, chlorophyll_threshold=0.6)
    
    try:
        result = analyzer.analyze_single_image(image_path)
        
        if result:
            # Export to JSON format
            json_result = analyzer.export_enhanced_results(result)
            return json_result
        else:
            print(f"Analysis returned no results for {image_path}")
            return None
    except Exception as e:
        print(f"Error in analyze_uploaded_image: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

def analyze_multiple_images(image_paths, timestamps=None, analyzer=None):
    """
    Analyze multiple images with enhanced ML time series analysis
    """
    if analyzer is None:
        analyzer = WolffiaAnalyzer(pixel_to_micron_ratio=0.5, chlorophyll_threshold=0.6)
    
    results = analyzer.analyze_time_series(image_paths, timestamps)
    
    if results:
        # Convert all results to JSON format
        json_results = []
        for result in results:
            json_result = analyzer.export_enhanced_results(result)
            json_results.append(json_result)
        
        return json_results
    
    return None



def smart_preprocess_with_tophat(self, image_path, auto_optimize=True):
    """
    Enhanced preprocessing with intelligent top-hat filtering
    """
    # Load image
    if isinstance(image_path, str):
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    else:
        image = image_path
    
    original = image.copy()
    
    # Convert to grayscale for morphological operations
    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    
    # Auto-detect optimal structuring element size
    if auto_optimize:
        optimal_size = self._find_optimal_tophat_size(gray)
    else:
        optimal_size = 30
    
    # Multi-scale top-hat transform
    tophat_results = []
    for size in [optimal_size//2, optimal_size, optimal_size*2]:
        selem = disk(size)
        
        # White top-hat (bright features on dark background)
        white_tophat = cv2.morphologyEx(gray, cv2.MORPH_TOPHAT, selem)
        
        # Black top-hat (dark features on bright background)
        black_tophat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, selem)
        
        # Combined result
        enhanced = gray + white_tophat - black_tophat
        tophat_results.append(enhanced)
    
    # Combine multi-scale results
    combined_tophat = np.mean(tophat_results, axis=0).astype(np.uint8)
    
    # Adaptive histogram equalization on top-hat result
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    enhanced_tophat = clahe.apply(combined_tophat)
    
    # Extract enhanced green channel
    green_channel = image[:,:,1].astype(np.float32) / 255.0
    
    # Apply top-hat enhancement to green channel
    green_enhanced = green_channel * (enhanced_tophat / 255.0)
    
    # Chlorophyll-specific enhancement
    # Use color deconvolution for chlorophyll
    chlorophyll_stain = np.array([[0.460, 0.570, 0.680]])  # Chlorophyll absorption
    
    # Normalize image
    img_normalized = image.astype(np.float32) / 255.0
    img_flat = img_normalized.reshape(-1, 3)
    
    # Project onto chlorophyll vector
    chlorophyll_projection = np.dot(img_flat, chlorophyll_stain.T)
    chlorophyll_map = chlorophyll_projection.reshape(gray.shape)
    
    # Enhance chlorophyll signal
    chlorophyll_enhanced = np.clip(chlorophyll_map * 2, 0, 1)
    
    # Intelligent background removal
    background_mask = self._intelligent_background_detection(
        image, enhanced_tophat, chlorophyll_enhanced
    )
    
    # Apply background mask
    green_enhanced = green_enhanced * background_mask
    chlorophyll_enhanced = chlorophyll_enhanced * background_mask
    
    # Return enhanced results
    return {
        'original': original,
        'gray': gray,
        'tophat_enhanced': enhanced_tophat,
        'green_enhanced': green_enhanced,
        'chlorophyll_enhanced': chlorophyll_enhanced,
        'background_mask': background_mask,
        'optimal_tophat_size': optimal_size
    }

def analyze_chlorophyll_spectrum(self, image, labels):
    """
    Advanced spectral analysis for chlorophyll quantification
    """
    # RGB to approximate spectral bands
    red = image[:,:,0].astype(np.float32) / 255.0
    green = image[:,:,1].astype(np.float32) / 255.0
    blue = image[:,:,2].astype(np.float32) / 255.0
    
    # Calculate various vegetation indices
    indices = {}
    
    # 1. Enhanced Vegetation Index (EVI) approximation
    indices['evi'] = 2.5 * ((green - red) / (green + 6*red - 7.5*blue + 1))
    
    # 2. Green Leaf Index (GLI)
    indices['gli'] = (2*green - red - blue) / (2*green + red + blue + 1e-10)
    
    # 3. Chlorophyll Index Green (CIg)
    indices['cig'] = (green / (red + 1e-10)) - 1
    
    # 4. Modified Chlorophyll Absorption Ratio Index (MCARI)
    # Using RGB approximation
    indices['mcari'] = ((green - red) - 0.2 * (green - blue)) * (green / (red + 1e-10))
    
    # 5. Triangular Greenness Index (TGI)
    indices['tgi'] = green - 0.39*red - 0.61*blue
    
    # Analyze per cell
    cell_spectral_data = []
    
    for prop in measure.regionprops(labels):
        cell_mask = labels == prop.label
        
        cell_data = {
            'cell_id': prop.label,
            'area_microns_sq': prop.area * (self.pixel_to_micron ** 2),
            'centroid': prop.centroid
        }
        
        # Extract spectral features for this cell
        for idx_name, idx_map in indices.items():
            cell_values = idx_map[cell_mask]
            cell_data[f'{idx_name}_mean'] = np.mean(cell_values)
            cell_data[f'{idx_name}_std'] = np.std(cell_values)
            cell_data[f'{idx_name}_max'] = np.max(cell_values)
        
        # RGB channel statistics
        cell_data['red_mean'] = np.mean(red[cell_mask])
        cell_data['green_mean'] = np.mean(green[cell_mask])
        cell_data['blue_mean'] = np.mean(blue[cell_mask])
        
        # Chlorophyll concentration estimation
        # Based on empirical relationship with green/red ratio
        gr_ratio = cell_data['green_mean'] / (cell_data['red_mean'] + 1e-10)
        
        # Chlorophyll a+b concentration (Î¼g/cmÂ²)
        # Using calibration curve: Chl = a * (G/R)^b
        a, b = 12.7, 1.5  # Empirical constants for Wolffia
        cell_data['chlorophyll_concentration'] = a * (gr_ratio ** b)
        
        # Total chlorophyll content (Î¼g)
        area_cm2 = cell_data['area_microns_sq'] / 1e8  # Convert Î¼mÂ² to cmÂ²
        cell_data['total_chlorophyll_ug'] = cell_data['chlorophyll_concentration'] * area_cm2
        
        # Health classification based on spectral signature
        health_score = (
            0.3 * cell_data['evi_mean'] +
            0.2 * cell_data['gli_mean'] +
            0.2 * cell_data['cig_mean'] +
            0.3 * cell_data['tgi_mean']
        )
        
        if health_score > 0.7:
            cell_data['spectral_health'] = 'very_healthy'
        elif health_score > 0.5:
            cell_data['spectral_health'] = 'healthy'
        elif health_score > 0.3:
            cell_data['spectral_health'] = 'moderate'
        else:
            cell_data['spectral_health'] = 'stressed'
        
        cell_spectral_data.append(cell_data)
    
    # Create wavelength simulation visualization
    wavelength_viz = self._create_wavelength_visualization(
        image, labels, cell_spectral_data
    )
    
    return pd.DataFrame(cell_spectral_data), wavelength_viz

def generate_spectral_report(self, spectral_df):
    """
    Generate detailed spectral analysis report
    """
    report = []
    report.append("SPECTRAL ANALYSIS REPORT")
    report.append("=" * 50)
    report.append(f"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append("")
    
    # Overall statistics
    report.append("CHLOROPHYLL STATISTICS")
    report.append("-" * 30)
    report.append(f"Total Cells Analyzed: {len(spectral_df)}")
    report.append(f"Mean Chlorophyll Concentration: {spectral_df['chlorophyll_concentration'].mean():.2f} Î¼g/cmÂ²")
    report.append(f"Total Chlorophyll Content: {spectral_df['total_chlorophyll_ug'].sum():.2f} Î¼g")
    report.append("")
    
    # Health distribution
    report.append("HEALTH DISTRIBUTION (Spectral)")
    report.append("-" * 30)
    health_dist = spectral_df['spectral_health'].value_counts()
    for health, count in health_dist.items():
        percentage = (count / len(spectral_df)) * 100
        report.append(f"{health}: {count} cells ({percentage:.1f}%)")
    report.append("")
    
    # Vegetation indices
    report.append("VEGETATION INDICES (Mean Values)")
    report.append("-" * 30)
    indices = ['evi', 'gli', 'cig', 'tgi']
    for idx in indices:
        mean_val = spectral_df[f'{idx}_mean'].mean()
        std_val = spectral_df[f'{idx}_mean'].std()
        report.append(f"{idx.upper()}: {mean_val:.3f} Â± {std_val:.3f}")
    report.append("")
    
    # Wavelength approximations
    report.append("SPECTRAL BAND ANALYSIS")
    report.append("-" * 30)
    report.append(f"Red (660nm) Reflectance: {spectral_df['red_mean'].mean():.3f}")
    report.append(f"Green (550nm) Reflectance: {spectral_df['green_mean'].mean():.3f}")
    report.append(f"Blue (450nm) Reflectance: {spectral_df['blue_mean'].mean():.3f}")
    report.append(f"Green/Red Ratio: {(spectral_df['green_mean']/spectral_df['red_mean']).mean():.3f}")
    
    return "\n".join(report)









#  usage
if __name__ == "__main__":
    print("=" * 80)
    print("BIOIMAGIN - Enhanced Wolffia Analysis System v2.0")
    print("=" * 80)
    print("\nFeatures:")
    print("âœ“ Multi-method segmentation (Otsu, Adaptive, K-means, Felzenszwalb, SLIC)")
    print("âœ“ Machine Learning cell classification")
    print("âœ“ Advanced biomass prediction models")
    print("âœ“ Population dynamics analysis")
    print("âœ“ Anomaly detection")
    print("âœ“ Automated parameter optimization")
    print("âœ“ Spectral analysis (NDVI, GCI, ExG)")
    print("âœ“ Growth stage classification")
    print("âœ“ Predictive growth modeling")
    print("âœ“ Feature importance analysis")
    print("âœ“ Real-time quality metrics")
    print("\nSystem ready for production use.")
    print("=" * 80)